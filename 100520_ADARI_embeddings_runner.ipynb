{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/manuelladron/iCloud_archive/Documents/_CMU/PHD-CD/PHD-CD_Research/multilabel_classifier/py_files\")\n",
    "sys.path.append(\"/Users/manuelladron/phd_cd/DL_11785/homework/hw3/hw3p1/mytorch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import pdb\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES\n",
    "im_path_fur = \"../../dataset/images/furniture/thumbs/big\"\n",
    "\n",
    "# JSON_FILES\n",
    "data_path_fur = \"../../dataset/json_files/cleaned/furniture_cleaned.json\"\n",
    "\n",
    "# FURNITURE VOCAB \n",
    "vocab_id2w = \"../../dataset/json_files/vocab/furniture_current/vocab_id2w.json\"\n",
    "vocab_w2id = \"../../dataset/json_files/vocab/furniture_current/vocab_w2id.json\"\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "word_embeddings_path = \"../../dataset/json_files/embeddings/fur_5c_50d_sk_glove_ft.json\"\n",
    "\n",
    "# FILES FOR DATALOADER\n",
    "dset_path = '/Users/manuelladron/iCloud_archive/Documents/_CMU/PHD-CD/PHD-CD_Research/dataset/json_files/dset_for_dataloader/dset_dataloader.json'\n",
    "im2idx_path = '/Users/manuelladron/iCloud_archive/Documents/_CMU/PHD-CD/PHD-CD_Research/dataset/json_files/dset_for_dataloader/im2idx.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = open_json(word_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = open_json(vocab_w2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = open_json(dset_path)\n",
    "im2idx = open_json(im2idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10825"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10825"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, im2idx, path_to_images, train=True):\n",
    "        self.img_path = path_to_images\n",
    "        self.data = dataset\n",
    "        self.im2idx = im2idx\n",
    "        self.images = list(dataset.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        idx = self.im2idx[image_name]\n",
    "        \n",
    "        name = self.img_path + \"/\" + image_name\n",
    "        img = Image.open(name)\n",
    "        \n",
    "        img = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor()])(img)\n",
    "        \n",
    "        return img, idx\n",
    "                \n",
    "def collate(sequence):\n",
    "    \"\"\"\n",
    "    \"the input of this function is the output of function __getitem__\"\n",
    "    \"this gets BATCH_SIZE times GETITEM! \"\n",
    "    if batch_Size == 2 --> sequence is a list with length 2. \n",
    "    Each list is a tuple (image, label) = ((3,64,64), label_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate all images in the batch\n",
    "    images = torch.cat(([batch_[0].view(-1, 3, 64, 64) for batch_ in sequence]), dim=0)\n",
    "    \n",
    "    # Pad labels with max_sequence_label\n",
    "    idxs = torch.LongTensor([batch_[1] for batch_ in sequence])     \n",
    "    \n",
    "    return images, idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(dset, im2idx, im_path_fur, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc (classification) layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedder = EncoderCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 8 if cuda else 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn = collate, shuffle=False, num_workers=num_workers, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this cell to get image embeddings from ResNet152 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = dict() # dictionary to store image embeddings\n",
    "with torch.no_grad():\n",
    "    for i, (images, idx) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "        print(batch_size)\n",
    "        images = images.to(device)\n",
    "        idx = idx.to(device)   \n",
    "        # Encode image with CNN\n",
    "        features = img_embedder(images).squeeze(3).squeeze(2) # shape. [batch, 2048]\n",
    "        \n",
    "        # Dictonary key:image_idx, embedding\n",
    "        image_embeddings[idx.item()] = features\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
