{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import pdb\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES\n",
    "im_path_fur = \"../ADARI/images/furniture/v2/thumbs/small\" # small are 64x64, medium 256x256 and large 512x512\n",
    "\n",
    "# JSON_FILES\n",
    "data_path_fur = \"../ADARI/json_files/cleaned/ADARI_v2/furniture_v2_c.json\"\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "word_embeddings_path = \"../ADARI/word_embeddings/fur_5c_50d_sk_glove_ft.json\"\n",
    "\n",
    "# FILES FOR DATALOADER\n",
    "dset_words_p = \"../ADARI/json_files/ADARI_FUR_images_sentences_words/ADARI_v2_FUR_images_words.json\"\n",
    "dset_sentences_p = \"../ADARI/json_files/ADARI_FUR_images_sentences_words/ADARI_v2_FUR_images_sentences.json\"\n",
    "dset_sentences_POS_p = \"../ADARI/json_files/ADARI_FUR_images_sentences_words/ADARI_v2_FUR_images_sentences_tokenized.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = open_json(word_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_words = open_json(dset_words_p)\n",
    "dset_sents = open_json(dset_sentences_p)\n",
    "dset_sents_tokenized = open_json(dset_sentences_POS_p)\n",
    "# im2idx = open_json(im2idx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary to get index of image names, A small parser to get words as image: list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date(word):\n",
    "    rx = r\"[0-9]+(?:st|[nr]d|th)\"\n",
    "    if re.findall(rx, word, flags=re.I) != []:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2 dictionaries below for dataset dataloader\n",
    "im2idx = dict()\n",
    "im_words = dict()\n",
    "\n",
    "# Temp lists \n",
    "image_names = list(dset_words.keys())\n",
    "words = list(dset_words.values())\n",
    "\n",
    "# Iterate over length of dictionary and get im2idx and im_words \n",
    "for i in range(len(image_names)):\n",
    "    im = image_names[i]\n",
    "    words_list = flatten(list(words[i].values()))\n",
    "    cleaned_w = []\n",
    "    for w in words_list:\n",
    "        if w != '\"the' and w != '\"The' and len(w) > 1 and is_date(w) != True:\n",
    "            cleaned_w.append(w)\n",
    "\n",
    "    im_words[im] = cleaned_w\n",
    "    im2idx[im] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, im2idx, path_to_images, train=True):\n",
    "        self.img_path = path_to_images\n",
    "        self.data = dataset\n",
    "        self.im2idx = im2idx\n",
    "        self.images = list(dataset.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        idx = self.im2idx[image_name]\n",
    "        \n",
    "        name = self.img_path + \"/\" + image_name\n",
    "        img = Image.open(name)\n",
    "        \n",
    "        img = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor()])(img)\n",
    "        \n",
    "        return img, idx\n",
    "                \n",
    "def collate(sequence):\n",
    "    \"\"\"\n",
    "    \"the input of this function is the output of function __getitem__\"\n",
    "    \"this gets BATCH_SIZE times GETITEM! \"\n",
    "    if batch_Size == 2 --> sequence is a list with length 2. \n",
    "    Each list is a tuple (image, label) = ((3,64,64), label_length)\n",
    "    \"\"\"\n",
    "    # Concatenate all images in the batch\n",
    "    images = torch.cat(([batch_[0].view(-1, 3, 64, 64) for batch_ in sequence]), dim=0)\n",
    "    \n",
    "    # Pad labels with max_sequence_label\n",
    "    idxs = torch.LongTensor([batch_[1] for batch_ in sequence])     \n",
    "    \n",
    "    return images, idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(im_words, im2idx, im_path_fur, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc (classification) layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedder = EncoderCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 8 if cuda else 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn = collate, shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this cell to get image embeddings from ResNet152 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [#-------------------] 5.4%\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = dict() # dictionary to store image embeddings\n",
    "with torch.no_grad():\n",
    "    for i, (images, idx) in enumerate(dataloader):\n",
    "        update_progress(i/len(dataloader))\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.to(device)\n",
    "        idx = idx.to(device)   \n",
    "        # Encode image with CNN\n",
    "        features = img_embedder(images).squeeze(3).squeeze(2) # shape. [batch, 2048]\n",
    "        \n",
    "        # Dictonary key:image_idx, embedding\n",
    "        image_embeddings[idx.item()] = features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resnet_image_embeddings.json\", \"w\") as f:\n",
    "    json.dump(image_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
