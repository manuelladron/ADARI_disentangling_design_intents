{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# from transformers.models.bert.modeling_bert import BertPreTrainingHeads\n",
    "from transformers.modeling_bert import BertPreTrainingHeads\n",
    "from utils import construct_bert_input, PreprocessedADARI_evaluation, save_json\n",
    "\n",
    "from fashionbert_model import FashionBert, FashionBertHead\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Evaluator(torch.nn.Module):     \n",
    "#     def __init__(self, pretrained_model=None):\n",
    "#         super(Evaluator, self).__init__()\n",
    "        \n",
    "#         if pretrained_model != None:\n",
    "#             print('-- Loading fashionbert_pretrained model: {}'.format(pretrained_model))\n",
    "#             fashion_bert = FashionBert.from_pretrained(pretrained_model, return_dict=True)\n",
    "#         else:\n",
    "#             fashion_bert = FashionBert.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        \n",
    "#         self.model = fashion_bert\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class FashionbertEvaluator(transformers.BertPreTrainedModel):  \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) \n",
    "    \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        self.im_to_embedding = torch.nn.Linear(2048, 768)\n",
    "        self.im_to_embedding_norm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    \n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "        \n",
    "        self.init_weights() \n",
    "    \n",
    "    def text2img_scores(self,\n",
    "                        input_ids,\n",
    "                        embeds,\n",
    "                        att_mask,\n",
    "                        embeds_n,    # list \n",
    "                        att_mask_n,    # list \n",
    "                        ):               \n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            input_ids     [1, 448]\n",
    "            embeds:       [1, 512, 768]\n",
    "            att_mask:     [1, 448]\n",
    "            embeds_n:     list with 100 of [1, 512, 768]\n",
    "            att_mask_n:   list with 100 of [1, 448]\n",
    "        \"\"\"\n",
    "        # Score for positive \n",
    "        query_dict_scores = []\n",
    "        query_scores = []\n",
    "        query_labels = []\n",
    "        \n",
    "        score_pos = self.get_scores_and_metrics(\n",
    "            embeds=embeds.to(device), \n",
    "            attention_mask=att_mask.to(device), \n",
    "            labels=input_ids.to(device),  \n",
    "            is_paired=torch.tensor(True).to(device),\n",
    "            only_alignment=True,\n",
    "            )\n",
    "        \n",
    "        #label = score_pos[1]\n",
    "        score_p = score_pos[0].squeeze()\n",
    "        score_p = score_p[1].detach().item() # confidence that is actually positive\n",
    "        score_pos_dict = {'text': input_ids,\n",
    "                         'score': score_p,\n",
    "                         'label': True}\n",
    "        query_dict_scores.append(score_pos_dict)\n",
    "        query_scores.append(score_p)\n",
    "        query_labels.append(True)\n",
    "        \n",
    "        # Scores for negative\n",
    "        for n in range(len(embeds_n)):\n",
    "            score_neg = self.get_scores_and_metrics(\n",
    "                        embeds=embeds_n[n].to(device), \n",
    "                        attention_mask=att_mask_n[n].to(device), \n",
    "                        labels=input_ids.to(device), \n",
    "                        is_paired= torch.tensor(False).to(device),\n",
    "                        only_alignment = True,\n",
    "                        )\n",
    "            \n",
    "            score_n  = score_neg[0].squeeze()\n",
    "            score_n  = score_n[1].detach().item() # confidence that is actually positive\n",
    "            score_neg_dict = {'text': input_ids,\n",
    "                             'score': score_n,\n",
    "                             'label': False}\n",
    "            \n",
    "            query_dict_scores.append(score_neg_dict)\n",
    "            query_scores.append(score_n)\n",
    "            query_labels.append(False)\n",
    "        \n",
    "        #print(evaluator.tokenizer.convert_ids_to_tokens(ids))\n",
    "        S = [(s,l) for s, l in sorted(zip(query_scores, query_labels), key=lambda x: x[0], reverse=True)]\n",
    "        return S\n",
    "    \n",
    "    def img2text_scores(self, input_ids_p, embeds_p, att_mask_p, input_ids_n, embeds_n, att_mask_n):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            input_ids_p : [1, 448]\n",
    "            embeds_p:     [1, 512, 768]\n",
    "            att_mask_p:   [1, 448]\n",
    "            input_ids_n:  list with 100 of [1, 448]\n",
    "            embeds_n:     list with 100 of [1, 512, 768]\n",
    "            att_mask_n:   list with 100 of [1, 448]\n",
    "        \"\"\"\n",
    "        # Score for positive \n",
    "        query_dict_scores = []\n",
    "        query_scores = []\n",
    "        query_labels = []\n",
    "        \n",
    "        score_pos = self.get_scores_and_metrics(\n",
    "            embeds=embeds_p.to(device), \n",
    "            attention_mask=att_mask_p.to(device), \n",
    "            labels=input_ids_p.to(device), \n",
    "            is_paired=torch.tensor(True).to(device),\n",
    "            only_alignment=True,\n",
    "            )\n",
    "        \n",
    "        #label = score_pos[1]\n",
    "        score_p = score_pos[0].squeeze()\n",
    "        score_p = score_p[1].detach().item() # confidence that is actually positive\n",
    "        score_pos_dict = {'text': input_ids_p,\n",
    "                         'score': score_p,\n",
    "                         'label': True}\n",
    "        query_dict_scores.append(score_pos_dict)\n",
    "        query_scores.append(score_p)\n",
    "        query_labels.append(True)\n",
    "        \n",
    "        # Scores for negative\n",
    "        for n in range(len(embeds_n)):\n",
    "            score_neg = self.get_scores_and_metrics(\n",
    "                embeds=embeds_n[n].to(device), \n",
    "                attention_mask=att_mask_n[n].to(device), \n",
    "                labels=input_ids_n[n].to(device), \n",
    "                is_paired= torch.tensor(False).to(device),\n",
    "                only_alignment = True,\n",
    "                )\n",
    "            \n",
    "            score_n  = score_neg[0].squeeze()\n",
    "            score_n  = score_n[1].detach().item() # confidence that is actually positive\n",
    "            score_neg_dict = {'text': input_ids_n[n],\n",
    "                             'score': score_n,\n",
    "                             'label': False}\n",
    "            \n",
    "            query_dict_scores.append(score_neg_dict)\n",
    "            query_scores.append(score_n)\n",
    "            query_labels.append(False)\n",
    "        \n",
    "        #print(evaluator.tokenizer.convert_ids_to_tokens(ids))\n",
    "        S = [(s,l) for s, l in sorted(zip(query_scores, query_labels), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "        return S\n",
    "        \n",
    "    def rank_at_K(self, dict_scores, save_file_name, img2text=True):\n",
    "        logs = ''\n",
    "        \n",
    "        if img2text:\n",
    "            l1 = '------ Image 2 Text ------\\n'\n",
    "            logs += l1\n",
    "            print(l1)\n",
    "        else:\n",
    "            l2 = '------ Text 2 Image ------\\n'\n",
    "            print(l2)\n",
    "        \n",
    "        Ks = [1, 5, 10]\n",
    "        for K in Ks:\n",
    "            #print('------ Rank @ {} ------'.format(K))\n",
    "            found = 0\n",
    "            for key, val in dict_scores.items():\n",
    "                tmp_range = K if K < len(val) else len(val)\n",
    "                for i in range(tmp_range):\n",
    "                    score, label = val[i]\n",
    "                    if label:\n",
    "                        found += 1\n",
    "                        break\n",
    "            l3 = '------ Rank @ {} = {} ------\\n'.format(K, (found/len(dict_scores.keys()) ))\n",
    "            logs += l3\n",
    "            print(l3)     \n",
    "        \n",
    "        save_json(save_file_name, logs)\n",
    "        \n",
    "    def get_scores_and_metrics(\n",
    "        self,\n",
    "        embeds,                       # text + image embedded \n",
    "        attention_mask,\n",
    "        labels=None,                  # [batch, 448]\n",
    "        is_paired=None,               # [batch]\n",
    "        only_alignment = False,\n",
    "        ):\n",
    "        \n",
    "        batch_size = embeds.shape[0]\n",
    "        seq_length = embeds.shape[1]\n",
    "        hidden_dim = embeds.shape[2]\n",
    "\n",
    "        outputs = self.bert(inputs_embeds=embeds, return_dict=True)\n",
    "        sequence_output = outputs.last_hidden_state # [batch, seq_length, hidden_size]\n",
    "        pooler_output = outputs.pooler_output      #  [batch_size, hidden_size] last layer of hidden-state of first token (CLS) + linear layer + tanh\n",
    "\n",
    "        # hidden states corresponding to the text part\n",
    "        text_output = sequence_output[:, :labels.shape[1], :]  # [batch, 448, 768]\n",
    "        # hidden states corresponding to the image part\n",
    "        image_output = sequence_output[:, labels.shape[1]:, :] # [batch, 64, 768]\n",
    "\n",
    "        ### FOR TEXT \n",
    "        # Predict the masked text tokens and alignment scores (whether image and text match)\n",
    "        prediction_scores, alignment_scores = self.cls(text_output, pooler_output)\n",
    "        # prediction score is [batch, 448, vocab_size = 30522]\n",
    "        # aligment score is [batch, 2] 2 with logits corresponding to 1 and  0\n",
    "        \n",
    "        if only_alignment:\n",
    "            return alignment_scores, is_paired.double().detach().item()\n",
    "        \n",
    "        text_evaluator = {'text_pred_logits': prediction_scores, \n",
    "                         'text_labels': labels}\n",
    "        \n",
    "        alignment_evaluator = {'alignment_logits': alignment_scores,\n",
    "                              'alignment_labels': is_paired}\n",
    "        \n",
    "        text_acc, alig_acc = self.accuracy_scores(text_evaluator, alignment_evaluator)\n",
    "        return text_acc, alig_acc\n",
    "    \n",
    "    def accuracy_scores(self, text_evaluator, alignment_evaluator):\n",
    "        \"\"\"\n",
    "        Text evaluator:  dictionary with preds and labels (aligned)\n",
    "        Image evaluator: dictionary with image output and image patches (aligned)\n",
    "        \"\"\"\n",
    "        # Text\n",
    "        text_pred_logits = text_evaluator['text_pred_logits'] # [num_aligned, 448, vocab_size]\n",
    "        text_labels = text_evaluator['text_labels']           # [num_aligned, 448]\n",
    "        \n",
    "        text_preds_logits = text_pred_logits.detach().cpu().numpy()\n",
    "        text_labels = text_labels.cpu().numpy().flatten()\n",
    "        text_preds = np.argmax(text_preds_logits, axis=2).flatten() # [num_algined, 448]\n",
    "        \n",
    "        # Alignment\n",
    "        alig_pred_logits = alignment_evaluator['alignment_logits']      # [1, 2]\n",
    "        alig_labels = alignment_evaluator['alignment_labels']           # [2]\n",
    "        \n",
    "        alig_pred_logits = alig_pred_logits.detach().cpu().numpy()\n",
    "        alig_labels = alig_labels.double().cpu().numpy().flatten()\n",
    "        alig_preds = np.argmax(alig_pred_logits, axis=1).flatten() # [1, 2]\n",
    "        \n",
    "        text_acc = accuracy_score(text_labels, text_preds)\n",
    "        alig_acc = accuracy_score(alig_labels, alig_preds)\n",
    "        \n",
    "        return text_acc, alig_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2text(i, patches, neg_patches, input_ids, is_paired, attention_mask, neg_input_ids, neg_attention_mask, evaluator):\n",
    "    \"\"\"\n",
    "    image2text retrieval: \n",
    "        Query = Image\n",
    "        Paired with: 1 positive text, 100 negative texts\n",
    "    \"\"\"\n",
    "    im_seq_len = patches.shape[1]\n",
    "    bs = input_ids.shape[0]\n",
    "    len_neg_inputs = neg_input_ids.shape[1]\n",
    "    \n",
    "    embeds = construct_bert_input(patches, input_ids, evaluator, device=device)\n",
    "    attention_mask = F.pad(attention_mask, (0, embeds.shape[1] - input_ids.shape[1]), value = 1)\n",
    "\n",
    "    # NEGATIVE SAMPLE # [batch, 100, 448]\n",
    "    all_embeds_neg = []\n",
    "    all_att_mask = []\n",
    "    all_neg_inputs = []\n",
    "\n",
    "    for j in range(len_neg_inputs):\n",
    "        neg_input_id_sample = neg_input_ids[:, j, :] # [1, 448]\n",
    "        neg_attention_mask_sample = neg_attention_mask[:, j, :]\n",
    "\n",
    "        embeds_neg = construct_bert_input(patches, neg_input_id_sample, evaluator, device=device)\n",
    "        attention_mask_neg = F.pad(neg_attention_mask_sample, (0, embeds_neg.shape[1] - neg_input_id_sample.shape[1]), value = 1)\n",
    "\n",
    "        all_embeds_neg.append(embeds_neg)\n",
    "        all_att_mask.append(attention_mask_neg)\n",
    "        all_neg_inputs.append(neg_input_id_sample.detach())\n",
    "\n",
    "        \n",
    "    # Now I have all joint embeddings for 1 positive sample and 100 neg samples\n",
    "    all_scores_query = evaluator.img2text_scores(\n",
    "                input_ids_p = input_ids,\n",
    "                embeds_p = embeds,\n",
    "                att_mask_p = attention_mask,\n",
    "                input_ids_n = all_neg_inputs,\n",
    "                embeds_n = all_embeds_neg,\n",
    "                att_mask_n = all_att_mask)\n",
    "\n",
    "    # Accuracy: only in positive example\n",
    "    txt_acc, alig_acc = evaluator.get_scores_and_metrics(\n",
    "                        embeds,                       # text + image embedded \n",
    "                        attention_mask,\n",
    "                        labels=input_ids,                  # [batch, 448]\n",
    "                        is_paired=is_paired,               # [batch]\n",
    "                        only_alignment = False,\n",
    "                        )\n",
    "\n",
    "    return all_scores_query, txt_acc, alig_acc\n",
    "    \n",
    "    \n",
    "def text2image(i, patches, neg_patches, input_ids, is_paired, attention_mask, neg_input_ids, neg_attention_mask, evaluator):\n",
    "    \"\"\"\n",
    "    text2image retrieval: \n",
    "        Query = Text\n",
    "        Paired with: 1 positive image, 100 negative images\n",
    "    \"\"\"    \n",
    "    im_seq_len = patches.shape[1]\n",
    "    bs = input_ids.shape[0]\n",
    "    len_neg_inputs = neg_input_ids.shape[1]\n",
    "    \n",
    "    \n",
    "    # POSITIVE IMAGE \n",
    "    embeds = construct_bert_input(patches, input_ids, evaluator, device=device)\n",
    "    attention_mask = F.pad(attention_mask, (0, embeds.shape[1] - input_ids.shape[1]), value = 1)\n",
    "        \n",
    "    # NEGATIVE SAMPLES\n",
    "    all_embeds_neg = []\n",
    "    all_att_mask = []\n",
    "    \n",
    "    for p in range(len_neg_inputs):\n",
    "        neg_patches_sample = neg_patches[:, p, :, :]\n",
    "        embeds_neg = construct_bert_input(neg_patches_sample, input_ids, evaluator, device=device)\n",
    "        attention_mask_neg = F.pad(attention_mask, (0, embeds_neg.shape[1] - input_ids.shape[1]), value = 1)\n",
    "\n",
    "        all_embeds_neg.append(embeds_neg)\n",
    "        all_att_mask.append(attention_mask_neg)\n",
    "\n",
    "    \n",
    "    # Now I have all joint embeddings for 1 positive sample and 100 neg samples\n",
    "    all_scores_query = evaluator.text2img_scores(\n",
    "                input_ids   = input_ids,\n",
    "                embeds      = embeds,\n",
    "                att_mask    = attention_mask,\n",
    "                embeds_n    = all_embeds_neg, # list\n",
    "                att_mask_n  = all_att_mask) # list\n",
    "              \n",
    "    \n",
    "    # Accuracy: only in positive example\n",
    "    txt_acc, alig_acc = evaluator.get_scores_and_metrics(\n",
    "                        embeds,                       # text + image embedded \n",
    "                        attention_mask,\n",
    "                        labels=input_ids,                  # [batch, 448]\n",
    "                        is_paired=is_paired,               # [batch]\n",
    "                        only_alignment = False,\n",
    "                        )\n",
    "    \n",
    "    return all_scores_query, txt_acc, alig_acc\n",
    "    \n",
    "    \n",
    "def test(dataset, device, num_samples, save_file_name, pretrained_model=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(0)    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=False,\n",
    "        sampler = torch.utils.data.SubsetRandomSampler(\n",
    "                    torch.randint(high=len(dataset), size=(num_samples,))),\n",
    "        )\n",
    "    print('dataloader len: ', len(dataloader))\n",
    "    if pretrained_model != None:\n",
    "        evaluator = FashionbertEvaluator.from_pretrained(pretrained_model, return_dict=True)\n",
    "    else:\n",
    "        evaluator = FashionbertEvaluator.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "    \n",
    "    evaluator.to(device)\n",
    "    evaluator.eval()\n",
    "\n",
    "    query_dict_im2txt = {}\n",
    "    query_dict_txt2im = {}\n",
    "    running_acc_alignment_im2txt = 0.0\n",
    "    running_acc_pred_im2txt = 0.0\n",
    "    running_acc_alignment_txt2im = 0.0\n",
    "    running_acc_pred_txt2im = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (patches, neg_patches, input_ids, is_paired, attention_mask, neg_input_ids, neg_attention_mask, img_name) in enumerate(dataloader):\n",
    "            update_progress(i/len(dataloader))\n",
    "            if i == 2:\n",
    "                break\n",
    "            # ****** Shapes ********\n",
    "            # input_ids shape:     [1, 448]\n",
    "            # neg_input_ids shape: [1, NUM_SAMPLES=100, 448]\n",
    "            # neg_patches:         [1, NUM_SAMPLES=100, 64, 2048]\n",
    "            \n",
    "            # IMAGE 2 TEXT\n",
    "            print('im2text..')\n",
    "            im2txt_query_scores, im2txt_pred_acc, im2txt_alig_acc = image2text(i, patches, neg_patches, input_ids, \n",
    "                                                                                is_paired, attention_mask, \n",
    "                                                                                neg_input_ids, neg_attention_mask,\n",
    "                                                                                evaluator)\n",
    "            print('done')\n",
    "            # Accuracies \n",
    "            running_acc_pred_im2txt += im2txt_pred_acc\n",
    "            running_acc_alignment_im2txt += im2txt_alig_acc\n",
    "            \n",
    "            # For Rank @ K\n",
    "            query_dict_im2txt[img_name[0]] = im2txt_query_scores\n",
    "            \n",
    "            \n",
    "            # TEXT 2 IMAGE\n",
    "            print('txt 2 img..')\n",
    "            txt2im_query_scores, txt2im_pred_acc, txt2im_alig_acc = text2image(i, patches, neg_patches, input_ids, \n",
    "                                                                                is_paired, attention_mask, \n",
    "                                                                                neg_input_ids, neg_attention_mask,\n",
    "                                                                                evaluator)\n",
    "            print('done')\n",
    "            # Accuracies \n",
    "            running_acc_pred_txt2im += txt2im_pred_acc\n",
    "            running_acc_alignment_txt2im += txt2im_alig_acc\n",
    "            \n",
    "            # For Rank @ K\n",
    "            query_dict_txt2im[img_name[0]] = txt2im_query_scores\n",
    "   \n",
    "\n",
    "    im2txt_test_set_accuracy_pred = (running_acc_pred_im2txt / len(dataloader))\n",
    "    im2txt_test_set_accuracy_alig = (running_acc_alignment_im2txt / len(dataloader))\n",
    "    txt2im_test_set_accuracy_pred = (running_acc_pred_txt2im / len(dataloader))\n",
    "    txt2im_test_set_accuracy_alig = (running_acc_alignment_txt2im / len(dataloader))\n",
    "    \n",
    "    print()\n",
    "    print('---- IMAGE 2 TEXT EVALUATIONS ---------------------')\n",
    "    evaluator.rank_at_K(query_dict_im2txt, save_file_name + 'IM2T_test.json', True)\n",
    "    print('---- Accuracy in token predictions: {} -----'.format(im2txt_test_set_accuracy_pred))\n",
    "    print('---- Accuracy in text-image alignment: {} -----'.format(im2txt_test_set_accuracy_alig))\n",
    "    print()\n",
    "    print('---- TEXT 2 IMAGE EVALUATIONS ---------------------')\n",
    "    evaluator.rank_at_K(query_dict_txt2im, save_file_name + 'T2IM_test.json', False)\n",
    "    print('---- Accuracy in token predictions: {} -----'.format(txt2im_test_set_accuracy_pred))\n",
    "    print('---- Accuracy in text-image alignment: {} -----'.format(txt2im_test_set_accuracy_alig))\n",
    "    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='Evaluate FashionBert.')\n",
    "#     parser.add_argument('--path_to_images', help='Path to images folder')\n",
    "#     parser.add_argument('--path_to_dict_pairs', help='Path to ADARI furniture dict .json file')\n",
    "#     parser.add_argument('--num_subsampler', help='Number subsampler int', default=10)\n",
    "#     parser.add_argument('--path_to_pretrained_model', help='Path to pretrained model', default=None)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     print('Processing the dataset...')\n",
    "#     dataset = PreprocessedADARI_evaluation(args.path_to_images, args.path_to_dict_pairs)\n",
    "#     print('Starting evaluation...')\n",
    "#     test(dataset, device, args.num_neg_samples, args.path_to_pretrained_model)\n",
    "#     print('Done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PreprocessedADARI_evaluation('../../../preprocess_adari_evaluation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [--------------------] 0.0%\n",
      "im2text..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-f85c28cde5fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AAAA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../../__fashionbert_vanilla_adaptive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-177-f1ae9fb148d1>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(dataset, device, num_samples, save_file_name, pretrained_model)\u001b[0m\n\u001b[1;32m    140\u001b[0m                                                                                 \u001b[0mis_paired\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                                                                                 \u001b[0mneg_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                                                                                 evaluator)\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-177-f1ae9fb148d1>\u001b[0m in \u001b[0;36mimage2text\u001b[0;34m(i, patches, neg_patches, input_ids, is_paired, attention_mask, neg_input_ids, neg_attention_mask, evaluator)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0minput_ids_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_neg_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0membeds_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_embeds_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 att_mask_n = all_att_mask)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Accuracy: only in positive example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-4d730a6b5348>\u001b[0m in \u001b[0;36mimg2text_scores\u001b[0;34m(self, input_ids_p, embeds_p, att_mask_p, input_ids_n, embeds_n, att_mask_n)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mis_paired\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0monly_alignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-4d730a6b5348>\u001b[0m in \u001b[0;36mget_scores_and_metrics\u001b[0;34m(self, embeds, attention_mask, labels, is_paired, only_alignment)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m \u001b[0;31m# [batch, seq_length, hidden_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mpooler_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m      \u001b[0;31m#  [batch_size, hidden_size] last layer of hidden-state of first token (CLS) + linear layer + tanh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m         )\n\u001b[1;32m    850\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m                 )\n\u001b[1;32m    485\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         )\n\u001b[1;32m    341\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(dataset, device, 5, 'AAAA','../../../__fashionbert_vanilla_adaptive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### fashionbert vanilla adaptive loss\n",
    "---- IMAGE 2 TEXT EVALUATIONS ---------------------\n",
    "------ Image 2 Text ------\n",
    "------ Rank @ 1 = 0.0167973124300112 ------\n",
    "------ Rank @ 5 = 0.05263157894736842 ------\n",
    "------ Rank @ 10 = 0.10974244120940649 ------\n",
    "---- Accuracy in token predictions: 0.6655357142857142 -----\n",
    "---- Accuracy in text-image alignment: 0.569 -----\n",
    "\n",
    "---- TEXT 2 IMAGE EVALUATIONS ---------------------\n",
    "------ Text 2 Image ------\n",
    "------ Rank @ 1 = 0.005599104143337066 ------\n",
    "------ Rank @ 5 = 0.04927211646136618 ------\n",
    "------ Rank @ 10 = 0.11086226203807391 ------\n",
    "---- Accuracy in token predictions: 0.6648794642857144 -----\n",
    "---- Accuracy in text-image alignment: 0.582 -----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### fashionbert vanilla \n",
    "---- IMAGE 2 TEXT EVALUATIONS ---------------------\n",
    "------ Image 2 Text ------\n",
    "------ Rank @ 1 = 0.010078387458006719 ------\n",
    "------ Rank @ 5 = 0.051511758118701005 ------\n",
    "------ Rank @ 10 = 0.0929451287793953 ------\n",
    "---- Accuracy in token predictions: 0.6576250000000009 -----\n",
    "---- Accuracy in text-image alignment: 0.385 -----\n",
    "\n",
    "---- TEXT 2 IMAGE EVALUATIONS ---------------------\n",
    "------ Text 2 Image ------\n",
    "------ Rank @ 1 = 0.006718924972004479 ------\n",
    "------ Rank @ 5 = 0.04591265397536394 ------\n",
    "------ Rank @ 10 = 0.08286674132138858 ------\n",
    "---- Accuracy in token predictions: 0.6571964285714301 -----\n",
    "---- Accuracy in text-image alignment: 0.379 -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasionbert_vanilla_adaptive_loss \n",
    "---- IMAGE 2 TEXT EVALUATIONS ---------------------\n",
    "\n",
    "------ Image 2 Text ------\n",
    "------ Rank @ 1 = 0.013437849944008958 ------\n",
    "------ Rank @ 5 = 0.060470324748040316 ------\n",
    "------ Rank @ 10 = 0.10414333706606943 ------\n",
    "\n",
    "---- Accuracy in token predictions: 0.7328080357142843 -----\n",
    "---- Accuracy in text-image alignment: 0.552 -----\n",
    "\n",
    "\n",
    "------ Text 2 Image ------\n",
    "\n",
    "---- TEXT 2 IMAGE EVALUATIONS ---------------------\n",
    "------ Rank @ 1 = 0.004479283314669653 ------\n",
    "------ Rank @ 5 = 0.055991041433370664 ------\n",
    "------ Rank @ 10 = 0.09742441209406495 ------\n",
    "\n",
    "---- Accuracy in token predictions: 0.7328080357142843 -----\n",
    "---- Accuracy in text-image alignment: 0.552 -----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fashionbert_vanilla\n",
    "---- IMAGE 2 TEXT EVALUATIONS ---------------------\n",
    "\n",
    "------ Image 2 Text ------\n",
    "------ Rank @ 1 = 0.012318029115341545 ------\n",
    "------ Rank @ 5 = 0.0593505039193729 ------\n",
    "------ Rank @ 10 = 0.10302351623740201 ------\n",
    "\n",
    "---- Accuracy in token predictions: 0.7242946428571431 -----\n",
    "---- Accuracy in text-image alignment: 0.031 -----\n",
    "\n",
    "---- TEXT 2 IMAGE EVALUATIONS ---------------------\n",
    "------ Rank @ 1 = 0.005599104143337066 ------\n",
    "------ Rank @ 5 = 0.051511758118701005 ------\n",
    "------ Rank @ 10 = 0.09630459126539753 ------\n",
    "\n",
    "---- Accuracy in token predictions: 0.7242946428571431 -----\n",
    "---- Accuracy in text-image alignment: 0.031 -----\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
