{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# from transformers.models.bert.modeling_bert import BertPreTrainingHeads\n",
    "from transformers.modeling_bert import BertPreTrainingHeads\n",
    "from utils import construct_bert_input, PreprocessedADARI_evaluation, save_json\n",
    "\n",
    "from fashionbert_model import FashionBert, FashionBertHead\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(torch.nn.Module):     \n",
    "    def __init__(self, pretrained_model=None):\n",
    "        super(Evaluator, self).__init__()\n",
    "        \n",
    "        if pretrained_model != None:\n",
    "            print('-- Loading fashionbert_pretrained model: {}'.format(pretrained_model))\n",
    "            fashion_bert = FashionBert.from_pretrained(pretrained_model, return_dict=True)\n",
    "        else:\n",
    "            fashion_bert = FashionBert.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        \n",
    "        self.model = fashion_bert\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    \n",
    "    def text2img_scores(self,\n",
    "                        input_ids,\n",
    "                        embeds,\n",
    "                        att_mask,\n",
    "                        embeds_n,    # list \n",
    "                        att_mask_n,    # list \n",
    "                        ):               \n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            input_ids     [1, 448]\n",
    "            embeds:       [1, 512, 768]\n",
    "            att_mask:     [1, 448]\n",
    "            embeds_n:     list with 100 of [1, 512, 768]\n",
    "            att_mask_n:   list with 100 of [1, 448]\n",
    "        \"\"\"\n",
    "        # Score for positive \n",
    "        query_dict_scores = []\n",
    "        query_scores = []\n",
    "        query_labels = []\n",
    "        \n",
    "        score_pos = self.get_scores_and_metrics(\n",
    "            embeds=embeds.to(device), \n",
    "            attention_mask=att_mask.to(device), \n",
    "            labels=input_ids.to(device),  \n",
    "            is_paired=torch.tensor(True).to(device),\n",
    "            only_alignment=True,\n",
    "            )\n",
    "        \n",
    "        #label = score_pos[1]\n",
    "        score_p = score_pos[0].squeeze()\n",
    "        score_p = score_p[1].detach().item() # confidence that is actually positive\n",
    "        score_pos_dict = {'text': input_ids,\n",
    "                         'score': score_p,\n",
    "                         'label': True}\n",
    "        query_dict_scores.append(score_pos_dict)\n",
    "        query_scores.append(score_p)\n",
    "        query_labels.append(True)\n",
    "        \n",
    "        # Scores for negative\n",
    "        for n in range(len(embeds_n)):\n",
    "            score_neg = self.get_scores_and_metrics(\n",
    "                        embeds=embeds_n[n].to(device), \n",
    "                        attention_mask=att_mask_n[n].to(device), \n",
    "                        labels=input_ids.to(device), \n",
    "                        is_paired= torch.tensor(False).to(device),\n",
    "                        only_alignment = True,\n",
    "                        )\n",
    "            \n",
    "            score_n  = score_neg[0].squeeze()\n",
    "            score_n  = score_n[1].detach().item() # confidence that is actually positive\n",
    "            score_neg_dict = {'text': input_ids,\n",
    "                             'score': score_n,\n",
    "                             'label': False}\n",
    "            \n",
    "            query_dict_scores.append(score_neg_dict)\n",
    "            query_scores.append(score_n)\n",
    "            query_labels.append(False)\n",
    "        \n",
    "        #print(evaluator.tokenizer.convert_ids_to_tokens(ids))\n",
    "        S = [(s,l) for s, l in sorted(zip(query_scores, query_labels), key=lambda x: x[0], reverse=True)]\n",
    "        return S\n",
    "    \n",
    "    def img2text_scores(self, input_ids_p, embeds_p, att_mask_p, input_ids_n, embeds_n, att_mask_n):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            input_ids_p : [1, 448]\n",
    "            embeds_p:     [1, 512, 768]\n",
    "            att_mask_p:   [1, 448]\n",
    "            input_ids_n:  list with 100 of [1, 448]\n",
    "            embeds_n:     list with 100 of [1, 512, 768]\n",
    "            att_mask_n:   list with 100 of [1, 448]\n",
    "        \"\"\"\n",
    "        # Score for positive \n",
    "        query_dict_scores = []\n",
    "        query_scores = []\n",
    "        query_labels = []\n",
    "        \n",
    "        score_pos = self.get_scores_and_metrics(\n",
    "            embeds=embeds_p.to(device), \n",
    "            attention_mask=att_mask_p.to(device), \n",
    "            labels=input_ids_p.to(device), \n",
    "            is_paired=torch.tensor(True).to(device),\n",
    "            only_alignment=True,\n",
    "            )\n",
    "        \n",
    "        #label = score_pos[1]\n",
    "        score_p = score_pos[0].squeeze()\n",
    "        score_p = score_p[1].detach().item() # confidence that is actually positive\n",
    "        score_pos_dict = {'text': input_ids_p,\n",
    "                         'score': score_p,\n",
    "                         'label': True}\n",
    "        query_dict_scores.append(score_pos_dict)\n",
    "        query_scores.append(score_p)\n",
    "        query_labels.append(True)\n",
    "        \n",
    "        # Scores for negative\n",
    "        for n in range(len(embeds_n)):\n",
    "            score_neg = self.get_scores_and_metrics(\n",
    "                embeds=embeds_n[n].to(device), \n",
    "                attention_mask=att_mask_n[n].to(device), \n",
    "                labels=input_ids_n[n].to(device), \n",
    "                is_paired= torch.tensor(False).to(device),\n",
    "                only_alignment = True,\n",
    "                )\n",
    "            \n",
    "            score_n  = score_neg[0].squeeze()\n",
    "            score_n  = score_n[1].detach().item() # confidence that is actually positive\n",
    "            score_neg_dict = {'text': input_ids_n[n],\n",
    "                             'score': score_n,\n",
    "                             'label': False}\n",
    "            \n",
    "            query_dict_scores.append(score_neg_dict)\n",
    "            query_scores.append(score_n)\n",
    "            query_labels.append(False)\n",
    "        \n",
    "        #print(evaluator.tokenizer.convert_ids_to_tokens(ids))\n",
    "        S = [(s,l) for s, l in sorted(zip(query_scores, query_labels), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "        return S\n",
    "        \n",
    "    def rank_at_K(self, dict_scores, img2text=True):\n",
    "        if img2text:\n",
    "            print('------ Image 2 Text ------')\n",
    "        else:\n",
    "            print('------ Text 2 Image ------')\n",
    "        \n",
    "        Ks = [1, 5, 10]\n",
    "        for K in Ks:\n",
    "            #print('------ Rank @ {} ------'.format(K))\n",
    "            found = 0\n",
    "            for key, val in dict_scores.items():\n",
    "                tmp_range = K if K < len(val) else len(val)\n",
    "                for i in range(tmp_range):\n",
    "                    score, label = val[i]\n",
    "                    if label:\n",
    "                        found += 1\n",
    "                        break\n",
    "            print('------ Rank @ {} = {} ------'.format(K, (found/len(dict_scores.keys()) )))        \n",
    "        \n",
    "    def get_scores_and_metrics(\n",
    "        self,\n",
    "        embeds,                       # text + image embedded \n",
    "        attention_mask,\n",
    "        labels=None,                  # [batch, 448]\n",
    "        is_paired=None,               # [batch]\n",
    "        only_alignment = False,\n",
    "        ):\n",
    "        \n",
    "        batch_size = embeds.shape[0]\n",
    "        seq_length = embeds.shape[1]\n",
    "        hidden_dim = embeds.shape[2]\n",
    "\n",
    "        outputs = self.model.bert(inputs_embeds=embeds, return_dict=True)\n",
    "        sequence_output = outputs.last_hidden_state # [batch, seq_length, hidden_size]\n",
    "        pooler_output = outputs.pooler_output      #  [batch_size, hidden_size] last layer of hidden-state of first token (CLS) + linear layer + tanh\n",
    "\n",
    "        # hidden states corresponding to the text part\n",
    "        text_output = sequence_output[:, :labels.shape[1], :]  # [batch, 448, 768]\n",
    "        # hidden states corresponding to the image part\n",
    "        image_output = sequence_output[:, labels.shape[1]:, :] # [batch, 64, 768]\n",
    "\n",
    "        ### FOR TEXT \n",
    "        # Predict the masked text tokens and alignment scores (whether image and text match)\n",
    "        prediction_scores, alignment_scores = self.model.cls(text_output, pooler_output)\n",
    "        # prediction score is [batch, 448, vocab_size = 30522]\n",
    "        # aligment score is [batch, 2] 2 with logits corresponding to 1 and  0\n",
    "        \n",
    "        if only_alignment:\n",
    "            return alignment_scores, is_paired.double().detach().item()\n",
    "        \n",
    "        text_evaluator = {'text_pred_logits': prediction_scores, \n",
    "                         'text_labels': labels}\n",
    "        \n",
    "        alignment_evaluator = {'alignment_logits': alignment_scores,\n",
    "                              'alignment_labels': is_paired}\n",
    "        \n",
    "        text_acc, alig_acc = self.accuracy_scores(text_evaluator, alignment_evaluator)\n",
    "        return text_acc, alig_acc\n",
    "    \n",
    "    def accuracy_scores(self, text_evaluator, alignment_evaluator):\n",
    "        \"\"\"\n",
    "        Text evaluator:  dictionary with preds and labels (aligned)\n",
    "        Image evaluator: dictionary with image output and image patches (aligned)\n",
    "        \"\"\"\n",
    "        # Text\n",
    "        text_pred_logits = text_evaluator['text_pred_logits'] # [num_aligned, 448, vocab_size]\n",
    "        text_labels = text_evaluator['text_labels']           # [num_aligned, 448]\n",
    "        \n",
    "        text_preds_logits = text_pred_logits.detach().cpu().numpy()\n",
    "        text_labels = text_labels.cpu().numpy().flatten()\n",
    "        text_preds = np.argmax(text_preds_logits, axis=2).flatten() # [num_algined, 448]\n",
    "        \n",
    "        # Alignment\n",
    "        alig_pred_logits = alignment_evaluator['alignment_logits']      # [1, 2]\n",
    "        alig_labels = alignment_evaluator['alignment_labels']           # [2]\n",
    "        \n",
    "        alig_pred_logits = alig_pred_logits.detach().cpu().numpy()\n",
    "        alig_labels = alig_labels.double().cpu().numpy().flatten()\n",
    "        alig_preds = np.argmax(alig_pred_logits, axis=1).flatten() # [1, 2]\n",
    "        \n",
    "        text_acc = accuracy_score(text_labels, text_preds)\n",
    "        alig_acc = accuracy_score(alig_labels, alig_preds)\n",
    "        \n",
    "        return text_acc, alig_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2text(i, patches, neg_patches, input_ids, is_paired, attention_mask, neg_input_ids, neg_attention_mask, evaluator):\n",
    "    \"\"\"\n",
    "    image2text retrieval: \n",
    "        Query = Image\n",
    "        Paired with: 1 positive text, 100 negative texts\n",
    "    \"\"\"\n",
    "    im_seq_len = patches.shape[1]\n",
    "    bs = input_ids.shape[0]\n",
    "    len_neg_inputs = neg_input_ids.shape[1]\n",
    "\n",
    "    # PAIRED PATCHES\n",
    "    # mask image patches with prob 10%\n",
    "    masked_patches = patches.detach().clone()\n",
    "    masked_patches = masked_patches.view(-1, patches.shape[2])\n",
    "    im_mask = torch.rand((masked_patches.shape[0], 1)) >= 0.1\n",
    "    masked_patches *= im_mask\n",
    "\n",
    "    try:\n",
    "        masked_patches = masked_patches.view(bs, im_seq_len, patches.shape[2])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"masked_patches: {masked_patches.shape}\")\n",
    "        print(f\"im_mask: {im_mask.shape}\")\n",
    "        print(f\"patches: {patches.shape}\")\n",
    "        return\n",
    "        \n",
    "    # Mask tokens with prob 15%, note id 103 is the [MASK] token\n",
    "    token_mask = torch.rand(input_ids.shape)\n",
    "    masked_input_ids = input_ids.detach().clone()\n",
    "    masked_input_ids[token_mask < 0.15] = 103\n",
    "\n",
    "    embeds = construct_bert_input(masked_patches, masked_input_ids, evaluator.model, device=device)\n",
    "    # pad attention mask with 1s so model pays attention to the image parts\n",
    "    attention_mask = F.pad(attention_mask, (0, embeds.shape[1] - input_ids.shape[1]), value = 1)\n",
    "\n",
    "    # NEGATIVE SAMPLE # [batch, 100, 448]\n",
    "    all_embeds_neg = []\n",
    "    all_att_mask = []\n",
    "    all_neg_inputs = []\n",
    "\n",
    "    for j in range(len_neg_inputs):\n",
    "        neg_input_id_sample = neg_input_ids[:, j, :] # [1, 448]\n",
    "        neg_attention_mask_sample = neg_attention_mask[:, j, :]\n",
    "\n",
    "        token_mask = torch.rand(neg_input_id_sample.shape)\n",
    "        masked_input_ids = neg_input_id_sample.detach().clone()\n",
    "        masked_input_ids[token_mask < 0.15] = 103\n",
    "\n",
    "        embeds_neg = construct_bert_input(masked_patches, masked_input_ids, evaluator.model, device=device)\n",
    "        attention_mask_neg = F.pad(neg_attention_mask_sample, (0, embeds_neg.shape[1] - neg_input_id_sample.shape[1]), value = 1)\n",
    "\n",
    "        all_embeds_neg.append(embeds_neg)\n",
    "        all_att_mask.append(attention_mask_neg)\n",
    "        all_neg_inputs.append(neg_input_id_sample.detach())\n",
    "\n",
    "    # Now I have all joint embeddings for 1 positive sample and 100 neg samples\n",
    "    all_scores_query = evaluator.img2text_scores(\n",
    "                input_ids_p = input_ids,\n",
    "                embeds_p = embeds,\n",
    "                att_mask_p = attention_mask,\n",
    "                input_ids_n = all_neg_inputs,\n",
    "                embeds_n = all_embeds_neg,\n",
    "                att_mask_n = all_att_mask)\n",
    "\n",
    "    # Accuracy: only in positive example\n",
    "    txt_acc, alig_acc = evaluator.get_scores_and_metrics(\n",
    "                        embeds,                       # text + image embedded \n",
    "                        attention_mask,\n",
    "                        labels=input_ids,                  # [batch, 448]\n",
    "                        is_paired=is_paired,               # [batch]\n",
    "                        only_alignment = False,\n",
    "                        )\n",
    "\n",
    "    return all_scores_query, txt_acc, alig_acc\n",
    "    \n",
    "    \n",
    "def text2image(i, patches, neg_patches, input_ids, is_paired, attention_mask, neg_input_ids, neg_attention_mask, evaluator):\n",
    "    \"\"\"\n",
    "    text2image retrieval: \n",
    "        Query = Text\n",
    "        Paired with: 1 positive image, 100 negative images\n",
    "    \"\"\"    \n",
    "    im_seq_len = patches.shape[1]\n",
    "    bs = input_ids.shape[0]\n",
    "    len_neg_inputs = neg_input_ids.shape[1]\n",
    "    \n",
    "    # PAIRED PATCHES\n",
    "    # mask image patches with prob 10%\n",
    "    masked_patches = patches.detach().clone()\n",
    "    masked_patches = masked_patches.view(-1, patches.shape[2])\n",
    "    im_mask = torch.rand((masked_patches.shape[0], 1)) >= 0.1\n",
    "    masked_patches *= im_mask\n",
    "\n",
    "    try:\n",
    "        masked_patches = masked_patches.view(bs, im_seq_len, patches.shape[2])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"masked_patches: {masked_patches.shape}\")\n",
    "        print(f\"im_mask: {im_mask.shape}\")\n",
    "        print(f\"patches: {patches.shape}\")\n",
    "        return\n",
    "\n",
    "    # UNPAIRED PATCHES\n",
    "    neg_masked_patches = neg_patches.detach().clone()\n",
    "    neg_masked_patches = neg_masked_patches.view(-1, neg_patches.shape[3])\n",
    "    neg_im_mask = torch.rand((neg_masked_patches.shape[0], 1)) >= 0.1\n",
    "    neg_masked_patches *= neg_im_mask\n",
    "\n",
    "    try:\n",
    "        neg_masked_patches = neg_masked_patches.view(bs, len_neg_inputs, im_seq_len, patches.shape[2])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"masked_patches: {neg_masked_patches.shape}\")\n",
    "        print(f\"im_mask: {neg_im_mask.shape}\")\n",
    "        print(f\"patches: {neg_patches.shape}\")\n",
    "        return\n",
    "        \n",
    "    # Mask tokens with prob 15%, note id 103 is the [MASK] token\n",
    "    token_mask = torch.rand(input_ids.shape)\n",
    "    masked_input_ids = input_ids.detach().clone()\n",
    "    masked_input_ids[token_mask < 0.15] = 103\n",
    "    \n",
    "    # POSITIVE IMAGE \n",
    "    embeds = construct_bert_input(masked_patches, masked_input_ids, evaluator.model, device=device)\n",
    "    attention_mask = F.pad(attention_mask, (0, embeds.shape[1] - input_ids.shape[1]), value = 1)\n",
    "        \n",
    "    # NEGATIVE SAMPLES\n",
    "    all_embeds_neg = []\n",
    "    all_att_mask = []\n",
    "    \n",
    "    for p in range(len_neg_inputs):\n",
    "        neg_masked_patches_sample = neg_masked_patches[:, p, :, :]\n",
    "        embeds_neg = construct_bert_input(neg_masked_patches_sample, masked_input_ids, evaluator.model, device=device)\n",
    "        attention_mask_neg = F.pad(attention_mask, (0, embeds_neg.shape[1] - input_ids.shape[1]), value = 1)\n",
    "\n",
    "        all_embeds_neg.append(embeds_neg)\n",
    "        all_att_mask.append(attention_mask_neg)\n",
    "\n",
    "    \n",
    "    # Now I have all joint embeddings for 1 positive sample and 100 neg samples\n",
    "    all_scores_query = evaluator.text2img_scores(\n",
    "                input_ids   = input_ids,\n",
    "                embeds      = embeds,\n",
    "                att_mask    = attention_mask,\n",
    "                embeds_n    = all_embeds_neg, # list\n",
    "                att_mask_n  = all_att_mask) # list\n",
    "              \n",
    "    \n",
    "    # Accuracy: only in positive example\n",
    "    txt_acc, alig_acc = evaluator.get_scores_and_metrics(\n",
    "                        embeds,                       # text + image embedded \n",
    "                        attention_mask,\n",
    "                        labels=input_ids,                  # [batch, 448]\n",
    "                        is_paired=is_paired,               # [batch]\n",
    "                        only_alignment = False,\n",
    "                        )\n",
    "    \n",
    "    return all_scores_query, txt_acc, alig_acc\n",
    "    \n",
    "    \n",
    "def test(dataset, device, num_samples, pretrained_model=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(0)    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=False,\n",
    "        sampler = torch.utils.data.SubsetRandomSampler(\n",
    "                    torch.randint(high=len(dataset), size=(num_samples,))),\n",
    "        )\n",
    "    print('dataloader len: ', len(dataloader))\n",
    "    if pretrained_model != None:\n",
    "        evaluator = Evaluator(pretrained_model)\n",
    "    else:\n",
    "        evaluator = Evaluator()\n",
    "    \n",
    "    evaluator.model.to(device)\n",
    "    evaluator.model.eval()\n",
    "\n",
    "    query_dict_im2txt = {}\n",
    "    query_dict_txt2im = {}\n",
    "    running_acc_alignment_im2txt = 0.0\n",
    "    running_acc_pred_im2txt = 0.0\n",
    "    running_acc_alignment_txt2im = 0.0\n",
    "    running_acc_pred_txt2im = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (patches, neg_patches, input_ids, is_paired, attention_mask, neg_input_ids, neg_attention_mask, img_name) in enumerate(dataloader):\n",
    "            update_progress(i/len(dataloader))\n",
    "            if i == 10:\n",
    "                break\n",
    "            # ****** Shapes ********\n",
    "            # input_ids shape:     [1, 448]\n",
    "            # neg_input_ids shape: [1, NUM_SAMPLES=100, 448]\n",
    "            # neg_patches:         [1, NUM_SAMPLES=100, 64, 2048]\n",
    "            \n",
    "            # IMAGE 2 TEXT\n",
    "            print('im2text..')\n",
    "            im2txt_query_scores, im2txt_pred_acc, im2txt_alig_acc = image2text(i, patches, neg_patches, input_ids, \n",
    "                                                                                is_paired, attention_mask, \n",
    "                                                                                neg_input_ids, neg_attention_mask,\n",
    "                                                                                evaluator)\n",
    "            print('done')\n",
    "            # Accuracies \n",
    "            running_acc_pred_im2txt += im2txt_pred_acc\n",
    "            running_acc_alignment_im2txt += im2txt_alig_acc\n",
    "            \n",
    "            # For Rank @ K\n",
    "            query_dict_im2txt[img_name[0]] = im2txt_query_scores\n",
    "            \n",
    "            \n",
    "            # TEXT 2 IMAGE\n",
    "            print('txt 2 img..')\n",
    "            txt2im_query_scores, txt2im_pred_acc, txt2im_alig_acc = text2image(i, patches, neg_patches, input_ids, \n",
    "                                                                                is_paired, attention_mask, \n",
    "                                                                                neg_input_ids, neg_attention_mask,\n",
    "                                                                                evaluator)\n",
    "            print('done')\n",
    "            # Accuracies \n",
    "            running_acc_pred_txt2im += txt2im_pred_acc\n",
    "            running_acc_alignment_txt2im += txt2im_alig_acc\n",
    "            \n",
    "            # For Rank @ K\n",
    "            query_dict_txt2im[img_name[0]] = txt2im_query_scores\n",
    "   \n",
    "\n",
    "    im2txt_test_set_accuracy_pred = (running_acc_pred_im2txt / len(dataloader))\n",
    "    im2txt_test_set_accuracy_alig = (running_acc_alignment_im2txt / len(dataloader))\n",
    "    txt2im_test_set_accuracy_pred = (running_acc_pred_txt2im / len(dataloader))\n",
    "    txt2im_test_set_accuracy_alig = (running_acc_alignment_txt2im / len(dataloader))\n",
    "    \n",
    "    print()\n",
    "    print('---- IMAGE 2 TEXT EVALUATIONS ---------------------')\n",
    "    evaluator.rank_at_K(query_dict_im2txt, True)\n",
    "    print('---- Accuracy in token predictions: {} -----'.format(im2txt_test_set_accuracy_pred))\n",
    "    print('---- Accuracy in text-image alignment: {} -----'.format(im2txt_test_set_accuracy_alig))\n",
    "    print()\n",
    "    print('---- TEXT 2 IMAGE EVALUATIONS ---------------------')\n",
    "    evaluator.rank_at_K(query_dict_txt2im, False)\n",
    "    print('---- Accuracy in token predictions: {} -----'.format(txt2im_test_set_accuracy_pred))\n",
    "    print('---- Accuracy in text-image alignment: {} -----'.format(txt2im_test_set_accuracy_alig))\n",
    "    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='Evaluate FashionBert.')\n",
    "#     parser.add_argument('--path_to_images', help='Path to images folder')\n",
    "#     parser.add_argument('--path_to_dict_pairs', help='Path to ADARI furniture dict .json file')\n",
    "#     parser.add_argument('--num_subsampler', help='Number subsampler int', default=10)\n",
    "#     parser.add_argument('--path_to_pretrained_model', help='Path to pretrained model', default=None)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     print('Processing the dataset...')\n",
    "#     dataset = PreprocessedADARI_evaluation(args.path_to_images, args.path_to_dict_pairs)\n",
    "#     print('Starting evaluation...')\n",
    "#     test(dataset, device, args.num_neg_samples, args.path_to_pretrained_model)\n",
    "#     print('Done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PreprocessedADARI_evaluation('../../../preprocess_adari_evaluation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [--------------------] 0.6%\n",
      "im2text..\n"
     ]
    }
   ],
   "source": [
    "test(dataset, device, 1000, '../../../__fashionbert_vanilla_adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
