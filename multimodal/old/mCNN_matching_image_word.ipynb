{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import io\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import pdb\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar to visualize progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES\n",
    "im_path_fur = '/home/ubuntu/ADARI/images/v2/full'\n",
    "# im_path_fur = \"../images/furniture/v2/full\" # small are 64x64, medium 256x256 and large 512x512\n",
    "\n",
    "# JSON_FILES\n",
    "# data_path_fur = \"../ADARI/json_files/cleaned/ADARI_v2/furniture_v2_c.json\"\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "word_embeddings_path = \"../json_files/fur_5c_50d_sk_glove_ft.json\"\n",
    "\n",
    "# IMAGE EMBEDDINGS\n",
    "img_embds_id_p = \"../json_files/afur_resnet_emb_id.json\"\n",
    "img_embds_name_p = \"../json_files/afur_resnet_emb_names.json\"\n",
    "\n",
    "# FILES FOR DATALOADER\n",
    "dset_words_p = \"../json_files/ADARI_v2_furniture_images_words.json\"\n",
    "# dset_sentences_p = \"../ADARI/json_files/ADARI_images_sentences_words/furniture/ADARI_v2_furniture_images_sentences.json\"\n",
    "# dset_sentences_POS_p = \"../ADARI/json_files/ADARI_images_sentences_words/furniture/ADARI_v2_furniture_images_sentences_tokenized.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open json files with embeddings \n",
    "image_embeddings = open_json(img_embds_name_p)\n",
    "dataset_labels = open_json(dset_words_p)\n",
    "labels_embeddings = open_json(word_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dictionary of ordered labels to list of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../json_files/glove.6B.50d.txt'\n",
    "with io.open(glove_path, 'r', encoding='utf8') as f:    \n",
    "    glove_file = f.read()\n",
    "    \n",
    "glove_sentences = glove_file.splitlines()\n",
    "glove_vocab = {}\n",
    "for sentence in glove_sentences:\n",
    "    word = sentence.split()[0]\n",
    "    embedding = np.array(sentence.split()[1:], dtype = float)\n",
    "    glove_vocab[word] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nasty temporal vector for unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(glove_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        pass\n",
    "n_vec = i + 1\n",
    "hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "with open(glove_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "AVG_VECTOR = np.mean(vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date(word):\n",
    "    rx = r\"[0-9]+(?:st|[nr]d|th)\"\n",
    "    if re.findall(rx, word, flags=re.I) != []:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def labels_dict2list(dset_words):\n",
    "    # The 2 dictionaries below for dataset dataloader\n",
    "    im2idx = dict()\n",
    "    im_words = dict()\n",
    "\n",
    "    # Temp lists \n",
    "    image_names = list(dset_words.keys())\n",
    "    words = list(dset_words.values())\n",
    "\n",
    "    # Iterate over length of dictionary and get im2idx and im_words \n",
    "    for i in range(len(image_names)):\n",
    "        im = image_names[i]\n",
    "        words_list = flatten(list(words[i].values()))\n",
    "        cleaned_w = []\n",
    "        for w in words_list:\n",
    "            if w != '\"the' and w != '\"The' and len(w) > 1 and is_date(w) != True:\n",
    "                cleaned_w.append(w)\n",
    "\n",
    "#         im_words[im] = cleaned_w\n",
    "        im_words[im] = list(set(cleaned_w))\n",
    "        im2idx[im] = i\n",
    "    return im_words, im2idx\n",
    "\n",
    "def create_vocab(dataset_labels):\n",
    "    \"\"\"\n",
    "    We have 17532 images and a total of 707852 adjectives, so average of 40 words per image\n",
    "    We have a total of 4786 unique words. This is our vocabulary size\n",
    "    \"\"\"\n",
    "    # 1) Convert raw dataset (dictionary of ordered labels per image) to list of labels\n",
    "    dset_im_words, _ = labels_dict2list(dataset_labels)\n",
    "    \n",
    "    # 2) Flatten the list \n",
    "    all_words = list(dset_im_words.values())\n",
    "    flat_list = []\n",
    "    for sublist in all_words:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    \n",
    "    # 3) Get set of unique words = vocabulary\n",
    "    unique_words = set(flat_list)\n",
    "    \n",
    "    # 4) Get dicitonary to map idx to words and viceversa\n",
    "    words2idx = dict()\n",
    "    idx2words = dict()\n",
    "    \n",
    "    set2list = list(unique_words)\n",
    "    for i in range(len(set2list)):\n",
    "        w = set2list[i]\n",
    "        words2idx[w] = i\n",
    "        idx2words[i] = w\n",
    "        \n",
    "    return dset_im_words, words2idx, idx2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_im_words, vocab2idx, idx2vocab = create_vocab(dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDict(d_img_words, d_img_embs, percent, val_number):\n",
    "\n",
    "    val_n = val_number\n",
    "    train_test_size = len(d_img_words) - val_n\n",
    "    train_n = int(train_test_size*percent)\n",
    "    test_n = train_test_size - train_n\n",
    "    \n",
    "    im_words = iter(d_img_words.items())      \n",
    "    im_embs = iter(d_img_embs.items())\n",
    "    \n",
    "    # Image - words\n",
    "    dtrain_imw = dict(itertools.islice(im_words, train_n))  \n",
    "    dtest_imw = dict(itertools.islice(im_words, test_n))   \n",
    "    dval_imw = dict(itertools.islice(im_words, val_n))\n",
    "    \n",
    "    \n",
    "    print('trainset size: ', len(dtrain_imw), 'dataset size: ',len(dtest_imw), 'val set size: ', len(dval_imw))\n",
    "    return dtrain_imw, dtest_imw, dval_imw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size:  16180 dataset size:  852 val set size:  500\n"
     ]
    }
   ],
   "source": [
    "dtrain_w, dtest_w, dval_w = splitDict(dset_im_words,image_embeddings, .95, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "class ADARIdataset(Dataset):\n",
    "    \"\"\"\n",
    "    Receives images and labels.\n",
    "    Returns tensor image and tensor labels\n",
    "    \"\"\"\n",
    "    def __init__(self, data_labels, word_embeddings, image_embeddings, vocab2idx, idx2vocab, img_path):\n",
    "\n",
    "        self.labels_data = data_labels # dictionary of images -> labels\n",
    "        self.word_embeds = word_embeddings\n",
    "        \n",
    "        self.images_names = list(self.labels_data.keys())    # names\n",
    "        self.images_embeds = list(image_embeddings.values()) # values\n",
    "        \n",
    "        self.vocab2idx = vocab2idx\n",
    "        self.idx2vocab = idx2vocab\n",
    "        \n",
    "        self.image_path = img_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_names)\n",
    "    \n",
    "    def name2idx(self):\n",
    "        self.name2idx = dict()\n",
    "        self.idx2name = dict()\n",
    "        for i, key in enumerate(self.images_names.keys()):\n",
    "            self.name2idx[key] = i\n",
    "            self.idx2name[i] = key\n",
    "        \n",
    "    def get_image_tensor(self, image_name):\n",
    "        \"\"\"\n",
    "        Gets image name and returns a tensor\n",
    "        \"\"\"\n",
    "        name = self.image_path + \"/\" + image_name\n",
    "        img = Image.open(name)\n",
    "        img = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor()])(img)\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    def get_labels_embeddings_from_idx(self, idx):\n",
    "\n",
    "        name_image = self.images_names[idx]\n",
    "        labels = self.labels_data[name_image]\n",
    "        labels = list(set(labels))\n",
    "        \n",
    "        # Set random distribution for setting a max number of labels\n",
    "        if len(labels) > 20:\n",
    "            labels = np.random.choice(labels, 20)\n",
    "        \n",
    "        # Get positive and negative labels\n",
    "        all_idx = list(self.vocab2idx.values())\n",
    "        pos_idxs = []\n",
    "        # Remove indexes that correspond to the positive labels\n",
    "        for l in labels:\n",
    "            v2i = self.vocab2idx[l]\n",
    "            pos_idxs.append(v2i)\n",
    "            if v2i in all_idx:\n",
    "                all_idx.remove(v2i)\n",
    "        \n",
    "        # Choose random labels as negative samples -> this can be improved with info about distance of labels\n",
    "        neg_idxs = np.random.choice(all_idx, len(labels))\n",
    "        \n",
    "        neg_samples = []\n",
    "        for n in neg_idxs:\n",
    "            neg_samples.append(self.idx2vocab[n])\n",
    "        \n",
    "        assert(len(labels) == len(neg_samples))\n",
    "        pos_w_embs = []\n",
    "        neg_w_embs = []\n",
    "        \n",
    "        # positive\n",
    "        for l in labels:\n",
    "            try:\n",
    "                pos_w_embs.append(self.word_embeds[l.lower()]) # appending 50 vector embedding\n",
    "            except:\n",
    "                try:\n",
    "                    pos_w_embs.append(glove_vocab[l.lower()])\n",
    "                except:\n",
    "                    pos_w_embs.append(AVG_VECTOR)\n",
    "        # negative\n",
    "        for nl in neg_samples:\n",
    "            try:\n",
    "                neg_w_embs.append(self.word_embeds[nl.lower()]) # appending 50 vector embedding\n",
    "            except:\n",
    "                try:\n",
    "                    neg_w_embs.append(glove_vocab[nl.lower()])\n",
    "                except:\n",
    "                    neg_w_embs.append(AVG_VECTOR)\n",
    "                    \n",
    "                    \n",
    "        return pos_w_embs, neg_w_embs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return tensor image and label embedding\n",
    "        \"\"\"\n",
    "        name_image = self.images_names[index]\n",
    "        img = self.get_image_tensor(name_image)\n",
    "        \n",
    "        #image_emb = self.images_embeds[index] # list size 2048 \n",
    "        pos_label_embs, neg_label_embs = self.get_labels_embeddings_from_idx(index) # list size variable \n",
    "        \n",
    "        return img, pos_label_embs, neg_label_embs\n",
    " \n",
    "\n",
    "def collate(sequence):\n",
    "    \"\"\"\n",
    "    \"the input of this function is the output of function __getitem__\"\n",
    "    \"this gets BATCH_SIZE times GETITEM! \"\n",
    "    if batch_Size == 2 --> sequence is a list with length 2. \n",
    "    Each list is a tuple (image_embedding, labels_embedding) = (2048 vector, list of vectors size 50)\n",
    "    Pad labels with maximum from batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate all images in the batch\n",
    "    # For images (not embeddigns)\n",
    "    images = torch.cat(([torch.FloatTensor(batch[0]).view(-1, 3, 64, 64) for batch in sequence]), dim=0)\n",
    "    # For images embeddings\n",
    "    #images = torch.cat(([torch.FloatTensor(batch[0]).view(-1, 2048) for batch in sequence]), dim=0)\n",
    "    \n",
    "    # Pad labels with max_sequence_label\n",
    "    # batch 1 is batch * word embedding\n",
    "    pos_labels = pad_sequence([torch.FloatTensor(batch[1]) for batch in sequence], batch_first=True)\n",
    "    labels_length = torch.LongTensor([len(batch[1]) for batch in sequence])     \n",
    "\n",
    "    neg_labels = pad_sequence([torch.FloatTensor(batch[2]) for batch in sequence], batch_first=True)\n",
    "    \n",
    "    return images, pos_labels, neg_labels, labels_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ADARIdataset(dtrain_w, labels_embeddings, image_embeddings, vocab2idx, idx2vocab, im_path_fur)\n",
    "dataset_test = ADARIdataset(dtest_w, labels_embeddings, image_embeddings, vocab2idx, idx2vocab, im_path_fur)\n",
    "dataset_val = ADARIdataset(dval_w, labels_embeddings, image_embeddings, vocab2idx, idx2vocab, im_path_fur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 8 if cuda else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, collate_fn = collate, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size, collate_fn = collate,shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, collate_fn = collate,shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# to test dataloader\n",
    "it = iter(test_dataloader)\n",
    "\n",
    "min_ = 10000\n",
    "for i in range(len(test_dataloader)):\n",
    "    first = next(it)\n",
    "    if first[1].shape[1] < min_:\n",
    "        min_ = first[1].shape[1]\n",
    "print(min_)\n",
    "   # print(second[0].shape, second[1].shape, second[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extract = False\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    input_size = 64\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "model_ft, input_size = initialize_model(50, feature_extract)\n",
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2048])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.fc.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTICONVOLUTION CONCATENATING\n",
    "class matchingCNN(nn.Module):\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(matchingCNN, self).__init__()\n",
    "        \n",
    "        # Multimodal Convolution \n",
    "        self.cnn1 = nn.Conv1d(in_channels = 1, out_channels = 200,  kernel_size = 12, padding=0, stride=2)\n",
    "        self.max1 = nn.MaxPool1d(2, stride=2)\n",
    "        \n",
    "        # 2nd Convolution\n",
    "        self.cnn2 = nn.Conv1d(in_channels = 200, out_channels = 300, kernel_size = 12, padding=0, stride=2)\n",
    "        self.max2 = nn.MaxPool1d(2, stride=2)\n",
    "        \n",
    "        # 3rd Convolution\n",
    "        self.cnn3 = nn.Conv1d(in_channels = 300, out_channels = 300, kernel_size = 12, padding=0, stride=2)\n",
    "        self.max3 = nn.MaxPool1d(2, stride=2)\n",
    "                    \n",
    "    def forward(self, image_feature, labels, labels_lengths):\n",
    "        batch_size = labels.shape[0]\n",
    "        max_length_per_batch = labels.shape[1]\n",
    "        dim_labels = labels.shape[2]\n",
    "        dim_image = image_feature.shape[1]\n",
    "        \n",
    "        # Mixing image and labels \n",
    "        new_vectors = [] # [batch, max_length, dim*3+256]\n",
    "        receptive_field_words = 3\n",
    "        new_vector_length = receptive_field_words * (dim_labels + dim_image)\n",
    "        \n",
    "        for i in range(labels.shape[1]): # iterating over words \n",
    "            next3words = labels[:, i:i+3, :] \n",
    "            next3words = next3words.view(batch_size, -1)\n",
    "            words_img = torch.cat((next3words, image_feature), 1).unsqueeze(1) # [batch, 1, 200]\n",
    "            len_last_dim = words_img.shape[2]\n",
    "            \n",
    "            if len_last_dim != new_vector_length:\n",
    "                pad_times = new_vector_length - len_last_dim\n",
    "                words_img = F.pad(words_img, (0, pad_times), \"constant\", 0)\n",
    "            new_vectors.append(words_img)\n",
    "        \n",
    "        joint = torch.cat((new_vectors),1) # [batch, 20, 406]\n",
    "\n",
    "        # Reshaping matrix\n",
    "        joint = joint.view(-1, 1, new_vector_length) # [batch*20, 1, 406]\n",
    "#         print('joint matrix shape: ', joint.shape)\n",
    "        joint = F.relu(F.dropout(self.cnn1(joint), .1))\n",
    "#         print('after first convolution: ', joint.shape)\n",
    "        joint = self.max1(joint)\n",
    "#         print('after maxpool1: ', joint.shape)\n",
    "\n",
    "        joint = F.relu(F.dropout(self.cnn2(joint), .1))\n",
    "#         print('after conv2: ', joint.shape)\n",
    "        joint = self.max2(joint)\n",
    "#         print('after 2nd maxpool: ', joint.shape)\n",
    "        joint = F.relu(F.dropout(self.cnn3(joint), .1))\n",
    "#         print('after conv3: ', joint.shape)\n",
    "        joint = self.max3(joint)\n",
    "#         print('after 3rd maxpool: ', joint.shape)\n",
    "        \n",
    "        joint = joint.view(batch_size, -1)\n",
    "#         print(joint.shape)\n",
    "        return joint\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score_deviseAndKiros(nn.Module):\n",
    "    def __init__(self, image_vector, label_true_vectors, label_random_vector, lengths, cnn_img_weights):\n",
    "        super(Score_deviseAndKiros, self).__init__()\n",
    "        M = cnn_img_weights      # [50, 2048]\n",
    "        v_image = image_vector.T # [batch, 50]\n",
    "        t_label = label_true_vectors   # [batch, 50, max_length]\n",
    "        t_j     = label_random_vectors # [batch, 50, max_length]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scoring_function(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scoring_function, self).__init__()\n",
    "        self.fc1 = nn.Linear(6000, 400)\n",
    "        self.fc2 = nn.Linear(400, 100)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_ranking_loss(margin, score_good, score_bad):\n",
    "    cost = (margin - score_good + score_bad)\n",
    "    z = torch.zeros_like(cost)\n",
    "    loss = torch.max(z, cost)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(vec):\n",
    "    norm = vec.norm(p=2, dim=1, keepdim=True)\n",
    "    vec_norm = vec.div(norm)\n",
    "    return vec_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(img_cnn, matching_cnn, mlp, train_loader, optimizer):\n",
    "    img_cnn.train()\n",
    "    matching_cnn.train()\n",
    "    mlp.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, pos_labels, neg_labels, length) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        \n",
    "        pos_labels = pos_labels.to(device) # [batch, 50, max_length]\n",
    "        neg_labels = neg_labels.to(device) # [batch, 50, max_length]\n",
    "        \n",
    "        # image_CNN\n",
    "        im_repres = img_cnn(data) # [batch, 50]\n",
    "        im_repres = F.relu(im_repres) # [batch, 50]\n",
    "        \n",
    "        # Matching CNN\n",
    "        joint_repre_related = matching_cnn(im_repres, pos_labels, length)   # [batch, 6000]\n",
    "        joint_repre_unrelated = matching_cnn(im_repres, neg_labels, length) # [batch, 6000]\n",
    "        \n",
    "        # Scoring mLP\n",
    "        score_good = mlp(joint_repre_related).mean((1,0))   # [batch, 100] -> [batch]\n",
    "        score_bad = mlp(joint_repre_unrelated).mean((1,0))  # [batch, 100] -> [batch]\n",
    "        \n",
    "        loss = margin_ranking_loss(0.5, score_good, score_bad)\n",
    "\n",
    "        if batch_idx % 10 == 0 and batch_idx != 0:\n",
    "            print('sg: ', score_good.item())\n",
    "            print('sb: ', score_bad.item())\n",
    "            print('loss: ', loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    loss_epoch = running_loss / len(train_loader)\n",
    "    print('------ Training -----')\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(img_cnn, matching_cnn, mlp, test_loader):\n",
    "    img_cnn.eval()\n",
    "    matching_cnn.eval()\n",
    "    mlp.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, pos_labels, neg_labels, length) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            pos_labels = pos_labels.to(device) # all data & model on same device\n",
    "            neg_labels = neg_labels.to(device) \n",
    "\n",
    "            # image_CNN\n",
    "            im_repres = img_cnn(data) # [batch, 50]\n",
    "            im_repres = F.relu(im_repres)\n",
    "\n",
    "            # Matching CNN\n",
    "            joint_repre_related = matching_cnn(im_repres, pos_labels, length)\n",
    "            joint_repre_unrelated = matching_cnn(im_repres, neg_labels, length)\n",
    "\n",
    "            # Scoring mLP\n",
    "            score_good = mlp(joint_repre_related).mean((1,0))\n",
    "            score_bad = mlp(joint_repre_unrelated).mean((1,0))\n",
    "            \n",
    "            loss = margin_ranking_loss(0.5, score_good, score_bad)\n",
    "            if batch_idx % 10 == 0 and batch_idx != 0:\n",
    "                print('sg: ', score_good.item())\n",
    "                print('sb: ', score_bad.item())\n",
    "\n",
    "                print('loss: ', loss.item())\n",
    "\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    loss_epoch = running_loss / len(test_loader)\n",
    "    print('------ Testing -----')\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.0.conv3.weight\n",
      "\t layer1.0.bn3.weight\n",
      "\t layer1.0.bn3.bias\n",
      "\t layer1.0.downsample.0.weight\n",
      "\t layer1.0.downsample.1.weight\n",
      "\t layer1.0.downsample.1.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer1.1.conv3.weight\n",
      "\t layer1.1.bn3.weight\n",
      "\t layer1.1.bn3.bias\n",
      "\t layer1.2.conv1.weight\n",
      "\t layer1.2.bn1.weight\n",
      "\t layer1.2.bn1.bias\n",
      "\t layer1.2.conv2.weight\n",
      "\t layer1.2.bn2.weight\n",
      "\t layer1.2.bn2.bias\n",
      "\t layer1.2.conv3.weight\n",
      "\t layer1.2.bn3.weight\n",
      "\t layer1.2.bn3.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.conv3.weight\n",
      "\t layer2.0.bn3.weight\n",
      "\t layer2.0.bn3.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer2.1.conv3.weight\n",
      "\t layer2.1.bn3.weight\n",
      "\t layer2.1.bn3.bias\n",
      "\t layer2.2.conv1.weight\n",
      "\t layer2.2.bn1.weight\n",
      "\t layer2.2.bn1.bias\n",
      "\t layer2.2.conv2.weight\n",
      "\t layer2.2.bn2.weight\n",
      "\t layer2.2.bn2.bias\n",
      "\t layer2.2.conv3.weight\n",
      "\t layer2.2.bn3.weight\n",
      "\t layer2.2.bn3.bias\n",
      "\t layer2.3.conv1.weight\n",
      "\t layer2.3.bn1.weight\n",
      "\t layer2.3.bn1.bias\n",
      "\t layer2.3.conv2.weight\n",
      "\t layer2.3.bn2.weight\n",
      "\t layer2.3.bn2.bias\n",
      "\t layer2.3.conv3.weight\n",
      "\t layer2.3.bn3.weight\n",
      "\t layer2.3.bn3.bias\n",
      "\t layer2.4.conv1.weight\n",
      "\t layer2.4.bn1.weight\n",
      "\t layer2.4.bn1.bias\n",
      "\t layer2.4.conv2.weight\n",
      "\t layer2.4.bn2.weight\n",
      "\t layer2.4.bn2.bias\n",
      "\t layer2.4.conv3.weight\n",
      "\t layer2.4.bn3.weight\n",
      "\t layer2.4.bn3.bias\n",
      "\t layer2.5.conv1.weight\n",
      "\t layer2.5.bn1.weight\n",
      "\t layer2.5.bn1.bias\n",
      "\t layer2.5.conv2.weight\n",
      "\t layer2.5.bn2.weight\n",
      "\t layer2.5.bn2.bias\n",
      "\t layer2.5.conv3.weight\n",
      "\t layer2.5.bn3.weight\n",
      "\t layer2.5.bn3.bias\n",
      "\t layer2.6.conv1.weight\n",
      "\t layer2.6.bn1.weight\n",
      "\t layer2.6.bn1.bias\n",
      "\t layer2.6.conv2.weight\n",
      "\t layer2.6.bn2.weight\n",
      "\t layer2.6.bn2.bias\n",
      "\t layer2.6.conv3.weight\n",
      "\t layer2.6.bn3.weight\n",
      "\t layer2.6.bn3.bias\n",
      "\t layer2.7.conv1.weight\n",
      "\t layer2.7.bn1.weight\n",
      "\t layer2.7.bn1.bias\n",
      "\t layer2.7.conv2.weight\n",
      "\t layer2.7.bn2.weight\n",
      "\t layer2.7.bn2.bias\n",
      "\t layer2.7.conv3.weight\n",
      "\t layer2.7.bn3.weight\n",
      "\t layer2.7.bn3.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.conv3.weight\n",
      "\t layer3.0.bn3.weight\n",
      "\t layer3.0.bn3.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer3.1.conv3.weight\n",
      "\t layer3.1.bn3.weight\n",
      "\t layer3.1.bn3.bias\n",
      "\t layer3.2.conv1.weight\n",
      "\t layer3.2.bn1.weight\n",
      "\t layer3.2.bn1.bias\n",
      "\t layer3.2.conv2.weight\n",
      "\t layer3.2.bn2.weight\n",
      "\t layer3.2.bn2.bias\n",
      "\t layer3.2.conv3.weight\n",
      "\t layer3.2.bn3.weight\n",
      "\t layer3.2.bn3.bias\n",
      "\t layer3.3.conv1.weight\n",
      "\t layer3.3.bn1.weight\n",
      "\t layer3.3.bn1.bias\n",
      "\t layer3.3.conv2.weight\n",
      "\t layer3.3.bn2.weight\n",
      "\t layer3.3.bn2.bias\n",
      "\t layer3.3.conv3.weight\n",
      "\t layer3.3.bn3.weight\n",
      "\t layer3.3.bn3.bias\n",
      "\t layer3.4.conv1.weight\n",
      "\t layer3.4.bn1.weight\n",
      "\t layer3.4.bn1.bias\n",
      "\t layer3.4.conv2.weight\n",
      "\t layer3.4.bn2.weight\n",
      "\t layer3.4.bn2.bias\n",
      "\t layer3.4.conv3.weight\n",
      "\t layer3.4.bn3.weight\n",
      "\t layer3.4.bn3.bias\n",
      "\t layer3.5.conv1.weight\n",
      "\t layer3.5.bn1.weight\n",
      "\t layer3.5.bn1.bias\n",
      "\t layer3.5.conv2.weight\n",
      "\t layer3.5.bn2.weight\n",
      "\t layer3.5.bn2.bias\n",
      "\t layer3.5.conv3.weight\n",
      "\t layer3.5.bn3.weight\n",
      "\t layer3.5.bn3.bias\n",
      "\t layer3.6.conv1.weight\n",
      "\t layer3.6.bn1.weight\n",
      "\t layer3.6.bn1.bias\n",
      "\t layer3.6.conv2.weight\n",
      "\t layer3.6.bn2.weight\n",
      "\t layer3.6.bn2.bias\n",
      "\t layer3.6.conv3.weight\n",
      "\t layer3.6.bn3.weight\n",
      "\t layer3.6.bn3.bias\n",
      "\t layer3.7.conv1.weight\n",
      "\t layer3.7.bn1.weight\n",
      "\t layer3.7.bn1.bias\n",
      "\t layer3.7.conv2.weight\n",
      "\t layer3.7.bn2.weight\n",
      "\t layer3.7.bn2.bias\n",
      "\t layer3.7.conv3.weight\n",
      "\t layer3.7.bn3.weight\n",
      "\t layer3.7.bn3.bias\n",
      "\t layer3.8.conv1.weight\n",
      "\t layer3.8.bn1.weight\n",
      "\t layer3.8.bn1.bias\n",
      "\t layer3.8.conv2.weight\n",
      "\t layer3.8.bn2.weight\n",
      "\t layer3.8.bn2.bias\n",
      "\t layer3.8.conv3.weight\n",
      "\t layer3.8.bn3.weight\n",
      "\t layer3.8.bn3.bias\n",
      "\t layer3.9.conv1.weight\n",
      "\t layer3.9.bn1.weight\n",
      "\t layer3.9.bn1.bias\n",
      "\t layer3.9.conv2.weight\n",
      "\t layer3.9.bn2.weight\n",
      "\t layer3.9.bn2.bias\n",
      "\t layer3.9.conv3.weight\n",
      "\t layer3.9.bn3.weight\n",
      "\t layer3.9.bn3.bias\n",
      "\t layer3.10.conv1.weight\n",
      "\t layer3.10.bn1.weight\n",
      "\t layer3.10.bn1.bias\n",
      "\t layer3.10.conv2.weight\n",
      "\t layer3.10.bn2.weight\n",
      "\t layer3.10.bn2.bias\n",
      "\t layer3.10.conv3.weight\n",
      "\t layer3.10.bn3.weight\n",
      "\t layer3.10.bn3.bias\n",
      "\t layer3.11.conv1.weight\n",
      "\t layer3.11.bn1.weight\n",
      "\t layer3.11.bn1.bias\n",
      "\t layer3.11.conv2.weight\n",
      "\t layer3.11.bn2.weight\n",
      "\t layer3.11.bn2.bias\n",
      "\t layer3.11.conv3.weight\n",
      "\t layer3.11.bn3.weight\n",
      "\t layer3.11.bn3.bias\n",
      "\t layer3.12.conv1.weight\n",
      "\t layer3.12.bn1.weight\n",
      "\t layer3.12.bn1.bias\n",
      "\t layer3.12.conv2.weight\n",
      "\t layer3.12.bn2.weight\n",
      "\t layer3.12.bn2.bias\n",
      "\t layer3.12.conv3.weight\n",
      "\t layer3.12.bn3.weight\n",
      "\t layer3.12.bn3.bias\n",
      "\t layer3.13.conv1.weight\n",
      "\t layer3.13.bn1.weight\n",
      "\t layer3.13.bn1.bias\n",
      "\t layer3.13.conv2.weight\n",
      "\t layer3.13.bn2.weight\n",
      "\t layer3.13.bn2.bias\n",
      "\t layer3.13.conv3.weight\n",
      "\t layer3.13.bn3.weight\n",
      "\t layer3.13.bn3.bias\n",
      "\t layer3.14.conv1.weight\n",
      "\t layer3.14.bn1.weight\n",
      "\t layer3.14.bn1.bias\n",
      "\t layer3.14.conv2.weight\n",
      "\t layer3.14.bn2.weight\n",
      "\t layer3.14.bn2.bias\n",
      "\t layer3.14.conv3.weight\n",
      "\t layer3.14.bn3.weight\n",
      "\t layer3.14.bn3.bias\n",
      "\t layer3.15.conv1.weight\n",
      "\t layer3.15.bn1.weight\n",
      "\t layer3.15.bn1.bias\n",
      "\t layer3.15.conv2.weight\n",
      "\t layer3.15.bn2.weight\n",
      "\t layer3.15.bn2.bias\n",
      "\t layer3.15.conv3.weight\n",
      "\t layer3.15.bn3.weight\n",
      "\t layer3.15.bn3.bias\n",
      "\t layer3.16.conv1.weight\n",
      "\t layer3.16.bn1.weight\n",
      "\t layer3.16.bn1.bias\n",
      "\t layer3.16.conv2.weight\n",
      "\t layer3.16.bn2.weight\n",
      "\t layer3.16.bn2.bias\n",
      "\t layer3.16.conv3.weight\n",
      "\t layer3.16.bn3.weight\n",
      "\t layer3.16.bn3.bias\n",
      "\t layer3.17.conv1.weight\n",
      "\t layer3.17.bn1.weight\n",
      "\t layer3.17.bn1.bias\n",
      "\t layer3.17.conv2.weight\n",
      "\t layer3.17.bn2.weight\n",
      "\t layer3.17.bn2.bias\n",
      "\t layer3.17.conv3.weight\n",
      "\t layer3.17.bn3.weight\n",
      "\t layer3.17.bn3.bias\n",
      "\t layer3.18.conv1.weight\n",
      "\t layer3.18.bn1.weight\n",
      "\t layer3.18.bn1.bias\n",
      "\t layer3.18.conv2.weight\n",
      "\t layer3.18.bn2.weight\n",
      "\t layer3.18.bn2.bias\n",
      "\t layer3.18.conv3.weight\n",
      "\t layer3.18.bn3.weight\n",
      "\t layer3.18.bn3.bias\n",
      "\t layer3.19.conv1.weight\n",
      "\t layer3.19.bn1.weight\n",
      "\t layer3.19.bn1.bias\n",
      "\t layer3.19.conv2.weight\n",
      "\t layer3.19.bn2.weight\n",
      "\t layer3.19.bn2.bias\n",
      "\t layer3.19.conv3.weight\n",
      "\t layer3.19.bn3.weight\n",
      "\t layer3.19.bn3.bias\n",
      "\t layer3.20.conv1.weight\n",
      "\t layer3.20.bn1.weight\n",
      "\t layer3.20.bn1.bias\n",
      "\t layer3.20.conv2.weight\n",
      "\t layer3.20.bn2.weight\n",
      "\t layer3.20.bn2.bias\n",
      "\t layer3.20.conv3.weight\n",
      "\t layer3.20.bn3.weight\n",
      "\t layer3.20.bn3.bias\n",
      "\t layer3.21.conv1.weight\n",
      "\t layer3.21.bn1.weight\n",
      "\t layer3.21.bn1.bias\n",
      "\t layer3.21.conv2.weight\n",
      "\t layer3.21.bn2.weight\n",
      "\t layer3.21.bn2.bias\n",
      "\t layer3.21.conv3.weight\n",
      "\t layer3.21.bn3.weight\n",
      "\t layer3.21.bn3.bias\n",
      "\t layer3.22.conv1.weight\n",
      "\t layer3.22.bn1.weight\n",
      "\t layer3.22.bn1.bias\n",
      "\t layer3.22.conv2.weight\n",
      "\t layer3.22.bn2.weight\n",
      "\t layer3.22.bn2.bias\n",
      "\t layer3.22.conv3.weight\n",
      "\t layer3.22.bn3.weight\n",
      "\t layer3.22.bn3.bias\n",
      "\t layer3.23.conv1.weight\n",
      "\t layer3.23.bn1.weight\n",
      "\t layer3.23.bn1.bias\n",
      "\t layer3.23.conv2.weight\n",
      "\t layer3.23.bn2.weight\n",
      "\t layer3.23.bn2.bias\n",
      "\t layer3.23.conv3.weight\n",
      "\t layer3.23.bn3.weight\n",
      "\t layer3.23.bn3.bias\n",
      "\t layer3.24.conv1.weight\n",
      "\t layer3.24.bn1.weight\n",
      "\t layer3.24.bn1.bias\n",
      "\t layer3.24.conv2.weight\n",
      "\t layer3.24.bn2.weight\n",
      "\t layer3.24.bn2.bias\n",
      "\t layer3.24.conv3.weight\n",
      "\t layer3.24.bn3.weight\n",
      "\t layer3.24.bn3.bias\n",
      "\t layer3.25.conv1.weight\n",
      "\t layer3.25.bn1.weight\n",
      "\t layer3.25.bn1.bias\n",
      "\t layer3.25.conv2.weight\n",
      "\t layer3.25.bn2.weight\n",
      "\t layer3.25.bn2.bias\n",
      "\t layer3.25.conv3.weight\n",
      "\t layer3.25.bn3.weight\n",
      "\t layer3.25.bn3.bias\n",
      "\t layer3.26.conv1.weight\n",
      "\t layer3.26.bn1.weight\n",
      "\t layer3.26.bn1.bias\n",
      "\t layer3.26.conv2.weight\n",
      "\t layer3.26.bn2.weight\n",
      "\t layer3.26.bn2.bias\n",
      "\t layer3.26.conv3.weight\n",
      "\t layer3.26.bn3.weight\n",
      "\t layer3.26.bn3.bias\n",
      "\t layer3.27.conv1.weight\n",
      "\t layer3.27.bn1.weight\n",
      "\t layer3.27.bn1.bias\n",
      "\t layer3.27.conv2.weight\n",
      "\t layer3.27.bn2.weight\n",
      "\t layer3.27.bn2.bias\n",
      "\t layer3.27.conv3.weight\n",
      "\t layer3.27.bn3.weight\n",
      "\t layer3.27.bn3.bias\n",
      "\t layer3.28.conv1.weight\n",
      "\t layer3.28.bn1.weight\n",
      "\t layer3.28.bn1.bias\n",
      "\t layer3.28.conv2.weight\n",
      "\t layer3.28.bn2.weight\n",
      "\t layer3.28.bn2.bias\n",
      "\t layer3.28.conv3.weight\n",
      "\t layer3.28.bn3.weight\n",
      "\t layer3.28.bn3.bias\n",
      "\t layer3.29.conv1.weight\n",
      "\t layer3.29.bn1.weight\n",
      "\t layer3.29.bn1.bias\n",
      "\t layer3.29.conv2.weight\n",
      "\t layer3.29.bn2.weight\n",
      "\t layer3.29.bn2.bias\n",
      "\t layer3.29.conv3.weight\n",
      "\t layer3.29.bn3.weight\n",
      "\t layer3.29.bn3.bias\n",
      "\t layer3.30.conv1.weight\n",
      "\t layer3.30.bn1.weight\n",
      "\t layer3.30.bn1.bias\n",
      "\t layer3.30.conv2.weight\n",
      "\t layer3.30.bn2.weight\n",
      "\t layer3.30.bn2.bias\n",
      "\t layer3.30.conv3.weight\n",
      "\t layer3.30.bn3.weight\n",
      "\t layer3.30.bn3.bias\n",
      "\t layer3.31.conv1.weight\n",
      "\t layer3.31.bn1.weight\n",
      "\t layer3.31.bn1.bias\n",
      "\t layer3.31.conv2.weight\n",
      "\t layer3.31.bn2.weight\n",
      "\t layer3.31.bn2.bias\n",
      "\t layer3.31.conv3.weight\n",
      "\t layer3.31.bn3.weight\n",
      "\t layer3.31.bn3.bias\n",
      "\t layer3.32.conv1.weight\n",
      "\t layer3.32.bn1.weight\n",
      "\t layer3.32.bn1.bias\n",
      "\t layer3.32.conv2.weight\n",
      "\t layer3.32.bn2.weight\n",
      "\t layer3.32.bn2.bias\n",
      "\t layer3.32.conv3.weight\n",
      "\t layer3.32.bn3.weight\n",
      "\t layer3.32.bn3.bias\n",
      "\t layer3.33.conv1.weight\n",
      "\t layer3.33.bn1.weight\n",
      "\t layer3.33.bn1.bias\n",
      "\t layer3.33.conv2.weight\n",
      "\t layer3.33.bn2.weight\n",
      "\t layer3.33.bn2.bias\n",
      "\t layer3.33.conv3.weight\n",
      "\t layer3.33.bn3.weight\n",
      "\t layer3.33.bn3.bias\n",
      "\t layer3.34.conv1.weight\n",
      "\t layer3.34.bn1.weight\n",
      "\t layer3.34.bn1.bias\n",
      "\t layer3.34.conv2.weight\n",
      "\t layer3.34.bn2.weight\n",
      "\t layer3.34.bn2.bias\n",
      "\t layer3.34.conv3.weight\n",
      "\t layer3.34.bn3.weight\n",
      "\t layer3.34.bn3.bias\n",
      "\t layer3.35.conv1.weight\n",
      "\t layer3.35.bn1.weight\n",
      "\t layer3.35.bn1.bias\n",
      "\t layer3.35.conv2.weight\n",
      "\t layer3.35.bn2.weight\n",
      "\t layer3.35.bn2.bias\n",
      "\t layer3.35.conv3.weight\n",
      "\t layer3.35.bn3.weight\n",
      "\t layer3.35.bn3.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.conv3.weight\n",
      "\t layer4.0.bn3.weight\n",
      "\t layer4.0.bn3.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t layer4.1.conv3.weight\n",
      "\t layer4.1.bn3.weight\n",
      "\t layer4.1.bn3.bias\n",
      "\t layer4.2.conv1.weight\n",
      "\t layer4.2.bn1.weight\n",
      "\t layer4.2.bn1.bias\n",
      "\t layer4.2.conv2.weight\n",
      "\t layer4.2.bn2.weight\n",
      "\t layer4.2.bn2.bias\n",
      "\t layer4.2.conv3.weight\n",
      "\t layer4.2.bn3.weight\n",
      "\t layer4.2.bn3.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# MODELS \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "image_CNN = model_ft.to(device)\n",
    "matching_CNN = matchingCNN().to(device)\n",
    "scoring_MLP = Scoring_function().to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "params = list(params_to_update) + list(matching_CNN.parameters()) + list(scoring_MLP.parameters())\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training epoch 0/9 --------\n",
      "sg:  0.004097073804587126\n",
      "sb:  0.0010304147144779563\n",
      "loss:  0.49693334102630615\n",
      "sg:  0.00986428651958704\n",
      "sb:  0.00474146194756031\n",
      "loss:  0.4948771595954895\n",
      "sg:  0.015590873546898365\n",
      "sb:  0.00855383649468422\n",
      "loss:  0.49296295642852783\n",
      "sg:  0.02124929428100586\n",
      "sb:  0.011777550913393497\n",
      "loss:  0.49052825570106506\n",
      "sg:  0.02864675037562847\n",
      "sb:  0.017119891941547394\n",
      "loss:  0.48847314715385437\n",
      "sg:  0.03575052320957184\n",
      "sb:  0.0215508583933115\n",
      "loss:  0.4858003556728363\n",
      "sg:  0.04412742331624031\n",
      "sb:  0.027623068541288376\n",
      "loss:  0.4834956228733063\n",
      "sg:  0.05635696277022362\n",
      "sb:  0.03603341802954674\n",
      "loss:  0.4796764552593231\n",
      "sg:  0.07268647849559784\n",
      "sb:  0.047514453530311584\n",
      "loss:  0.47482794523239136\n",
      "sg:  0.09205743670463562\n",
      "sb:  0.05860103294253349\n",
      "loss:  0.466543585062027\n",
      "sg:  0.12075681239366531\n",
      "sb:  0.0774388462305069\n",
      "loss:  0.456682026386261\n",
      "sg:  0.15404389798641205\n",
      "sb:  0.10454294830560684\n",
      "loss:  0.450499027967453\n",
      "sg:  0.1936601996421814\n",
      "sb:  0.13027535378932953\n",
      "loss:  0.4366151690483093\n",
      "sg:  0.26449665427207947\n",
      "sb:  0.17844820022583008\n",
      "loss:  0.4139515459537506\n",
      "sg:  0.3904147148132324\n",
      "sb:  0.25725656747817993\n",
      "loss:  0.3668418526649475\n",
      "sg:  0.5478155612945557\n",
      "sb:  0.3621748983860016\n",
      "loss:  0.3143593370914459\n",
      "sg:  0.8972972631454468\n",
      "sb:  0.6213681697845459\n",
      "loss:  0.22407090663909912\n",
      "sg:  1.5690003633499146\n",
      "sb:  1.043269157409668\n",
      "loss:  0.0\n",
      "sg:  2.0763468742370605\n",
      "sb:  1.396079421043396\n",
      "loss:  0.0\n",
      "sg:  2.358059883117676\n",
      "sb:  1.5918594598770142\n",
      "loss:  0.0\n",
      "sg:  2.44936466217041\n",
      "sb:  1.6511282920837402\n",
      "loss:  0.0\n",
      "sg:  2.549107074737549\n",
      "sb:  1.6972298622131348\n",
      "loss:  0.0\n",
      "sg:  2.4313032627105713\n",
      "sb:  1.6564037799835205\n",
      "loss:  0.0\n",
      "sg:  2.466381072998047\n",
      "sb:  1.6978716850280762\n",
      "loss:  0.0\n",
      "sg:  2.495112657546997\n",
      "sb:  1.7165148258209229\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 0, loss: 0.3064572076081287\n",
      "\n",
      "------Testing epoch 0/9 --------\n",
      "sg:  2.5938820838928223\n",
      "sb:  1.7429680824279785\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 0, loss: 0.0\n",
      "-----Training epoch 1/9 --------\n",
      "sg:  2.4192724227905273\n",
      "sb:  1.6527165174484253\n",
      "loss:  0.0\n",
      "sg:  2.4056124687194824\n",
      "sb:  1.6262257099151611\n",
      "loss:  0.0\n",
      "sg:  2.4416069984436035\n",
      "sb:  1.6615819931030273\n",
      "loss:  0.0\n",
      "sg:  2.3874576091766357\n",
      "sb:  1.5967655181884766\n",
      "loss:  0.0\n",
      "sg:  2.3971025943756104\n",
      "sb:  1.6259256601333618\n",
      "loss:  0.0\n",
      "sg:  2.3744657039642334\n",
      "sb:  1.610304832458496\n",
      "loss:  0.0\n",
      "sg:  2.369235038757324\n",
      "sb:  1.602038860321045\n",
      "loss:  0.0\n",
      "sg:  2.445556163787842\n",
      "sb:  1.6884621381759644\n",
      "loss:  0.0\n",
      "sg:  2.4818713665008545\n",
      "sb:  1.6531193256378174\n",
      "loss:  0.0\n",
      "sg:  2.5054352283477783\n",
      "sb:  1.6904606819152832\n",
      "loss:  0.0\n",
      "sg:  2.4606969356536865\n",
      "sb:  1.6743398904800415\n",
      "loss:  0.0\n",
      "sg:  2.3626387119293213\n",
      "sb:  1.5913512706756592\n",
      "loss:  0.0\n",
      "sg:  2.4286818504333496\n",
      "sb:  1.6221204996109009\n",
      "loss:  0.0\n",
      "sg:  2.506300210952759\n",
      "sb:  1.6786935329437256\n",
      "loss:  0.0\n",
      "sg:  2.4029664993286133\n",
      "sb:  1.644896149635315\n",
      "loss:  0.0\n",
      "sg:  2.408247232437134\n",
      "sb:  1.6319489479064941\n",
      "loss:  0.0\n",
      "sg:  2.4396095275878906\n",
      "sb:  1.6518137454986572\n",
      "loss:  0.0\n",
      "sg:  2.4098620414733887\n",
      "sb:  1.6375291347503662\n",
      "loss:  0.0\n",
      "sg:  2.4883460998535156\n",
      "sb:  1.668622374534607\n",
      "loss:  0.0\n",
      "sg:  2.38871169090271\n",
      "sb:  1.600332260131836\n",
      "loss:  0.0\n",
      "sg:  2.4455149173736572\n",
      "sb:  1.6363650560379028\n",
      "loss:  0.0\n",
      "sg:  2.4096169471740723\n",
      "sb:  1.6078808307647705\n",
      "loss:  0.0\n",
      "sg:  2.4783880710601807\n",
      "sb:  1.6996057033538818\n",
      "loss:  0.0\n",
      "sg:  2.386847734451294\n",
      "sb:  1.6706838607788086\n",
      "loss:  0.0\n",
      "sg:  2.411175489425659\n",
      "sb:  1.6183581352233887\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 1, loss: 0.0\n",
      "\n",
      "------Testing epoch 1/9 --------\n",
      "sg:  2.5922210216522217\n",
      "sb:  1.7434405088424683\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 1, loss: 0.0\n",
      "-----Training epoch 2/9 --------\n",
      "sg:  2.465904712677002\n",
      "sb:  1.6481983661651611\n",
      "loss:  0.0\n",
      "sg:  2.4629135131835938\n",
      "sb:  1.6672929525375366\n",
      "loss:  0.0\n",
      "sg:  2.447502613067627\n",
      "sb:  1.6299762725830078\n",
      "loss:  0.0\n",
      "sg:  2.4379069805145264\n",
      "sb:  1.6398266553878784\n",
      "loss:  0.0\n",
      "sg:  2.4343466758728027\n",
      "sb:  1.661607027053833\n",
      "loss:  0.0\n",
      "sg:  2.438584327697754\n",
      "sb:  1.680038571357727\n",
      "loss:  0.0\n",
      "sg:  2.4594316482543945\n",
      "sb:  1.6539188623428345\n",
      "loss:  0.0\n",
      "sg:  2.404040575027466\n",
      "sb:  1.6439001560211182\n",
      "loss:  0.0\n",
      "sg:  2.3764419555664062\n",
      "sb:  1.6068699359893799\n",
      "loss:  0.0\n",
      "sg:  2.455129384994507\n",
      "sb:  1.638277292251587\n",
      "loss:  0.0\n",
      "sg:  2.4397130012512207\n",
      "sb:  1.6626131534576416\n",
      "loss:  0.0\n",
      "sg:  2.390550136566162\n",
      "sb:  1.6366384029388428\n",
      "loss:  0.0\n",
      "sg:  2.4014275074005127\n",
      "sb:  1.5930267572402954\n",
      "loss:  0.0\n",
      "sg:  2.4806201457977295\n",
      "sb:  1.629355788230896\n",
      "loss:  0.0\n",
      "sg:  2.4423699378967285\n",
      "sb:  1.7065688371658325\n",
      "loss:  0.0\n",
      "sg:  2.455143690109253\n",
      "sb:  1.666718602180481\n",
      "loss:  0.0\n",
      "sg:  2.4034359455108643\n",
      "sb:  1.627488613128662\n",
      "loss:  0.0\n",
      "sg:  2.4583070278167725\n",
      "sb:  1.630784511566162\n",
      "loss:  0.0\n",
      "sg:  2.501237630844116\n",
      "sb:  1.6625280380249023\n",
      "loss:  0.0\n",
      "sg:  2.4006123542785645\n",
      "sb:  1.6292966604232788\n",
      "loss:  0.0\n",
      "sg:  2.4339065551757812\n",
      "sb:  1.6256073713302612\n",
      "loss:  0.0\n",
      "sg:  2.4948432445526123\n",
      "sb:  1.65330171585083\n",
      "loss:  0.0\n",
      "sg:  2.5068132877349854\n",
      "sb:  1.691141963005066\n",
      "loss:  0.0\n",
      "sg:  2.4355173110961914\n",
      "sb:  1.669700264930725\n",
      "loss:  0.0\n",
      "sg:  2.2878060340881348\n",
      "sb:  1.5853502750396729\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 2, loss: 0.0\n",
      "\n",
      "------Testing epoch 2/9 --------\n",
      "sg:  2.5954856872558594\n",
      "sb:  1.7449548244476318\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 2, loss: 0.0\n",
      "-----Training epoch 3/9 --------\n",
      "sg:  2.50081467628479\n",
      "sb:  1.6407971382141113\n",
      "loss:  0.0\n",
      "sg:  2.3417129516601562\n",
      "sb:  1.6138339042663574\n",
      "loss:  0.0\n",
      "sg:  2.4944980144500732\n",
      "sb:  1.6504364013671875\n",
      "loss:  0.0\n",
      "sg:  2.4439218044281006\n",
      "sb:  1.634387493133545\n",
      "loss:  0.0\n",
      "sg:  2.4745116233825684\n",
      "sb:  1.6496371030807495\n",
      "loss:  0.0\n",
      "sg:  2.5545427799224854\n",
      "sb:  1.6951582431793213\n",
      "loss:  0.0\n",
      "sg:  2.419644355773926\n",
      "sb:  1.6872174739837646\n",
      "loss:  0.0\n",
      "sg:  2.4744300842285156\n",
      "sb:  1.6955010890960693\n",
      "loss:  0.0\n",
      "sg:  2.4650075435638428\n",
      "sb:  1.6893768310546875\n",
      "loss:  0.0\n",
      "sg:  2.4505839347839355\n",
      "sb:  1.6319259405136108\n",
      "loss:  0.0\n",
      "sg:  2.400075674057007\n",
      "sb:  1.6172780990600586\n",
      "loss:  0.0\n",
      "sg:  2.4785351753234863\n",
      "sb:  1.7009410858154297\n",
      "loss:  0.0\n",
      "sg:  2.410414934158325\n",
      "sb:  1.6723779439926147\n",
      "loss:  0.0\n",
      "sg:  2.5005693435668945\n",
      "sb:  1.6278389692306519\n",
      "loss:  0.0\n",
      "sg:  2.4186737537384033\n",
      "sb:  1.6492197513580322\n",
      "loss:  0.0\n",
      "sg:  2.444572687149048\n",
      "sb:  1.6462770700454712\n",
      "loss:  0.0\n",
      "sg:  2.372316837310791\n",
      "sb:  1.6609973907470703\n",
      "loss:  0.0\n",
      "sg:  2.5199458599090576\n",
      "sb:  1.6568306684494019\n",
      "loss:  0.0\n",
      "sg:  2.459123134613037\n",
      "sb:  1.5966302156448364\n",
      "loss:  0.0\n",
      "sg:  2.4653494358062744\n",
      "sb:  1.6428042650222778\n",
      "loss:  0.0\n",
      "sg:  2.4603633880615234\n",
      "sb:  1.6523497104644775\n",
      "loss:  0.0\n",
      "sg:  2.472787618637085\n",
      "sb:  1.6710271835327148\n",
      "loss:  0.0\n",
      "sg:  2.435359239578247\n",
      "sb:  1.633949875831604\n",
      "loss:  0.0\n",
      "sg:  2.467787742614746\n",
      "sb:  1.6958783864974976\n",
      "loss:  0.0\n",
      "sg:  2.387899160385132\n",
      "sb:  1.6156878471374512\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 3, loss: 0.0\n",
      "\n",
      "------Testing epoch 3/9 --------\n",
      "sg:  2.5976364612579346\n",
      "sb:  1.7445348501205444\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 3, loss: 0.0\n",
      "-----Training epoch 4/9 --------\n",
      "sg:  2.445220470428467\n",
      "sb:  1.6563814878463745\n",
      "loss:  0.0\n",
      "sg:  2.412588119506836\n",
      "sb:  1.6493165493011475\n",
      "loss:  0.0\n",
      "sg:  2.560081720352173\n",
      "sb:  1.7069894075393677\n",
      "loss:  0.0\n",
      "sg:  2.467302083969116\n",
      "sb:  1.631866216659546\n",
      "loss:  0.0\n",
      "sg:  2.40781831741333\n",
      "sb:  1.6443448066711426\n",
      "loss:  0.0\n",
      "sg:  2.408865213394165\n",
      "sb:  1.6556850671768188\n",
      "loss:  0.0\n",
      "sg:  2.4191842079162598\n",
      "sb:  1.6387032270431519\n",
      "loss:  0.0\n",
      "sg:  2.3642935752868652\n",
      "sb:  1.656327486038208\n",
      "loss:  0.0\n",
      "sg:  2.2791812419891357\n",
      "sb:  1.551743745803833\n",
      "loss:  0.0\n",
      "sg:  2.522099256515503\n",
      "sb:  1.7225719690322876\n",
      "loss:  0.0\n",
      "sg:  2.4102566242218018\n",
      "sb:  1.6554583311080933\n",
      "loss:  0.0\n",
      "sg:  2.4083962440490723\n",
      "sb:  1.5974748134613037\n",
      "loss:  0.0\n",
      "sg:  2.4638795852661133\n",
      "sb:  1.6527962684631348\n",
      "loss:  0.0\n",
      "sg:  2.5573348999023438\n",
      "sb:  1.6882762908935547\n",
      "loss:  0.0\n",
      "sg:  2.473935604095459\n",
      "sb:  1.6715055704116821\n",
      "loss:  0.0\n",
      "sg:  2.4422128200531006\n",
      "sb:  1.6281304359436035\n",
      "loss:  0.0\n",
      "sg:  2.441603899002075\n",
      "sb:  1.6916460990905762\n",
      "loss:  0.0\n",
      "sg:  2.4088571071624756\n",
      "sb:  1.6854522228240967\n",
      "loss:  0.0\n",
      "sg:  2.3447368144989014\n",
      "sb:  1.6205819845199585\n",
      "loss:  0.0\n",
      "sg:  2.5109705924987793\n",
      "sb:  1.7469927072525024\n",
      "loss:  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg:  2.3773210048675537\n",
      "sb:  1.6056060791015625\n",
      "loss:  0.0\n",
      "sg:  2.403116464614868\n",
      "sb:  1.618457555770874\n",
      "loss:  0.0\n",
      "sg:  2.4896867275238037\n",
      "sb:  1.7250471115112305\n",
      "loss:  0.0\n",
      "sg:  2.4375646114349365\n",
      "sb:  1.6536462306976318\n",
      "loss:  0.0\n",
      "sg:  2.333554267883301\n",
      "sb:  1.622067928314209\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 4, loss: 0.0\n",
      "\n",
      "------Testing epoch 4/9 --------\n",
      "sg:  2.594128370285034\n",
      "sb:  1.7460918426513672\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 4, loss: 0.0\n",
      "-----Training epoch 5/9 --------\n",
      "sg:  2.5142130851745605\n",
      "sb:  1.6515380144119263\n",
      "loss:  0.0\n",
      "sg:  2.4245498180389404\n",
      "sb:  1.673907995223999\n",
      "loss:  0.0\n",
      "sg:  2.4741058349609375\n",
      "sb:  1.675045132637024\n",
      "loss:  0.0\n",
      "sg:  2.4104628562927246\n",
      "sb:  1.6379343271255493\n",
      "loss:  0.0\n",
      "sg:  2.4063973426818848\n",
      "sb:  1.610346794128418\n",
      "loss:  0.0\n",
      "sg:  2.470592737197876\n",
      "sb:  1.6634306907653809\n",
      "loss:  0.0\n",
      "sg:  2.4511282444000244\n",
      "sb:  1.6806341409683228\n",
      "loss:  0.0\n",
      "sg:  2.4704549312591553\n",
      "sb:  1.6823315620422363\n",
      "loss:  0.0\n",
      "sg:  2.4423959255218506\n",
      "sb:  1.667771816253662\n",
      "loss:  0.0\n",
      "sg:  2.4366488456726074\n",
      "sb:  1.625260829925537\n",
      "loss:  0.0\n",
      "sg:  2.4625887870788574\n",
      "sb:  1.666475772857666\n",
      "loss:  0.0\n",
      "sg:  2.351896286010742\n",
      "sb:  1.6047866344451904\n",
      "loss:  0.0\n",
      "sg:  2.469053268432617\n",
      "sb:  1.6389864683151245\n",
      "loss:  0.0\n",
      "sg:  2.4400501251220703\n",
      "sb:  1.6226164102554321\n",
      "loss:  0.0\n",
      "sg:  2.320627212524414\n",
      "sb:  1.6114486455917358\n",
      "loss:  0.0\n",
      "sg:  2.4660258293151855\n",
      "sb:  1.6280654668807983\n",
      "loss:  0.0\n",
      "sg:  2.5486927032470703\n",
      "sb:  1.7251440286636353\n",
      "loss:  0.0\n",
      "sg:  2.3592214584350586\n",
      "sb:  1.6074892282485962\n",
      "loss:  0.0\n",
      "sg:  2.427586555480957\n",
      "sb:  1.6469274759292603\n",
      "loss:  0.0\n",
      "sg:  2.466989040374756\n",
      "sb:  1.6856752634048462\n",
      "loss:  0.0\n",
      "sg:  2.4887170791625977\n",
      "sb:  1.6545439958572388\n",
      "loss:  0.0\n",
      "sg:  2.5165152549743652\n",
      "sb:  1.683174967765808\n",
      "loss:  0.0\n",
      "sg:  2.4069957733154297\n",
      "sb:  1.6539052724838257\n",
      "loss:  0.0\n",
      "sg:  2.4596962928771973\n",
      "sb:  1.731321096420288\n",
      "loss:  0.0\n",
      "sg:  2.4553403854370117\n",
      "sb:  1.6201763153076172\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 5, loss: 0.0\n",
      "\n",
      "------Testing epoch 5/9 --------\n",
      "sg:  2.5963079929351807\n",
      "sb:  1.7431073188781738\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 5, loss: 0.0\n",
      "-----Training epoch 6/9 --------\n",
      "sg:  2.3996739387512207\n",
      "sb:  1.6076323986053467\n",
      "loss:  0.0\n",
      "sg:  2.4359161853790283\n",
      "sb:  1.6301748752593994\n",
      "loss:  0.0\n",
      "sg:  2.4933037757873535\n",
      "sb:  1.677224040031433\n",
      "loss:  0.0\n",
      "sg:  2.42566180229187\n",
      "sb:  1.623424768447876\n",
      "loss:  0.0\n",
      "sg:  2.4415783882141113\n",
      "sb:  1.6332294940948486\n",
      "loss:  0.0\n",
      "sg:  2.391974687576294\n",
      "sb:  1.5963882207870483\n",
      "loss:  0.0\n",
      "sg:  2.443821430206299\n",
      "sb:  1.6336610317230225\n",
      "loss:  0.0\n",
      "sg:  2.362499952316284\n",
      "sb:  1.5812957286834717\n",
      "loss:  0.0\n",
      "sg:  2.4355123043060303\n",
      "sb:  1.604859709739685\n",
      "loss:  0.0\n",
      "sg:  2.4292731285095215\n",
      "sb:  1.7031103372573853\n",
      "loss:  0.0\n",
      "sg:  2.439781427383423\n",
      "sb:  1.6536377668380737\n",
      "loss:  0.0\n",
      "sg:  2.434265375137329\n",
      "sb:  1.6630984544754028\n",
      "loss:  0.0\n",
      "sg:  2.4200658798217773\n",
      "sb:  1.6913119554519653\n",
      "loss:  0.0\n",
      "sg:  2.4778506755828857\n",
      "sb:  1.6680151224136353\n",
      "loss:  0.0\n",
      "sg:  2.5211198329925537\n",
      "sb:  1.6881400346755981\n",
      "loss:  0.0\n",
      "sg:  2.430506944656372\n",
      "sb:  1.6633639335632324\n",
      "loss:  0.0\n",
      "sg:  2.493623971939087\n",
      "sb:  1.6742210388183594\n",
      "loss:  0.0\n",
      "sg:  2.448871612548828\n",
      "sb:  1.6448824405670166\n",
      "loss:  0.0\n",
      "sg:  2.4803996086120605\n",
      "sb:  1.6511647701263428\n",
      "loss:  0.0\n",
      "sg:  2.514862060546875\n",
      "sb:  1.6720730066299438\n",
      "loss:  0.0\n",
      "sg:  2.4342546463012695\n",
      "sb:  1.6512620449066162\n",
      "loss:  0.0\n",
      "sg:  2.3985702991485596\n",
      "sb:  1.6405234336853027\n",
      "loss:  0.0\n",
      "sg:  2.469931125640869\n",
      "sb:  1.6435917615890503\n",
      "loss:  0.0\n",
      "sg:  2.4350831508636475\n",
      "sb:  1.6712144613265991\n",
      "loss:  0.0\n",
      "sg:  2.455303907394409\n",
      "sb:  1.6449570655822754\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 6, loss: 0.0\n",
      "\n",
      "------Testing epoch 6/9 --------\n",
      "sg:  2.594740867614746\n",
      "sb:  1.749727725982666\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 6, loss: 0.0\n",
      "-----Training epoch 7/9 --------\n",
      "sg:  2.5424818992614746\n",
      "sb:  1.7438174486160278\n",
      "loss:  0.0\n",
      "sg:  2.438748359680176\n",
      "sb:  1.6521042585372925\n",
      "loss:  0.0\n",
      "sg:  2.4439690113067627\n",
      "sb:  1.6269103288650513\n",
      "loss:  0.0\n",
      "sg:  2.4987237453460693\n",
      "sb:  1.655173897743225\n",
      "loss:  0.0\n",
      "sg:  2.4381797313690186\n",
      "sb:  1.6194820404052734\n",
      "loss:  0.0\n",
      "sg:  2.5265591144561768\n",
      "sb:  1.703221082687378\n",
      "loss:  0.0\n",
      "sg:  2.418375015258789\n",
      "sb:  1.659085750579834\n",
      "loss:  0.0\n",
      "sg:  2.3424570560455322\n",
      "sb:  1.5904467105865479\n",
      "loss:  0.0\n",
      "sg:  2.4900245666503906\n",
      "sb:  1.647518277168274\n",
      "loss:  0.0\n",
      "sg:  2.409144878387451\n",
      "sb:  1.638977289199829\n",
      "loss:  0.0\n",
      "sg:  2.425180196762085\n",
      "sb:  1.6676470041275024\n",
      "loss:  0.0\n",
      "sg:  2.448267698287964\n",
      "sb:  1.684038758277893\n",
      "loss:  0.0\n",
      "sg:  2.4695301055908203\n",
      "sb:  1.678682804107666\n",
      "loss:  0.0\n",
      "sg:  2.475820541381836\n",
      "sb:  1.6802477836608887\n",
      "loss:  0.0\n",
      "sg:  2.4013590812683105\n",
      "sb:  1.6309876441955566\n",
      "loss:  0.0\n",
      "sg:  2.343567132949829\n",
      "sb:  1.5839823484420776\n",
      "loss:  0.0\n",
      "sg:  2.3671951293945312\n",
      "sb:  1.6187692880630493\n",
      "loss:  0.0\n",
      "sg:  2.4011800289154053\n",
      "sb:  1.6609454154968262\n",
      "loss:  0.0\n",
      "sg:  2.513031005859375\n",
      "sb:  1.7040683031082153\n",
      "loss:  0.0\n",
      "sg:  2.4857864379882812\n",
      "sb:  1.6676430702209473\n",
      "loss:  0.0\n",
      "sg:  2.395695447921753\n",
      "sb:  1.6329641342163086\n",
      "loss:  0.0\n",
      "sg:  2.4165236949920654\n",
      "sb:  1.60675048828125\n",
      "loss:  0.0\n",
      "sg:  2.4273877143859863\n",
      "sb:  1.6328215599060059\n",
      "loss:  0.0\n",
      "sg:  2.4823176860809326\n",
      "sb:  1.6403743028640747\n",
      "loss:  0.0\n",
      "sg:  2.4942219257354736\n",
      "sb:  1.660093069076538\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 7, loss: 0.0\n",
      "\n",
      "------Testing epoch 7/9 --------\n",
      "sg:  2.594996213912964\n",
      "sb:  1.7408971786499023\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 7, loss: 0.0\n",
      "-----Training epoch 8/9 --------\n",
      "sg:  2.458082437515259\n",
      "sb:  1.6435515880584717\n",
      "loss:  0.0\n",
      "sg:  2.388651132583618\n",
      "sb:  1.6296217441558838\n",
      "loss:  0.0\n",
      "sg:  2.472484588623047\n",
      "sb:  1.6591110229492188\n",
      "loss:  0.0\n",
      "sg:  2.3647701740264893\n",
      "sb:  1.5865768194198608\n",
      "loss:  0.0\n",
      "sg:  2.419536590576172\n",
      "sb:  1.6340312957763672\n",
      "loss:  0.0\n",
      "sg:  2.469123125076294\n",
      "sb:  1.6991407871246338\n",
      "loss:  0.0\n",
      "sg:  2.3763372898101807\n",
      "sb:  1.6298794746398926\n",
      "loss:  0.0\n",
      "sg:  2.4659388065338135\n",
      "sb:  1.6711907386779785\n",
      "loss:  0.0\n",
      "sg:  2.4126687049865723\n",
      "sb:  1.6422886848449707\n",
      "loss:  0.0\n",
      "sg:  2.3458898067474365\n",
      "sb:  1.5572580099105835\n",
      "loss:  0.0\n",
      "sg:  2.404677152633667\n",
      "sb:  1.6101902723312378\n",
      "loss:  0.0\n",
      "sg:  2.3502371311187744\n",
      "sb:  1.5861401557922363\n",
      "loss:  0.0\n",
      "sg:  2.4940333366394043\n",
      "sb:  1.67367422580719\n",
      "loss:  0.0\n",
      "sg:  2.555103302001953\n",
      "sb:  1.7070813179016113\n",
      "loss:  0.0\n",
      "sg:  2.4846954345703125\n",
      "sb:  1.6539090871810913\n",
      "loss:  0.0\n",
      "sg:  2.32500958442688\n",
      "sb:  1.6146847009658813\n",
      "loss:  0.0\n",
      "sg:  2.4782347679138184\n",
      "sb:  1.66941499710083\n",
      "loss:  0.0\n",
      "sg:  2.474001407623291\n",
      "sb:  1.7281908988952637\n",
      "loss:  0.0\n",
      "sg:  2.439885139465332\n",
      "sb:  1.6757168769836426\n",
      "loss:  0.0\n",
      "sg:  2.5256993770599365\n",
      "sb:  1.67471182346344\n",
      "loss:  0.0\n",
      "sg:  2.484922409057617\n",
      "sb:  1.7064940929412842\n",
      "loss:  0.0\n",
      "sg:  2.4347336292266846\n",
      "sb:  1.633600115776062\n",
      "loss:  0.0\n",
      "sg:  2.4464802742004395\n",
      "sb:  1.6642041206359863\n",
      "loss:  0.0\n",
      "sg:  2.436828374862671\n",
      "sb:  1.62832510471344\n",
      "loss:  0.0\n",
      "sg:  2.451152801513672\n",
      "sb:  1.652313470840454\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 8, loss: 0.0\n",
      "\n",
      "------Testing epoch 8/9 --------\n",
      "sg:  2.593926429748535\n",
      "sb:  1.7419703006744385\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 8, loss: 0.0\n",
      "-----Training epoch 9/9 --------\n",
      "sg:  2.5279557704925537\n",
      "sb:  1.6527456045150757\n",
      "loss:  0.0\n",
      "sg:  2.414154529571533\n",
      "sb:  1.6493045091629028\n",
      "loss:  0.0\n",
      "sg:  2.3539233207702637\n",
      "sb:  1.603016972541809\n",
      "loss:  0.0\n",
      "sg:  2.3715596199035645\n",
      "sb:  1.593893051147461\n",
      "loss:  0.0\n",
      "sg:  2.4016759395599365\n",
      "sb:  1.6236228942871094\n",
      "loss:  0.0\n",
      "sg:  2.518728017807007\n",
      "sb:  1.6853278875350952\n",
      "loss:  0.0\n",
      "sg:  2.4403491020202637\n",
      "sb:  1.6231402158737183\n",
      "loss:  0.0\n",
      "sg:  2.4654698371887207\n",
      "sb:  1.645190715789795\n",
      "loss:  0.0\n",
      "sg:  2.4275548458099365\n",
      "sb:  1.6894736289978027\n",
      "loss:  0.0\n",
      "sg:  2.4282548427581787\n",
      "sb:  1.676830768585205\n",
      "loss:  0.0\n",
      "sg:  2.5339083671569824\n",
      "sb:  1.6947026252746582\n",
      "loss:  0.0\n",
      "sg:  2.4316866397857666\n",
      "sb:  1.641856074333191\n",
      "loss:  0.0\n",
      "sg:  2.4748075008392334\n",
      "sb:  1.661303997039795\n",
      "loss:  0.0\n",
      "sg:  2.450918436050415\n",
      "sb:  1.6335010528564453\n",
      "loss:  0.0\n",
      "sg:  2.497973680496216\n",
      "sb:  1.6683499813079834\n",
      "loss:  0.0\n",
      "sg:  2.4458518028259277\n",
      "sb:  1.6782879829406738\n",
      "loss:  0.0\n",
      "sg:  2.383840560913086\n",
      "sb:  1.6091467142105103\n",
      "loss:  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg:  2.4486687183380127\n",
      "sb:  1.674466848373413\n",
      "loss:  0.0\n",
      "sg:  2.4145143032073975\n",
      "sb:  1.624483346939087\n",
      "loss:  0.0\n",
      "sg:  2.466353416442871\n",
      "sb:  1.6414159536361694\n",
      "loss:  0.0\n",
      "sg:  2.336916208267212\n",
      "sb:  1.6099854707717896\n",
      "loss:  0.0\n",
      "sg:  2.365604877471924\n",
      "sb:  1.6548970937728882\n",
      "loss:  0.0\n",
      "sg:  2.4263298511505127\n",
      "sb:  1.6877131462097168\n",
      "loss:  0.0\n",
      "sg:  2.440871000289917\n",
      "sb:  1.6585395336151123\n",
      "loss:  0.0\n",
      "sg:  2.4778621196746826\n",
      "sb:  1.6961959600448608\n",
      "loss:  0.0\n",
      "------ Training -----\n",
      "train epoch: 9, loss: 0.0\n",
      "\n",
      "------Testing epoch 9/9 --------\n",
      "sg:  2.601405620574951\n",
      "sb:  1.7554138898849487\n",
      "loss:  0.0\n",
      "------ Testing -----\n",
      "test epoch: 9, loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "save_model_path = './saved_models/cnn_im_50_finetune/'\n",
    "for i in range(10):\n",
    "    print('-----Training epoch {}/{} --------'.format(i,9))\n",
    "    tr_loss = train_epoch(image_CNN, matching_CNN, scoring_MLP, train_dataloader, optimizer)\n",
    "    print('train epoch: {}, loss: {}'.format(i, tr_loss))\n",
    "    print()\n",
    "    print('------Testing epoch {}/{} --------'.format(i,9))\n",
    "    tst_loss = test_epoch(image_CNN, matching_CNN, scoring_MLP, test_dataloader)\n",
    "    print('test epoch: {}, loss: {}'.format(i, tst_loss))\n",
    "    \n",
    "    train_losses.append(tr_loss)\n",
    "    test_losses.append(tst_loss)\n",
    "    \n",
    "    save_path_im = save_model_path + 'cnnim_{}.pt'.format(i)\n",
    "    save_path_match= save_model_path + 'cnnmatch_{}.pt'.format(i)\n",
    "    save_path_mlp = save_model_path + 'mlp_{}.pt'.format(i)\n",
    "    \n",
    "    torch.save(image_CNN, save_path_im)\n",
    "    torch.save(matching_CNN, save_path_match)\n",
    "    torch.save(scoring_MLP, save_path_mlp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
