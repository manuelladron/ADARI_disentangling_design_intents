{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResVe v4.2 \n",
    "Eliminating matching CNN and trianing directly on resnet and word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import io\n",
    "import os \n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import pdb\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "def save_json(file_path, data):\n",
    "    out_file = open(file_path, \"w\")\n",
    "    json.dump(data, out_file)\n",
    "    out_file.close()\n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar to visualize progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES\n",
    "im_path_fur = '/home/ubuntu/ADARI/images/v2/full'\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "word_embeddings_path = \"../json_files/fur_5c_50d_sk_glove_ft.json\"\n",
    "word_embeddings_path2 = \"../json_files/fur_v2_5c_50d_adjs.json\"\n",
    "\n",
    "# IMAGE EMBEDDINGS\n",
    "img_embds_id_p = \"../json_files/afur_resnet_emb_id.json\"\n",
    "img_embds_name_p = \"../json_files/afur_resnet_emb_names.json\"\n",
    "\n",
    "# FILES FOR DATALOADER\n",
    "dset_words_p = \"../json_files/ADARI_furniture_words.json\"\n",
    "vocab_p = \"../json_files/ADARI_furniture_vocab_adjs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open json files with embeddings \n",
    "image_embeddings = open_json(img_embds_name_p) \n",
    "dataset_labels = open_json(dset_words_p)        # dictionary image: list of adjectives \n",
    "labels_embeddings = open_json(word_embeddings_path)\n",
    "labels_embeddings_v2 = open_json(word_embeddings_path2)\n",
    "vocab = open_json(vocab_p)                      # vocab only adjectives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7491"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_rate_and_negative_sample(vocab, w2i):\n",
    "    # Returns sampling rate of word (prob of keeping the word ) and negative sampling rate\n",
    "    # 1) variables for sampling_rate\n",
    "    \n",
    "    frequencies_ids = dict()\n",
    "    frequencies = dict()\n",
    "    total_number_words = sum(vocab.values())\n",
    "    threshold = 1e-5\n",
    "    for word, count in vocab.items():\n",
    "        # for sampling rate \n",
    "        z_w = count / total_number_words # this all add up to 1\n",
    "        frequencies[word] = z_w\n",
    "        w_id = w2i[word]\n",
    "        frequencies_ids[w_id] = z_w\n",
    "\n",
    "    # Noise_dist\n",
    "    noise_dist = {key:val**(3/4) for key, val in frequencies.items()}\n",
    "    \n",
    "    # Frequency of dropping\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/frequencies[word]) for word in vocab}\n",
    "    \n",
    "    # Noise dist normalized \n",
    "    Z = sum(noise_dist.values())\n",
    "    neg_sampling = dict()\n",
    "    neg_sampling_ids = dict()\n",
    "    \n",
    "    for k, v in noise_dist.items():\n",
    "        k_id = w2i[k]\n",
    "        n_s_value = v/Z\n",
    "        neg_sampling[k] = n_s_value\n",
    "        neg_sampling_ids[k_id] = n_s_value\n",
    "\n",
    "    return frequencies, frequencies_ids, neg_sampling, neg_sampling_ids, p_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2idx_w2idx(im_words, vocab):\n",
    "    # The 2 dictionaries below for dataset dataloader\n",
    "    im2idx = dict()\n",
    "    idx2im = dict()\n",
    "    w2i = dict()\n",
    "    i2w = dict()\n",
    "    for i,image_name in enumerate(im_words.keys()):\n",
    "        im2idx[image_name] = i\n",
    "        idx2im[i] = image_name\n",
    "\n",
    "    for i, w in enumerate(vocab.keys()):\n",
    "        w2i[w] = i\n",
    "        i2w[i] = w\n",
    "    \n",
    "    return im2idx, idx2im, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2id, id2im, w2i, i2w = im2idx_w2idx(dataset_labels, vocab)\n",
    "s_rate, s_rate_ids, n_rate, n_rate_ids, p_drop = sampling_rate_and_negative_sample(vocab, w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_dict(d):\n",
    "    l = list(d.items())\n",
    "    random.shuffle(l)\n",
    "    d = dict(l)\n",
    "    return d\n",
    "\n",
    "def splitDict(d_img_words, percent, val_number):\n",
    "\n",
    "    val_n = val_number\n",
    "    train_test_size = len(d_img_words) - val_n\n",
    "    train_n = int(train_test_size*percent)\n",
    "    test_n = train_test_size - train_n\n",
    "    \n",
    "    d_img_words = shuffle_dict(d_img_words)\n",
    "    \n",
    "    im_words = iter(d_img_words.items())      \n",
    "    \n",
    "    # Image - words\n",
    "    dtrain = dict(itertools.islice(im_words, train_n))  \n",
    "    dtest= dict(itertools.islice(im_words, test_n))   \n",
    "    dval= dict(itertools.islice(im_words, val_n))\n",
    "    \n",
    "    # Save jsons \n",
    "    path = './train_test_val_mCNN/'\n",
    "    os.makedirs(path)\n",
    "    save_json('./train_test_val_mCNN/trainset.json', dtrain)\n",
    "    save_json('./train_test_val_mCNN/testset.json', dtest)\n",
    "    save_json('./train_test_val_mCNN/valset.json', dval)\n",
    "\n",
    "    print('trainset size: ', len(dtrain), 'trainset dataset size: ',len(dtest), 'val set size: ', len(dval))\n",
    "    return dtrain, dtest, dval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If already created the split dataset, just load the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtrain_w, dtest_w, dval_w = splitDict(dataset_labels, .95, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_w = open_json('./train_test_val_mCNN/trainset.json')\n",
    "dtest_w = open_json('./train_test_val_mCNN/testset.json')\n",
    "dval_w = open_json('./train_test_val_mCNN/valset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "class ADARIdataset(Dataset):\n",
    "    \"\"\"\n",
    "    Receives images and labels.\n",
    "    Returns tensor image and tensor labels\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_data, image_embeddings, w2i, i2w, im2id, id2im,\n",
    "                 s_rate, n_rate, p_drop, img_path):\n",
    "\n",
    "        self.labels_data = labels_data   # dictionary of images -> labels\n",
    "        \n",
    "        self.images_names = list(self.labels_data.keys())    # names\n",
    "        self.images_embeds = list(image_embeddings.values()) # values\n",
    "        \n",
    "        self.w2i = w2i\n",
    "        self.i2w = i2w\n",
    "        self.im2id = im2id\n",
    "        self.id2im = id2im\n",
    "        \n",
    "        self.sampling_r = s_rate\n",
    "        self.neg_sampling = n_rate\n",
    "        self.pdrop = p_drop\n",
    "        \n",
    "        self.image_path = img_path\n",
    "        self.number_adjs = 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_names)\n",
    "    \n",
    "    def get_image_tensor(self, image_name):\n",
    "        \"\"\"\n",
    "        Gets image name and returns a tensor\n",
    "        \"\"\"\n",
    "        name = self.image_path + \"/\" + image_name\n",
    "        img = Image.open(name)\n",
    "        img = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor()])(img)\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    def get_labels_embeddings_from_idx(self, idx):\n",
    "\n",
    "        name_image = self.images_names[idx]\n",
    "        labels = self.labels_data[name_image]\n",
    "        \n",
    "        # Set random distribution for setting a max number of labels = 10\n",
    "        labels1 = np.random.choice(labels, self.number_adjs)\n",
    "        labels2 = np.random.choice(labels, self.number_adjs)\n",
    "        \n",
    "        pos1_idxs = [self.w2i[l] for l in labels1]\n",
    "        pos2_idxs = [self.w2i[l] for l in labels2]\n",
    "        \n",
    "        # Get positive and negative labels\n",
    "        all_idx = list(self.w2i.values())\n",
    "        pos_idxs = []\n",
    "        \n",
    "        # Remove indexes that correspond to the positive labels\n",
    "        for l in labels:\n",
    "            v2i = self.w2i[l]\n",
    "            pos_idxs.append(v2i)\n",
    "            \n",
    "            if v2i in all_idx:\n",
    "                all_idx.remove(v2i)\n",
    "        \n",
    "        # Choose random labels as negative samples -> this can be improved with info about distance of labels\n",
    "        #neg_s_norm = [s/sum(all_idx) for s in all_idx]\n",
    "        \n",
    "        neg_idxs = np.random.choice(all_idx, self.number_adjs) #, p=neg_s_norm)\n",
    "        \n",
    "        neg_samples = []\n",
    "        for n in neg_idxs:\n",
    "            neg_samples.append(self.i2w[n])\n",
    "\n",
    "        assert(len(labels1) == len(neg_samples))\n",
    "        \n",
    "        return torch.LongTensor(pos1_idxs), torch.LongTensor(pos2_idxs), torch.LongTensor(neg_idxs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return tensor image and label index\n",
    "        \"\"\"\n",
    "        name_image = self.images_names[index]\n",
    "        img = self.get_image_tensor(name_image)\n",
    "        \n",
    "        # Get embeddigns of labels\n",
    "        pos1_idxs, pos2_idxs, neg_idxs = self.get_labels_embeddings_from_idx(index) # list size variable \n",
    "        return img, pos1_idxs, pos2_idxs, neg_idxs, name_image\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ADARIdataset(dtrain_w, image_embeddings, w2i, i2w, im2id, id2im,\n",
    "                 s_rate, n_rate, p_drop, im_path_fur)\n",
    "dataset_test = ADARIdataset(dtest_w, image_embeddings, w2i, i2w, im2id, id2im,\n",
    "                 s_rate, n_rate, p_drop, im_path_fur)\n",
    "dataset_val = ADARIdataset(dval_w, image_embeddings, w2i, i2w, im2id, id2im,\n",
    "                 s_rate, n_rate, p_drop, im_path_fur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 8 if cuda else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extract = True\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    input_size = 64\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "resnet, input_size = initialize_model(50, feature_extract)\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2048])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.fc.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_embeddings(vocab, labels_embeddings, labels_embeddings_v2):\n",
    "    vocab_embeddings = dict()\n",
    "    c = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            emb = labels_embeddings[word]\n",
    "            c += 1\n",
    "        except:\n",
    "            emb = labels_embeddings_v2[word]\n",
    "            c += 1\n",
    "        vocab_embeddings[word] = emb\n",
    "    return vocab_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.requires_grad = False\n",
    "    \n",
    "    return emb_layer #, num_embeddings, embedding_dim\n",
    "\n",
    "\n",
    "vocab_embeddings = get_vocab_embeddings(vocab, labels_embeddings, labels_embeddings_v2)\n",
    "# Fill vocabulary with pretrained vectors, if it exists. \n",
    "weights_matrix = torch.zeros((len(w2i), 50))\n",
    "\n",
    "for w, id in w2i.items():\n",
    "    try:\n",
    "        weights_matrix[id] = torch.from_numpy(np.asarray(vocab_embeddings[w]))\n",
    "    except KeyError:\n",
    "        weights_matrix[id] = torch.from_numpy(np.random.normal(scale=0.6, size=(50,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordMLP(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = create_embedding_layer(weights_matrix, False)\n",
    "        self.linear1 = nn.Linear(50, 50) # reshape later to (batch, 10, 50)\n",
    "        \n",
    "    def forward(self, inputs): # inputs [batch, 10]\n",
    "        bs = inputs.shape[0]\n",
    "        embeds = self.embeddings(inputs) # [batch, 50]\n",
    "        embeds = embeds.view(bs, -1) # [batch, 50]\n",
    "        out = self.linear1(embeds) \n",
    "        #out = out.view(bs, 10, 50)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(vec):\n",
    "    norm = vec.norm(p=2, dim=1, keepdim=True)\n",
    "    vec_norm = vec.div(norm)\n",
    "    return vec_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_at_K_batch(ground_truth, predictions, K):\n",
    "    \"\"\"\n",
    "    ground_truth and predictions are [batch, 20]\n",
    "    \"\"\"\n",
    "    batch = ground_truth.shape[0]\n",
    "    running_batch = 0.0\n",
    "    for b in range(batch):\n",
    "        res = R_at_K(ground_truth[b], predictions[b], K)\n",
    "        running_batch += res\n",
    "    return running_batch / batch\n",
    "    \n",
    "def R_at_K(ground_truth, predictions, K):\n",
    "    \"\"\"\n",
    "    Ground truth: vector with 10 idxs\n",
    "    Predictions: vector with 10 idxs\n",
    "    \"\"\"\n",
    "    ground_truth = ground_truth.tolist()\n",
    "    predictions = predictions.tolist()\n",
    "    total_right = 0\n",
    "    for pred in predictions[:K]:\n",
    "        if pred in ground_truth:\n",
    "            total_right += 1\n",
    "    r_at_k = total_right / len(ground_truth)\n",
    "    return r_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(vector, N):\n",
    "    \"\"\"\n",
    "    Return multihot vector with N entries\n",
    "    \"\"\"\n",
    "    print('vector shape: ', vector.shape)\n",
    "    batch = vector.shape[0]\n",
    "    onehot = torch.zeros(batch, N, device=device).scatter_(1, vector.to(device), 1.)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot_onevector(vector, N):\n",
    "    \"\"\"\n",
    "    Use this if batch=1\n",
    "    Return multihot vector with N entries\n",
    "    \"\"\"\n",
    "    onehot = torch.zeros(N, device=device)\n",
    "    for idx in vector:\n",
    "        onehot[idx] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(target, preds, batch):\n",
    "    \"\"\"\n",
    "    Calculates f1, precision, accuracy, lraps, mAP, auc weighted and unweighted\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metrics \n",
    "    N = len(vocab)\n",
    "    target = make_one_hot_onevector(target, N)\n",
    "    SAMPLE_WEIGHT = compute_sample_weight('balanced', target.to(\"cpu\"))\n",
    "    preds = make_one_hot_onevector(preds, N)\n",
    "    \n",
    "    # WEIGHTED \n",
    "    f1 = f1_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\", sample_weight=SAMPLE_WEIGHT)*batch\n",
    "    precision = precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\",sample_weight=SAMPLE_WEIGHT)*batch\n",
    "    accuracy = accuracy_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(),sample_weight=SAMPLE_WEIGHT)*batch\n",
    "    #lraps = label_ranking_average_precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(),sample_weight=SAMPLE_WEIGHT)*batch\n",
    "    mAP = average_precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\",sample_weight=SAMPLE_WEIGHT)*batch\n",
    "#     auc = roc_auc_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\", sample_weight=SAMPLE_WEIGHT)*batch\n",
    "    \n",
    "\n",
    "    # UNWEIGHTED\n",
    "    f1_ = f1_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\")*batch\n",
    "    precision_ = precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\")*batch\n",
    "    accuracy_ = accuracy_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy())*batch\n",
    "    #lraps_ = label_ranking_average_precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy())*batch\n",
    "    mAP_ = average_precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\")*batch\n",
    "#     auc_ = roc_auc_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"macro\")*batch\n",
    "\n",
    "    \n",
    "    return f1, precision, accuracy, 0, mAP, 0, f1_, precision_, accuracy_, 0, mAP_, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(img_cnn, word_mlp, loader, optimizer, criterion):\n",
    "    img_cnn.train()\n",
    "    word_mlp.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    running_corrects = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    running_lraps = 0.0\n",
    "    running_map = 0.0\n",
    "    running_auc = 0.0\n",
    "\n",
    "    running_corrects_ = 0.0\n",
    "    running_precision_ = 0.0\n",
    "    running_accuracy_ = 0.0\n",
    "    running_lraps_ = 0.0\n",
    "    running_map_ = 0.0\n",
    "    running_auc_ = 0.0\n",
    "    \n",
    "    running_r_at_k = [0.0, 0.0, 0.0]\n",
    "    result = []\n",
    "    result_ = []\n",
    "    \n",
    "    emb_words = list(vocab_embeddings.keys())\n",
    "    embeds = list(vocab_embeddings.values())\n",
    "    word_embeddings_np = np.asarray(embeds)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, pos1_idxs, pos2_idxs, neg_idxs, name_image) in enumerate(loader): \n",
    "        \n",
    "        # Data to device \n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Ground truth labels \n",
    "        pos1_idxs = pos1_idxs.to(device) # [batch, 10]\n",
    "        pos2_idxs = pos2_idxs.to(device) # [batch, 10]\n",
    "        neg_idxs = neg_idxs.to(device)   # [batch, 10]\n",
    "        \n",
    "        # image_CNN\n",
    "        im_repres = img_cnn(data)     # [batch, 50]\n",
    "        \n",
    "        # word Embeddings\n",
    "        pos1_label_embs = word_mlp(pos1_idxs).squeeze(1) # [batch, 10, 50]\n",
    "        pos2_label_embs = word_mlp(pos2_idxs).squeeze(1)# [batch, 10, 50]\n",
    "        neg_label_embs = word_mlp(neg_idxs).squeeze(1)  # [batch, 10, 50]\n",
    "        \n",
    "        # Triplet loss \n",
    "        loss = criterion(im_repres, pos1_label_embs, neg_label_embs)\n",
    "        running_loss += loss.item() * data.shape[0]\n",
    "        \n",
    "        # Get label predictions (K-neighbors words of the image embedding)\n",
    "        neigh = NearestNeighbors(n_neighbors=20) # initilize algorithm\n",
    "        neigh.fit(word_embeddings_np)           # fit population (words)\n",
    "        d, wids = neigh.kneighbors(im_repres.detach().cpu().numpy())   # find the 10 nearest words to the image\n",
    "        preds = wids.squeeze()#.tolist() # array of indices type Long (int)\n",
    "        \n",
    "        # Metrics \n",
    "        targets = torch.cat((pos1_idxs, pos2_idxs), dim=1) #[batch, 20]\n",
    "        \n",
    "        # Recall @ K for k = 1, 5, 10\n",
    "        K = [1,5,10]\n",
    "        r_at_k_list = []\n",
    "        \n",
    "        for k in K:\n",
    "            r_at_k = R_at_K_batch(targets, preds, k)\n",
    "            r_at_k_list.append(r_at_k)\n",
    "        \n",
    "        for i, ratk in enumerate(r_at_k_list):\n",
    "            running_r_at_k[i] += ratk * data.shape[0]\n",
    "            \n",
    "        f1, precision, accuracy, lraps, mAP, auc, f1_, precision_, accuracy_, lraps_, mAP_, auc_ = calculate_metrics(targets.long(), torch.from_numpy(preds), data.shape[0])\n",
    "        \n",
    "        running_corrects += f1\n",
    "        running_precision += precision\n",
    "        running_accuracy += accuracy\n",
    "        running_lraps += lraps\n",
    "        running_map += mAP\n",
    "        running_auc += auc\n",
    "\n",
    "        running_corrects_ += f1_\n",
    "        running_precision_ += precision_\n",
    "        running_accuracy_ += accuracy_\n",
    "        running_lraps_ += lraps_\n",
    "        running_map_ += mAP_\n",
    "        running_auc_ += auc_\n",
    "\n",
    "        if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "            print('loss: ', loss.item() * data.shape[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        \n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Epoch loss\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    \n",
    "    # Epoch metrics \n",
    "    epoch_f1 = running_corrects / len(loader.dataset)\n",
    "    epoch_precision = running_precision / len(loader.dataset)\n",
    "    epoch_acc = running_accuracy / len(loader.dataset)\n",
    "    epoch_auc = running_auc / len(loader.dataset)\n",
    "    epoch_lraps = running_lraps / len(loader.dataset)\n",
    "    epoch_map = running_map / len(loader.dataset)\n",
    "    \n",
    "    epoch_f1_ = running_corrects_ / len(loader.dataset)\n",
    "    epoch_precision_ = running_precision_ / len(loader.dataset)\n",
    "    epoch_acc_ = running_accuracy_ / len(loader.dataset)\n",
    "    epoch_auc_ = running_auc_ / len(loader.dataset)\n",
    "    epoch_lraps_ = running_lraps_ / len(loader.dataset)\n",
    "    epoch_map_ = running_map_ / len(loader.dataset)\n",
    "    \n",
    "    # R@K metric\n",
    "    for r in running_r_at_k:\n",
    "        r = r/len(loader.dataset)\n",
    "    \n",
    "    print('------ Training -----')\n",
    "    result.append('WEIGHTED Training Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f} mAP: {:.4f} lraps: {:.4f}'.format(epoch_loss, epoch_f1, epoch_acc, epoch_precision, epoch_auc, epoch_map, epoch_lraps))\n",
    "    print(result)\n",
    "    result_.append('UNWEIGHTED Training Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f} mAP: {:.4f} lraps: {:.4f}'.format(epoch_loss, epoch_f1_, epoch_acc_, epoch_precision_, epoch_auc_, epoch_map_, epoch_lraps_))\n",
    "    print(result_)\n",
    "    print('R@K=1: {:.4f}  R@K=5: {:.4f} R@K=10: {:.4f}'.format(running_r_at_k[0], running_r_at_k[1], running_r_at_k[2]))\n",
    "    \n",
    "    return epoch_loss, result, result_, epoch_f1, epoch_f1_, epoch_auc, epoch_auc_, epoch_lraps, epoch_lraps_, running_r_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(img_cnn, word_mlp, loader, criterion):\n",
    "    img_cnn.eval()\n",
    "    word_mlp.eval()\n",
    "    \n",
    "    running_loss = 0.0 \n",
    "    \n",
    "    running_corrects = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    running_lraps = 0.0\n",
    "    running_map = 0.0\n",
    "    running_auc = 0.0\n",
    "\n",
    "    running_corrects_ = 0.0\n",
    "    running_precision_ = 0.0\n",
    "    running_accuracy_ = 0.0\n",
    "    running_lraps_ = 0.0\n",
    "    running_map_ = 0.0\n",
    "    running_auc_ = 0.0\n",
    "    \n",
    "    running_r_at_k = [0.0, 0.0, 0.0]\n",
    "    result = []\n",
    "    result_ = []\n",
    "\n",
    "    emb_words = list(vocab_embeddings.keys())\n",
    "    embeds = list(vocab_embeddings.values())\n",
    "    word_embeddings_np = np.asarray(embeds)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, pos1_idxs, pos2_idxs, neg_idxs, name_image) in enumerate(loader):  \n",
    "            \n",
    "            # Data to device \n",
    "            data = data.to(device)\n",
    "        \n",
    "            # Ground truth labels \n",
    "            pos1_idxs = pos1_idxs.to(device) # [batch, 10]\n",
    "            pos2_idxs = pos2_idxs.to(device) # [batch, 10]\n",
    "            neg_idxs = neg_idxs.to(device)   # [batch, 10]\n",
    "            \n",
    "            # image_CNN\n",
    "            im_repres = img_cnn(data) # [batch, 50]\n",
    "            \n",
    "            # word Embeddings\n",
    "            pos1_label_embs = word_mlp(pos1_idxs).squeeze(1) # [batch, 50]\n",
    "            pos2_label_embs = word_mlp(pos2_idxs).squeeze(1) # [batch, 50]\n",
    "            neg_label_embs = word_mlp(neg_idxs).squeeze(1)   # [batch, 50]\n",
    "\n",
    "            # Triplet loss \n",
    "            loss = criterion(im_repres, pos1_label_embs, neg_label_embs)\n",
    "            running_loss += loss.item() * data.shape[0]\n",
    "\n",
    "            # Get label predictions (K-neighbors words of the image embedding)\n",
    "            neigh = NearestNeighbors(n_neighbors=20) # initilize algorithm\n",
    "            neigh.fit(word_embeddings_np)           # fit population (words)\n",
    "            d, wids = neigh.kneighbors(im_repres.detach().cpu().numpy())   # find the 10 nearest words to the image\n",
    "            preds = wids.squeeze()#.tolist() # array of indices type Long (int)\n",
    "\n",
    "            # Metrics \n",
    "            targets = torch.cat((pos1_idxs, pos2_idxs), dim=1) #[batch, 20]\n",
    "\n",
    "            K = [1,5,10]\n",
    "            r_at_k_list = []\n",
    "\n",
    "            for k in K:\n",
    "                r_at_k = R_at_K_batch(targets, preds, k)\n",
    "                r_at_k_list.append(r_at_k)\n",
    "\n",
    "            for i, ratk in enumerate(r_at_k_list):\n",
    "                running_r_at_k[i] += ratk * data.shape[0]\n",
    "\n",
    "            f1, precision, accuracy, lraps, mAP, auc, f1_, precision_, accuracy_, lraps_, mAP_, auc_ = calculate_metrics(targets.long(), torch.from_numpy(preds), data.shape[0])\n",
    "\n",
    "            running_corrects += f1\n",
    "            running_precision += precision\n",
    "            running_accuracy += accuracy\n",
    "            running_lraps += lraps\n",
    "            running_map += mAP\n",
    "            running_auc += auc\n",
    "\n",
    "            running_corrects_ += f1_\n",
    "            running_precision_ += precision_\n",
    "            running_accuracy_ += accuracy_\n",
    "            running_lraps_ += lraps_\n",
    "            running_map_ += mAP_\n",
    "            running_auc_ += auc_\n",
    "\n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "                print('loss: ', loss.item() * data.shape[0])\n",
    "    \n",
    "    # Epoch loss\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    \n",
    "    # Epoch metrics \n",
    "    epoch_f1 = running_corrects / len(loader.dataset)\n",
    "    epoch_precision = running_precision / len(loader.dataset)\n",
    "    epoch_acc = running_accuracy / len(loader.dataset)\n",
    "    epoch_auc = running_auc / len(loader.dataset)\n",
    "    epoch_lraps = running_lraps / len(loader.dataset)\n",
    "    epoch_map = running_map / len(loader.dataset)\n",
    "    \n",
    "    epoch_f1_ = running_corrects_ / len(loader.dataset)\n",
    "    epoch_precision_ = running_precision_ / len(loader.dataset)\n",
    "    epoch_acc_ = running_accuracy_ / len(loader.dataset)\n",
    "    epoch_auc_ = running_auc_ / len(loader.dataset)\n",
    "    epoch_lraps_ = running_lraps_ / len(loader.dataset)\n",
    "    epoch_map_ = running_map_ / len(loader.dataset)\n",
    "    \n",
    "    # R@K metric\n",
    "    for r in running_r_at_k:\n",
    "        r = r/len(loader.dataset)\n",
    "    \n",
    "    print('------ Testing -----')\n",
    "    result.append('WEIGHTED Testing Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f} mAP: {:.4f} lraps: {:.4f}'.format(epoch_loss, epoch_f1, epoch_acc, epoch_precision, epoch_auc, epoch_map, epoch_lraps))\n",
    "    print(result)\n",
    "    result_.append('UNWEIGHTED Testing Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f} mAP: {:.4f} lraps: {:.4f}'.format(epoch_loss, epoch_f1_, epoch_acc_, epoch_precision_, epoch_auc_, epoch_map_, epoch_lraps_))\n",
    "    print(result_)\n",
    "    print('R@K=1: {:.4f}  R@K=5: {:.4f} R@K=10: {:.4f}'.format(running_r_at_k[0], running_r_at_k[1], running_r_at_k[2]))\n",
    "    \n",
    "    return epoch_loss, result, result_, epoch_f1, epoch_f1_, epoch_auc, epoch_auc_, epoch_lraps, epoch_lraps_, running_r_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score, label_ranking_average_precision_score, average_precision_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "\n",
    "# Models\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "image_CNN = resnet.to(device)\n",
    "word_MLP = wordMLP(50).to(device)\n",
    "criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "\n",
    "params_to_update = image_CNN.parameters()\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in image_CNN.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name, param in image_CNN.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "params = params_to_update + list(word_MLP.parameters()) \n",
    "\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCheckpoint(filename, batch_size, epoch, model):\n",
    "    checkpoint = {\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              \"batch_size\":batch_size,\n",
    "    } # save all important stuff\n",
    "    torch.save(checkpoint , filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training epoch 0/9 --------\n",
      "loss:  9.848185539245605\n",
      "loss:  6.553659915924072\n",
      "loss:  11.914786338806152\n",
      "loss:  16.0324764251709\n",
      "loss:  13.644606590270996\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.2010 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.2010 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "R@K=1: 0.5000  R@K=5: 12.0000 R@K=10: 20.5000\n",
      "train epoch: 0, loss: 0.20099637627012207\n",
      "\n",
      "------Testing epoch 0/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1595 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1595 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.5000\n",
      "test epoch: 0, loss: 0.1594698626390645\n",
      "-----Training epoch 1/9 --------\n",
      "loss:  12.587745666503906\n",
      "loss:  10.830168724060059\n",
      "loss:  11.509572982788086\n",
      "loss:  14.712960243225098\n",
      "loss:  11.381413459777832\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1696 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1696 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "R@K=1: 0.0000  R@K=5: 4.0000 R@K=10: 13.5000\n",
      "train epoch: 1, loss: 0.169560775702315\n",
      "\n",
      "------Testing epoch 1/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1591 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1591 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.5000\n",
      "test epoch: 1, loss: 0.15907935014632946\n",
      "-----Training epoch 2/9 --------\n",
      "loss:  12.411493301391602\n",
      "loss:  6.355612754821777\n",
      "loss:  8.023621559143066\n",
      "loss:  10.698636054992676\n",
      "loss:  14.316650390625\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1659 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1659 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "R@K=1: 0.0000  R@K=5: 3.5000 R@K=10: 11.5000\n",
      "train epoch: 2, loss: 0.16587984107333445\n",
      "\n",
      "------Testing epoch 2/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1554 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1554 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 2, loss: 0.15544512790973197\n",
      "-----Training epoch 3/9 --------\n",
      "loss:  11.197178840637207\n",
      "loss:  12.223234176635742\n",
      "loss:  11.68171501159668\n",
      "loss:  9.195062637329102\n",
      "loss:  8.19893741607666\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1642 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1642 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "R@K=1: 0.0000  R@K=5: 5.5000 R@K=10: 14.0000\n",
      "train epoch: 3, loss: 0.1642424463820546\n",
      "\n",
      "------Testing epoch 3/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1495 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1495 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 3, loss: 0.1495209792410264\n",
      "-----Training epoch 4/9 --------\n",
      "loss:  7.216782093048096\n",
      "loss:  11.623936653137207\n",
      "loss:  7.205959796905518\n",
      "loss:  10.163244247436523\n",
      "loss:  9.169971466064453\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1634 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1634 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "R@K=1: 0.0000  R@K=5: 7.0000 R@K=10: 19.5000\n",
      "train epoch: 4, loss: 0.16338391925481105\n",
      "\n",
      "------Testing epoch 4/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1494 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1494 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 4, loss: 0.149444418963692\n",
      "-----Training epoch 5/9 --------\n",
      "loss:  12.282386779785156\n",
      "loss:  13.627851486206055\n",
      "loss:  10.896154403686523\n",
      "loss:  8.343754768371582\n",
      "loss:  9.793256759643555\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1622 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1622 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.5000  R@K=5: 6.5000 R@K=10: 11.5000\n",
      "train epoch: 5, loss: 0.16219605488520764\n",
      "\n",
      "------Testing epoch 5/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1499 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1499 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 5, loss: 0.149925151481315\n",
      "-----Training epoch 6/9 --------\n",
      "loss:  12.57336139678955\n",
      "loss:  13.432785987854004\n",
      "loss:  11.452260971069336\n",
      "loss:  9.780936241149902\n",
      "loss:  9.122478485107422\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1600 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1600 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 3.5000 R@K=10: 10.0000\n",
      "train epoch: 6, loss: 0.16004940332529424\n",
      "\n",
      "------Testing epoch 6/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1482 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1482 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 6, loss: 0.14824532364175913\n",
      "-----Training epoch 7/9 --------\n",
      "loss:  9.870919227600098\n",
      "loss:  7.959348678588867\n",
      "loss:  11.442828178405762\n",
      "loss:  9.364204406738281\n",
      "loss:  8.194127082824707\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1578 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1578 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "R@K=1: 0.0000  R@K=5: 5.0000 R@K=10: 20.0000\n",
      "train epoch: 7, loss: 0.15783110726233318\n",
      "\n",
      "------Testing epoch 7/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1498 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1498 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 7, loss: 0.14981937184580055\n",
      "-----Training epoch 8/9 --------\n",
      "loss:  7.508061408996582\n",
      "loss:  11.08574390411377\n",
      "loss:  9.75009536743164\n",
      "loss:  7.654791831970215\n",
      "loss:  10.696969032287598\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1570 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1570 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 4.0000 R@K=10: 12.0000\n",
      "train epoch: 8, loss: 0.15703212923850354\n",
      "\n",
      "------Testing epoch 8/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1479 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1479 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 8, loss: 0.14789580060562618\n",
      "-----Training epoch 9/9 --------\n",
      "loss:  7.685901641845703\n",
      "loss:  12.68801212310791\n",
      "loss:  11.634737968444824\n",
      "loss:  16.585594177246094\n",
      "loss:  11.525494575500488\n",
      "------ Training -----\n",
      "['WEIGHTED Training Loss: 0.1539 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "['UNWEIGHTED Training Loss: 0.1539 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0004']\n",
      "R@K=1: 0.0000  R@K=5: 3.0000 R@K=10: 13.5000\n",
      "train epoch: 9, loss: 0.15387106448854004\n",
      "\n",
      "------Testing epoch 9/9 --------\n",
      "------ Testing -----\n",
      "['WEIGHTED Testing Loss: 0.1487 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "['UNWEIGHTED Testing Loss: 0.1487 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.0000 mAP: nan lraps: 0.0003']\n",
      "R@K=1: 0.0000  R@K=5: 0.0000 R@K=10: 0.0000\n",
      "test epoch: 9, loss: 0.14872186389607442\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "train_w_results = []\n",
    "train_uw_results = []\n",
    "test_w_results = []\n",
    "test_uw_results = []\n",
    "\n",
    "train_ratks = []\n",
    "test_ratks = []\n",
    "\n",
    "save_model_path = './saved_models/cnn_im_v4.2/'\n",
    "if os.path.isdir(save_model_path) == False:\n",
    "    os.makedirs(save_model_path)\n",
    "\n",
    "best_loss = 1000.0\n",
    "diff_train_test_losses = [1000]\n",
    "for i in range(10):\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "    print('-----Training epoch {}/{} --------'.format(i,9))\n",
    "    tr_loss, tr_weighted_res, tr_unweighted_res, tr_w_f1, tr_uw_f1, \\\n",
    "    tr_w_auc, tr_uw_auc, tr_w_lraps, tr_uw_lraps, tr_ratk= train_epoch(image_CNN, word_MLP, train_dataloader, optimizer, criterion)\n",
    "    print('train epoch: {}, loss: {}'.format(i, tr_loss))\n",
    "    print()\n",
    "    print('------Testing epoch {}/{} --------'.format(i,9))\n",
    "    tst_loss, tst_weighted_res, tst_unweighted_res, tst_w_f1, tst_uw_f1, \\\n",
    "    tst_w_auc, tst_uw_auc, tst_w_lraps, tst_uw_lraps, tst_ratk = test_epoch(image_CNN, word_MLP, test_dataloader, criterion)\n",
    "    print('test epoch: {}, loss: {}'.format(i, tst_loss))\n",
    "    \n",
    "    train_losses.append(tr_loss)\n",
    "    test_losses.append(tst_loss)\n",
    "    \n",
    "    train_w_results.append(tr_weighted_res)\n",
    "    train_uw_results.append(tr_unweighted_res)\n",
    "    \n",
    "    test_w_results.append(tst_weighted_res)\n",
    "    test_uw_results.append(tst_unweighted_res)\n",
    "    \n",
    "    train_ratks.append(tr_ratk)\n",
    "    test_ratks.append(tst_ratk)\n",
    "\n",
    "    diff_train_test = np.absolute(tr_loss-tst_loss)\n",
    "\n",
    "\n",
    "save_path_resnet = save_model_path + 'resnet_{}.pt'.format(i)\n",
    "save_path_wordmlp = save_model_path + 'word_mlp_{}.pt'.format(i)\n",
    "createCheckpoint(save_path_resnet, 64, i, image_CNN)\n",
    "createCheckpoint(save_path_wordmlp, 64, i, word_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(epochs, train, test, train_name, val_name, name_long, name_short):\n",
    "    plt.plot(epochs, train, 'g', label=train_name, c=\"mediumvioletred\")\n",
    "    plt.plot(epochs, test, 'b', label=val_name, c=\"darkturquoise\")\n",
    "    plt.title(name_long)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(name_short)\n",
    "    plt.legend()\n",
    "    plt.savefig('ADARI_resnet152_pretrained_adam0001.pdf', dpi=300)\n",
    "    plt.savefig('ADARI_resnet152_pretrained_adam0001.png', dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [e for e in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_graph(epochs_list, train_losses, test_losses, 'train', 'test', 'train_test_loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_losses\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(str(item) for item in train_losses))\n",
    "with open(\"test_losses\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(str(item) for item in test_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './saved_models/cnn_im_v4.2/'\n",
    "checkpoint = torch.load(save_model_path + \"resnet_9.pt\")\n",
    "image_CNN.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "batch_size = checkpoint['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './saved_models/cnn_im_v4.2/'\n",
    "checkpoint = torch.load(save_model_path + \"word_mlp_9.pt\")\n",
    "word_MLP.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "batch_size = checkpoint['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'mCNN_v4.2_word_embeddings.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(object):\n",
    "    def __init__(self, file_name, model):\n",
    "        self.file_name = file_name\n",
    "        self.create_embedding(model)\n",
    "        self.save_json()\n",
    "\n",
    "    def create_embedding(self, model):\n",
    "        embeddings = model.embeddings.weight.cpu().data.numpy()\n",
    "        self.embedding = {}\n",
    "        for id, w in i2w.items():\n",
    "            e = embeddings[id].tolist()\n",
    "            self.embedding[w] = e\n",
    "        \n",
    "    def save_json(self):\n",
    "        out_file = open(self.file_name, \"w\")\n",
    "        json.dump(self.embedding, out_file)\n",
    "        out_file.close()\n",
    "    \n",
    "E = Embedding(save_path, word_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDINGS = open_json(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create all image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "from matplotlib.image import imread\n",
    "from torchvision.transforms import ToPILImage\n",
    "import warnings\n",
    "\n",
    "def showImagesHorizontally(list_of_files):\n",
    "    fig = figure(figsize=(20,20))\n",
    "    number_of_files = len(list_of_files)\n",
    "    for i in range(number_of_files):\n",
    "        a=fig.add_subplot(1,number_of_files,i+1)\n",
    "        image = imread(list_of_files[i])\n",
    "        imshow(image)\n",
    "        axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_tensor(image_name, im_path):\n",
    "    \"\"\"\n",
    "    Gets image name and returns a tensor\n",
    "    \"\"\"\n",
    "    name = im_path + \"/\" + image_name\n",
    "    img = Image.open(name)\n",
    "    img = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor()])(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_im_embeddings(img_cnn, im_path):\n",
    "    im_embds_name = list(image_embeddings.keys())  # names\n",
    "    im_embds_emb = list(image_embeddings.values()) # embeddings\n",
    "    \n",
    "    all_embeds = dict()\n",
    "    for name in im_embds_name:\n",
    "        im = get_image_tensor(name, im_path).unsqueeze(0)\n",
    "        emb_im = img_cnn(im.to(device)).squeeze(0).detach().cpu().numpy().reshape(1,-1) # [1,50]\n",
    "        all_embeds[name] = emb_im\n",
    "    return all_embeds\n",
    "\n",
    "#all_embeds = get_im_embeddings(image_CNN, im_path_fur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = dict()\n",
    "for k,v in all_embeds.items():\n",
    "    new_d[k] = v.tolist()\n",
    "\n",
    "save_json('mCNN_v4.2_image_embeddings.json', new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EMBEDDINGS_RN = open_json('mCNN_v4.2_image_embeddings.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
