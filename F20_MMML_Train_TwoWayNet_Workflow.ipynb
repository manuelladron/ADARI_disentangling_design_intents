{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/alexschneidman/CMU/F20/777/TwoWayNets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import pdb\n",
    "\n",
    "from model import TwoWayNet\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES\n",
    "im_path_fur = \"../ADARI/images/furniture/thumbs/big\"\n",
    "\n",
    "# ResNet IMAGE EMBEDDINGS\n",
    "image_embeddings_path = \"../ADARI/image_embeddings/resnet_image_embeddings.json\"\n",
    "\n",
    "# JSON_FILES\n",
    "data_path_fur = \"../ADARI/json_files/cleaned/furniture_cleaned.json\"\n",
    "\n",
    "# FURNITURE VOCAB \n",
    "vocab_id2w = \"../ADARI/json_files/vocabulary/furniture/vocab_id2w.json\"\n",
    "vocab_w2id = \"../ADARI/json_files/vocabulary/furniture/vocab_w2id.json\"\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "word_embeddings_path = \"../ADARI/word_embeddings/fur_5c_50d_sk_glove_ft.json\"\n",
    "\n",
    "# FILES FOR DATALOADER\n",
    "dset_path = '../ADARI/json_files/dataset_for_dataloader/dset_dataloader.json'\n",
    "im2idx_path = '../ADARI/json_files/dataset_for_dataloader/im2idx.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = open_json(word_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embs = open_json(image_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = open_json(vocab_w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = open_json(dset_path)\n",
    "im2idx = open_json(im2idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "class ResnetImagesGloveWordsDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 img_name_to_words, \n",
    "                 im2idx, \n",
    "                 word_embds, \n",
    "                 img_embds, \n",
    "                 train=True, \n",
    "                 device=None):\n",
    "        # word_embds is word -> embedding\n",
    "        self.word_embds = word_embds\n",
    "        # img_embeds is img_idx -> embedding\n",
    "        self.img_embds = img_embds\n",
    "        # Dataset is image_name -> [words]\n",
    "        self.img_name_to_words = img_name_to_words\n",
    "        self.images = list(img_name_to_words.keys())\n",
    "        self.img_embed_shape = len(img_embds[list(img_embds.keys())[0]])\n",
    "        self.im2idx = im2idx\n",
    "        \n",
    "        \n",
    "        self.max_words = 40\n",
    "        self.word_shape = len(word_embds[list(word_embds.keys())[0]])\n",
    "        self.word_embed_shape = self.word_shape * self.max_words\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        idx = self.im2idx[image_name]\n",
    "        \n",
    "        words = self.img_name_to_words[image_name]\n",
    "        w_embds = []\n",
    "        for w in words[:min(self.max_words, len(words))]:\n",
    "            if w in self.word_embds:\n",
    "                w_embds.append(torch.tensor(self.word_embds[w], device=self.device)\n",
    "                               .reshape(self.word_shape, 1))\n",
    "        # pad the rest\n",
    "        for _ in range(self.max_words - len(w_embds)):\n",
    "            w_embds.append(torch.full((self.word_shape, 1), 0.0, device=self.device))\n",
    "            \n",
    "        w_concat = torch.cat(w_embds)\n",
    "        \n",
    "        return torch.tensor(self.img_embds[str(idx)]), w_concat.reshape((w_concat.shape[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ResnetImagesGloveWordsDataset(dset, im2idx, word_embs, img_embs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, params, img_shape, words_shape, device=None):\n",
    "    model = TwoWayNet(img_shape, \n",
    "                      words_shape, \n",
    "                      params.LAYER_SIZES, \n",
    "                      params.TEST_LAYER, \n",
    "                      params.DROP_PROBABILITY)\n",
    "    model.to(device)\n",
    "    optim = torch.optim.SGD(model.parameters(), \n",
    "                            params.BASE_LEARNING_RATE, \n",
    "                            nesterov=True, \n",
    "                            momentum=Params.MOMENTUM)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for epoch in range(params.EPOCH_NUMBER):\n",
    "        avg_loss = 0\n",
    "        for img_embed, words_embed in dataloader:\n",
    "            img_embed = img_embed.to(device)\n",
    "            words_embed = words_embed.to(device)\n",
    "            \n",
    "            print(f\"img shape: {img_embed.shape}\")\n",
    "            print(f\"words_embed shape: {words_embed.shape}\")\n",
    "            \n",
    "            data = {\"x\": img_embed, \"y\": words_embed}\n",
    "            optim.zero_grad()\n",
    "            # Forward pass\n",
    "            xprime, yprime, hidden_xs, hidden_ys = model(data)\n",
    "\n",
    "            # Compute losses\n",
    "            # reconstruction losses\n",
    "            loss_x = mse_loss(xprime, data[\"x\"])\n",
    "            loss_y = mse_loss(yprime, data[\"y\"])\n",
    "\n",
    "            # hidden loss\n",
    "            loss_hidden = mse_loss(hidden_xs[model.hidden_output_layer], hidden_ys[model.hidden_output_layer])\n",
    "\n",
    "            # covariance loss\n",
    "            hidden_xs_tensor, hidden_ys_tensor = torch.stack(hidden_xs), torch.stack(hidden_ys)\n",
    "            cov_x = torch.dot(hidden_xs_tensor.T, hidden_xs_tensor) / hidden_xs_tensor.shape[0]\n",
    "            cov_y = torch.dot(hidden_ys_tensor.T, hidden_ys_tensor) / hidden_ys_tensor.shape[0]\n",
    "\n",
    "            cov_loss_x = torch.sqrt(torch.sum(cov_x ** 2)) - torch.sqrt(torch.sum(torch.diag(cov_x) ** 2))\n",
    "            cov_loss_y = torch.sqrt(torch.sum(cov_y ** 2)) - torch.sqrt(torch.sum(torch.diag(cov_y) ** 2))\n",
    "\n",
    "            # Weight Decay loss\n",
    "            loss_weight_decay = model.get_summed_l2_weights()\n",
    "            \n",
    "            # Gamma loss\n",
    "            loss_gamma = model.get_summed_gammas()\n",
    "\n",
    "            loss = (params.LOSS_X * loss_x + \n",
    "                    params.LOSS_Y * loss_y + \n",
    "                    params.L2_LOSS * loss_hidden + \n",
    "                    params.WITHEN_REG_X * cov_loss_x + \n",
    "                    params.WITHIN_REG_Y * cov_loss_y + \n",
    "                    params.GAMMA_COEF * loss_gamma +\n",
    "                    params.WEIGHT_DECAY * loss_weight_decay)\n",
    "            \n",
    "            # Backward step\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # Update step\n",
    "            optim.step()\n",
    "        losses.append(avg_loss / len(dataset))\n",
    "        print(f\"Epoch {epoch+1}, loss: {losses[-1]}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "\n",
    "    # region Training Params\n",
    "    BATCH_SIZE = 128\n",
    "    VALIDATION_BATCH_SIZE = 1000\n",
    "    EPOCH_NUMBER = 100\n",
    "    DECAY_RATE = 0.5\n",
    "    BASE_LEARNING_RATE = 0.0001\n",
    "    MOMENTUM = 0.9\n",
    "    # endregion\n",
    "\n",
    "    # region Loss Weights\n",
    "    WEIGHT_DECAY = 0.05\n",
    "    GAMMA_COEF = 0.05\n",
    "    WITHEN_REG_X = 0.5\n",
    "    WITHEN_REG_Y = 0.5\n",
    "    L2_LOSS = 0.25\n",
    "    LOSS_X = 1\n",
    "    LOSS_Y = 1\n",
    "    # endregion\n",
    "\n",
    "    # region Architecture\n",
    "    LAYER_SIZES = [2000, 3000, 16000]\n",
    "    TEST_LAYER = 1\n",
    "    DROP_PROBABILITY = 0.5\n",
    "    LEAKINESS = 0.3\n",
    "    # endregion\n",
    "\n",
    "    @classmethod\n",
    "    def print_params(cls):\n",
    "        OutputLog().write('Params:\\n')\n",
    "        for (key, value) in cls.__dict__.iteritems():\n",
    "            if not key.startswith('__'):\n",
    "                OutputLog().write('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder shapes:\n",
      "D_in: 2048, D_out: 2000\n",
      "D_in: 2000, D_out: 3000\n",
      "D_in: 3000, D_out: 16000\n",
      "D_in: 16000, D_out: 2000\n",
      "Y Encoder shapes:\n",
      "D_in: 2000, D_out: 16000\n",
      "D_in: 16000, D_out: 3000\n",
      "D_in: 3000, D_out: 2000\n",
      "D_in: 2000, D_out: 2048\n",
      "img shape: torch.Size([128, 2048])\n",
      "words_embed shape: torch.Size([128, 2000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2000 and 3000 in dimension 2 at ../aten/src/TH/generic/THTensor.cpp:612",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1f05ee88b1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_embed_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embed_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-b7baded746e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, params, img_shape, words_shape, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# covariance loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mhidden_xs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_ys_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mcov_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_xs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_xs_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mhidden_xs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mcov_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_ys_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_ys_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mhidden_ys_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2000 and 3000 in dimension 2 at ../aten/src/TH/generic/THTensor.cpp:612"
     ]
    }
   ],
   "source": [
    "params = Params()\n",
    "dataloader = DataLoader(dataset, batch_size=params.BATCH_SIZE, shuffle=True)\n",
    "model = train(dataloader, params, dataset.img_embed_shape, dataset.word_embed_shape, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.x_encoder[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
