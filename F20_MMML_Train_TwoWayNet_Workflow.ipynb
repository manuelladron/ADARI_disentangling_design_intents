{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# DIRECTORY STRUCTURE\n",
    "\"\"\"\n",
    "ADARI/\n",
    "    .....\n",
    "mmml_f20/\n",
    "    this\n",
    "TwoWayNets/\n",
    "    .....\n",
    "\"\"\"\n",
    "# REPLACE WITH PATH TO TwoWayNets Repo\n",
    "sys.path.append('../TwoWayNets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device - cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "from model import TwoWayNet\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet IMAGE EMBEDDINGS\n",
    "image_embeddings_path = \"../ADARI/image_embeddings/resnet_image_embeddings.json\"\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "word_embeddings_path = \"../ADARI/word_embeddings/fur_5c_50d_sk_glove_ft.json\"\n",
    "\n",
    "# FILES FOR DATALOADER\n",
    "dset_words_p = \"../ADARI/json_files/ADARI_FUR_images_sentences_words/ADARI_v2_FUR_images_words.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = open_json(word_embeddings_path)\n",
    "\n",
    "dset_words = open_json(dset_words_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embs = open_json(image_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date(word):\n",
    "    rx = r\"[0-9]+(?:st|[nr]d|th)\"\n",
    "    if re.findall(rx, word, flags=re.I) != []:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# The 2 dictionaries below for dataset dataloader\n",
    "im2idx = dict()\n",
    "im_words = dict()\n",
    "\n",
    "# Temp lists \n",
    "image_names = list(dset_words.keys())\n",
    "words = list(dset_words.values())\n",
    "\n",
    "# Iterate over length of dictionary and get im2idx and im_words \n",
    "for i in range(len(image_names)):\n",
    "    im = image_names[i]\n",
    "    words_list = flatten(list(words[i].values()))\n",
    "    cleaned_w = []\n",
    "    for w in words_list:\n",
    "        if w != '\"the' and w != '\"The' and len(w) > 1 and is_date(w) != True:\n",
    "            cleaned_w.append(w)\n",
    "\n",
    "    im_words[im] = cleaned_w\n",
    "    im2idx[im] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "class ResnetImagesGloveWordsDataset(Dataset):\n",
    "    \"\"\"\n",
    "        __getitem__ should return (image encoding, text encoding), image name\n",
    "            where image encoding has shape [image_encoding_feature_dim],\n",
    "            text encoding has shape [text_encoding_feature_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_name_to_words, \n",
    "                 im2idx, \n",
    "                 word_embds, \n",
    "                 img_embds, \n",
    "                 train=True, \n",
    "                 device=None):\n",
    "        # word_embds is word -> embedding\n",
    "        self.word_embds = word_embds\n",
    "        # img_embeds is img_idx -> embedding\n",
    "        self.img_embds = img_embds\n",
    "        # Dataset is image_name -> [words]\n",
    "        self.img_name_to_words = img_name_to_words\n",
    "        self.images = list(img_name_to_words.keys())\n",
    "        self.img_embed_shape = len(img_embds[list(img_embds.keys())[0]])\n",
    "        self.im2idx = im2idx\n",
    "        \n",
    "        \n",
    "        self.max_words = 40\n",
    "        self.word_shape = len(word_embds[list(word_embds.keys())[0]])\n",
    "        self.word_embed_shape = self.word_shape * self.max_words\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        idx = self.im2idx[image_name]\n",
    "        \n",
    "        words = self.img_name_to_words[image_name]\n",
    "        w_embds = []\n",
    "        for w in words[:min(self.max_words, len(words))]:\n",
    "            if w in self.word_embds:\n",
    "                w_embds.append(torch.tensor(self.word_embds[w], device=self.device)\n",
    "                               .reshape(self.word_shape, 1))\n",
    "        # pad the rest\n",
    "        for _ in range(self.max_words - len(w_embds)):\n",
    "            w_embds.append(torch.full((self.word_shape, 1), 0.0, device=self.device))\n",
    "            \n",
    "        w_concat = torch.cat(w_embds)\n",
    "        \n",
    "        return (torch.tensor(self.img_embds[str(idx)]), w_concat.reshape((w_concat.shape[0]))), image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ResnetImagesGloveWordsDataset(im_words, im2idx, word_embs, img_embs, device=device)\n",
    "train_set_size = int(.8 * len(dataset))\n",
    "# Split dataset into test and train\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_set_size, len(dataset) - train_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, params, img_shape, words_shape, losses, device=None):\n",
    "    model.to(device)\n",
    "    \n",
    "    all_gamma_weights = set(model.gammas)\n",
    "    all_linear_weights = set(model.weights)\n",
    "    other_params = set(model.parameters()).difference(all_gamma_weights.union(all_linear_weights))\n",
    "    \n",
    "    optim = torch.optim.SGD([{'params': list(all_gamma_weights), 'weight_decay': params.GAMMA_COEF},\n",
    "                            {'params': list(all_linear_weights), 'weight_decay': params.WEIGHT_DECAY},\n",
    "                            {'params': list(other_params)}], \n",
    "                            params.BASE_LEARNING_RATE, \n",
    "                            nesterov=True, \n",
    "                            momentum=Params.MOMENTUM)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(params.EPOCH_NUMBER):\n",
    "        avg_loss = 0\n",
    "        for (img_embed, words_embed), _ in dataloader:\n",
    "            img_embed = img_embed.to(device)\n",
    "            words_embed = words_embed.to(device)\n",
    "            \n",
    "            data = {\"x\": img_embed, \"y\": words_embed}\n",
    "            optim.zero_grad()\n",
    "            # Forward pass\n",
    "            xprime, yprime, hidden_xs, hidden_ys = model(data)\n",
    "                \n",
    "            # Compute losses\n",
    "            # reconstruction losses\n",
    "            loss_x = mse_loss(xprime, data[\"x\"])\n",
    "            loss_y = mse_loss(yprime, data[\"y\"])\n",
    "\n",
    "            # shape [batch_size, hidden_dim]\n",
    "            embedded_x, embedded_y = hidden_xs[model.hidden_output_layer], hidden_ys[model.hidden_output_layer]\n",
    "\n",
    "            # hidden loss\n",
    "            loss_hidden = mse_loss(embedded_x, embedded_y)\n",
    "\n",
    "            # covariance loss\n",
    "            # cov shape: [hidden_dim, hidden_dim]\n",
    "            cov_x = torch.matmul(embedded_x.T, embedded_x) / embedded_x.shape[0]\n",
    "            cov_y = torch.matmul(embedded_y.T, embedded_y) / embedded_y.shape[0]\n",
    "\n",
    "            # Compute covariance losses\n",
    "            cov_loss_x = torch.sqrt(torch.norm(cov_x, p='fro')) - torch.sqrt(torch.norm(torch.diag(cov_x)))\n",
    "            cov_loss_y = torch.sqrt(torch.norm(cov_y, p='fro')) - torch.sqrt(torch.norm(torch.diag(cov_y)))\n",
    "\n",
    "            loss = (params.LOSS_X * loss_x + \n",
    "                    params.LOSS_Y * loss_y + \n",
    "                    params.L2_LOSS * loss_hidden + \n",
    "                    params.WITHEN_REG_X * cov_loss_x + \n",
    "                    params.WITHEN_REG_Y * cov_loss_y)\n",
    "            \n",
    "            # Backward step\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # Update step\n",
    "            optim.step()\n",
    "        losses.append(avg_loss / len(dataset))\n",
    "        print(f\"Epoch {epoch+1}, loss: {losses[-1]}\")\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    # region Training Params\n",
    "    BATCH_SIZE = 2\n",
    "    VALIDATION_BATCH_SIZE = 1000\n",
    "    EPOCH_NUMBER = 100\n",
    "    DECAY_RATE = 0.5\n",
    "    BASE_LEARNING_RATE = 0.0001\n",
    "    MOMENTUM = 0.9\n",
    "    # endregion\n",
    "\n",
    "    # region Loss Weights\n",
    "    WEIGHT_DECAY = 0.05\n",
    "    GAMMA_COEF = 0.05\n",
    "    WITHEN_REG_X = 0.5\n",
    "    WITHEN_REG_Y = 0.5\n",
    "    L2_LOSS = 0.25\n",
    "    LOSS_X = 1\n",
    "    LOSS_Y = 1\n",
    "    # endregion\n",
    "\n",
    "    # region Architecture\n",
    "    LAYER_SIZES = [2000, 3000, 16000]\n",
    "    TEST_LAYER = 1\n",
    "    DROP_PROBABILITY = 0.5\n",
    "    LEAKINESS = 0.3\n",
    "    # endregion\n",
    "\n",
    "    @classmethod\n",
    "    def print_params(cls):\n",
    "        OutputLog().write('Params:\\n')\n",
    "        for (key, value) in cls.__dict__.iteritems():\n",
    "            if not key.startswith('__'):\n",
    "                OutputLog().write('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()\n",
    "dataloader = DataLoader(train_set, batch_size=params.BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder shapes:\n",
      "D_in: 2048, D_out: 2000\n",
      "D_in: 2000, D_out: 3000\n",
      "D_in: 3000, D_out: 16000\n",
      "D_in: 16000, D_out: 2000\n",
      "Y Encoder shapes:\n",
      "D_in: 2000, D_out: 16000\n",
      "D_in: 16000, D_out: 3000\n",
      "D_in: 3000, D_out: 2000\n",
      "D_in: 2000, D_out: 2048\n"
     ]
    }
   ],
   "source": [
    "model_name = datetime.datetime.now()\n",
    "model = TwoWayNet(dataset.img_embed_shape, \n",
    "                  dataset.word_embed_shape, \n",
    "                  params.LAYER_SIZES, \n",
    "                  params.TEST_LAYER, \n",
    "                  params.DROP_PROBABILITY)\n",
    "losses = []\n",
    "# UNCOMMENT THIS AND MODIFY PATH TO RESUME TRAINING\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "try:\n",
    "    train(model, dataloader, params, dataset.img_embed_shape, dataset.word_embed_shape, losses, device=device)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), f\"TwoWayNet_ADARI_{model_name}.pth\")\n",
    "    with open(f\"TwoWayNet_ADARI_losses_{datetime.datetime.now()}.json\", \"w\") as f:\n",
    "        json.dump(losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings of tests images and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "dataloader = DataLoader(train_set, batch_size=1, shuffle=False)\n",
    "# img_name -> (img_embedding, text embedding)\n",
    "all_embeddings = dict()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (img, txt), img_name in dataloader:\n",
    "        xprime, yprime, hidden_imgs, hidden_txts = model({\"x\": img, \"y\": txt})\n",
    "        embedded_x, embedded_y = hidden_imgs[model.hidden_output_layer], hidden_txts[model.hidden_output_layer]\n",
    "        all_embeddings[img_name[0]] = (embedded_x[0].tolist(), embedded_y[0].tolist())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"TwoWayNet_paired_embedding_result_{model_name}.json\", \"w\") as f:\n",
    "    json.dump(all_embeddings, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
