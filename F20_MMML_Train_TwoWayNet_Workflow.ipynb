{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# DIRECTORY STRUCTURE\n",
    "\"\"\"\n",
    "ADARI/\n",
    "    .....\n",
    "mmml_f20/\n",
    "    this\n",
    "TwoWayNets/\n",
    "    .....\n",
    "\"\"\"\n",
    "# REPLACE WITH PATH TO TwoWayNets Repo\n",
    "sys.path.append('../TwoWayNets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "from model import TwoWayNet\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# IMAGES\n",
    "im_path_fur = \"../ADARI/images/furniture/thumbs/big\"\n",
    "\n",
    "# ResNet IMAGE EMBEDDINGS\n",
    "image_embeddings_path = \"../ADARI/image_embeddings/resnet_image_embeddings.json\"\n",
    "\n",
    "# JSON_FILES\n",
    "data_path_fur = \"../ADARI/json_files/cleaned/furniture_cleaned.json\"\n",
    "\n",
    "# FURNITURE VOCAB \n",
    "vocab_id2w = \"../ADARI/json_files/vocabulary/furniture/vocab_id2w.json\"\n",
    "vocab_w2id = \"../ADARI/json_files/vocabulary/furniture/vocab_w2id.json\"\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "word_embeddings_path = \"../ADARI/word_embeddings/fur_5c_50d_sk_glove_ft.json\"\n",
    "\n",
    "# FILES FOR DATALOADER\n",
    "dset_path = '../ADARI/json_files/dataset_for_dataloader/dset_dataloader.json'\n",
    "im2idx_path = '../ADARI/json_files/dataset_for_dataloader/im2idx.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = open_json(word_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embs = open_json(image_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = open_json(vocab_w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = open_json(dset_path)\n",
    "im2idx = open_json(im2idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "class ResnetImagesGloveWordsDataset(Dataset):\n",
    "    \"\"\"\n",
    "        __getitem__ should return (image encoding, text encoding), image name\n",
    "            where image encoding has shape [image_encoding_feature_dim],\n",
    "            text encoding has shape [text_encoding_feature_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_name_to_words, \n",
    "                 im2idx, \n",
    "                 word_embds, \n",
    "                 img_embds, \n",
    "                 train=True, \n",
    "                 device=None):\n",
    "        # word_embds is word -> embedding\n",
    "        self.word_embds = word_embds\n",
    "        # img_embeds is img_idx -> embedding\n",
    "        self.img_embds = img_embds\n",
    "        # Dataset is image_name -> [words]\n",
    "        self.img_name_to_words = img_name_to_words\n",
    "        self.images = list(img_name_to_words.keys())\n",
    "        self.img_embed_shape = len(img_embds[list(img_embds.keys())[0]])\n",
    "        self.im2idx = im2idx\n",
    "        \n",
    "        \n",
    "        self.max_words = 40\n",
    "        self.word_shape = len(word_embds[list(word_embds.keys())[0]])\n",
    "        self.word_embed_shape = self.word_shape * self.max_words\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        idx = self.im2idx[image_name]\n",
    "        \n",
    "        words = self.img_name_to_words[image_name]\n",
    "        w_embds = []\n",
    "        for w in words[:min(self.max_words, len(words))]:\n",
    "            if w in self.word_embds:\n",
    "                w_embds.append(torch.tensor(self.word_embds[w], device=self.device)\n",
    "                               .reshape(self.word_shape, 1))\n",
    "        # pad the rest\n",
    "        for _ in range(self.max_words - len(w_embds)):\n",
    "            w_embds.append(torch.full((self.word_shape, 1), 0.0, device=self.device))\n",
    "            \n",
    "        w_concat = torch.cat(w_embds)\n",
    "        \n",
    "        return (torch.tensor(self.img_embds[str(idx)]), w_concat.reshape((w_concat.shape[0]))), image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ResnetImagesGloveWordsDataset(dset, im2idx, word_embs, img_embs, device=device)\n",
    "# Split dataset into test and train\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [int(.8 * len(dataset)), int(.2 * len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, params, img_shape, words_shape, losses, device=None):\n",
    "    model.to(device)\n",
    "    \n",
    "    all_gamma_weights = set(model.gammas)\n",
    "    all_linear_weights = set(model.weights)\n",
    "    other_params = set(model.parameters()).difference(all_gamma_weights.union(all_linear_weights))\n",
    "    \n",
    "    optim = torch.optim.SGD([{'params': list(all_gamma_weights), 'weight_decay': params.GAMMA_COEF},\n",
    "                            {'params': list(all_linear_weights), 'weight_decay': params.WEIGHT_DECAY},\n",
    "                            {'params': list(other_params)}], \n",
    "                            params.BASE_LEARNING_RATE, \n",
    "                            nesterov=True, \n",
    "                            momentum=Params.MOMENTUM)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(params.EPOCH_NUMBER):\n",
    "        avg_loss = 0\n",
    "        for (img_embed, words_embed), _ in dataloader:\n",
    "            img_embed = img_embed.to(device)\n",
    "            words_embed = words_embed.to(device)\n",
    "            \n",
    "            data = {\"x\": img_embed, \"y\": words_embed}\n",
    "            optim.zero_grad()\n",
    "            # Forward pass\n",
    "            xprime, yprime, hidden_xs, hidden_ys = model(data)\n",
    "                \n",
    "            # Compute losses\n",
    "            # reconstruction losses\n",
    "            loss_x = mse_loss(xprime, data[\"x\"])\n",
    "            loss_y = mse_loss(yprime, data[\"y\"])\n",
    "\n",
    "            # shape [batch_size, hidden_dim]\n",
    "            embedded_x, embedded_y = hidden_xs[model.hidden_output_layer], hidden_ys[model.hidden_output_layer]\n",
    "\n",
    "            # hidden loss\n",
    "            loss_hidden = mse_loss(embedded_x, embedded_y)\n",
    "\n",
    "            # covariance loss\n",
    "            # cov shape: [hidden_dim, hidden_dim]\n",
    "            cov_x = torch.matmul(embedded_x.T, embedded_x) / embedded_x.shape[0]\n",
    "            cov_y = torch.matmul(embedded_y.T, embedded_y) / embedded_y.shape[0]\n",
    "\n",
    "            # Compute covariance losses\n",
    "            cov_loss_x = torch.sqrt(torch.norm(cov_x, p='fro')) - torch.sqrt(torch.norm(torch.diag(cov_x)))\n",
    "            cov_loss_y = torch.sqrt(torch.norm(cov_y, p='fro')) - torch.sqrt(torch.norm(torch.diag(cov_y)))\n",
    "\n",
    "            loss = (params.LOSS_X * loss_x + \n",
    "                    params.LOSS_Y * loss_y + \n",
    "                    params.L2_LOSS * loss_hidden + \n",
    "                    params.WITHEN_REG_X * cov_loss_x + \n",
    "                    params.WITHEN_REG_Y * cov_loss_y)\n",
    "            \n",
    "            # Backward step\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # Update step\n",
    "            optim.step()\n",
    "        losses.append(avg_loss / len(dataset))\n",
    "        print(f\"Epoch {epoch+1}, loss: {losses[-1]}\")\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    # region Training Params\n",
    "    BATCH_SIZE = 2\n",
    "    VALIDATION_BATCH_SIZE = 1000\n",
    "    EPOCH_NUMBER = 100\n",
    "    DECAY_RATE = 0.5\n",
    "    BASE_LEARNING_RATE = 0.0001\n",
    "    MOMENTUM = 0.9\n",
    "    # endregion\n",
    "\n",
    "    # region Loss Weights\n",
    "    WEIGHT_DECAY = 0.05\n",
    "    GAMMA_COEF = 0.05\n",
    "    WITHEN_REG_X = 0.5\n",
    "    WITHEN_REG_Y = 0.5\n",
    "    L2_LOSS = 0.25\n",
    "    LOSS_X = 1\n",
    "    LOSS_Y = 1\n",
    "    # endregion\n",
    "\n",
    "    # region Architecture\n",
    "    LAYER_SIZES = [2000, 3000, 16000]\n",
    "    TEST_LAYER = 1\n",
    "    DROP_PROBABILITY = 0.5\n",
    "    LEAKINESS = 0.3\n",
    "    # endregion\n",
    "\n",
    "    @classmethod\n",
    "    def print_params(cls):\n",
    "        OutputLog().write('Params:\\n')\n",
    "        for (key, value) in cls.__dict__.iteritems():\n",
    "            if not key.startswith('__'):\n",
    "                OutputLog().write('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()\n",
    "dataloader = DataLoader(train_set, batch_size=params.BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder shapes:\n",
      "D_in: 2048, D_out: 2000\n",
      "D_in: 2000, D_out: 3000\n",
      "D_in: 3000, D_out: 16000\n",
      "D_in: 16000, D_out: 2000\n",
      "Y Encoder shapes:\n",
      "D_in: 2000, D_out: 16000\n",
      "D_in: 16000, D_out: 3000\n",
      "D_in: 3000, D_out: 2000\n",
      "D_in: 2000, D_out: 2048\n"
     ]
    }
   ],
   "source": [
    "model_name = datetime.datetime.now()\n",
    "model = TwoWayNet(dataset.img_embed_shape, \n",
    "                  dataset.word_embed_shape, \n",
    "                  params.LAYER_SIZES, \n",
    "                  params.TEST_LAYER, \n",
    "                  params.DROP_PROBABILITY)\n",
    "losses = []\n",
    "# UNCOMMENT THIS AND MODIFY PATH TO RESUME TRAINING\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "try:\n",
    "    train(model, dataloader, params, dataset.img_embed_shape, dataset.word_embed_shape, losses, device=device)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), f\"TwoWayNet_ADARI_{model_name}.pth\")\n",
    "    with open(f\"TwoWayNet_ADARI_losses_{datetime.datetime.now()}.json\", \"w\") as f:\n",
    "        json.dump(losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings of tests images and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.9253e-05,  4.8122e-02, -6.8531e-02,  ..., -8.6740e-05,\n",
      "          9.3502e-02,  8.6351e-02]])\n",
      "tensor([[-9.9253e-05, -9.6644e-02,  1.1464e-01,  ..., -8.6740e-05,\n",
      "          1.6126e-01, -2.3337e-01]])\n",
      "tensor([[-9.9253e-05,  7.0451e-02, -1.6967e-01,  ..., -8.6740e-05,\n",
      "          9.4332e-02,  2.4264e-01]])\n",
      "tensor([[-9.9253e-05, -2.4740e-02, -2.1265e-03,  ..., -8.6740e-05,\n",
      "         -6.4868e-02,  4.7632e-02]])\n",
      "tensor([[-9.9253e-05,  2.6802e-02,  3.8801e-02,  ..., -8.6740e-05,\n",
      "          1.1330e-01,  1.0752e-01]])\n",
      "tensor([[-9.9253e-05, -7.8923e-02,  1.9054e-03,  ..., -8.6740e-05,\n",
      "          8.2679e-02,  7.2636e-02]])\n",
      "tensor([[-9.9253e-05, -3.8653e-02,  1.8752e-01,  ..., -8.6740e-05,\n",
      "          9.0655e-02,  2.8481e-02]])\n",
      "tensor([[-9.9253e-05,  1.0653e-01, -5.3904e-02,  ..., -8.6740e-05,\n",
      "         -8.2955e-02,  1.3139e-01]])\n",
      "tensor([[-9.9253e-05,  1.4339e-01,  2.3375e-01,  ..., -8.6740e-05,\n",
      "         -5.3440e-02, -9.1981e-02]])\n",
      "tensor([[-9.9253e-05, -6.3471e-02,  2.0816e-01,  ..., -8.6740e-05,\n",
      "          7.3511e-03, -9.5012e-02]])\n",
      "tensor([[-9.9253e-05,  1.7356e-01,  1.7538e-01,  ..., -8.6740e-05,\n",
      "          2.2045e-02, -1.0686e-02]])\n",
      "tensor([[-9.9253e-05, -5.9053e-03,  9.2655e-02,  ..., -8.6740e-05,\n",
      "         -3.2765e-02,  6.1483e-02]])\n",
      "tensor([[-9.9253e-05, -2.8044e-02,  6.4421e-02,  ..., -8.6740e-05,\n",
      "          1.5196e-01,  7.2139e-03]])\n",
      "tensor([[-9.9253e-05,  2.9006e-02,  8.5169e-02,  ..., -8.6740e-05,\n",
      "          1.6493e-01, -1.2947e-02]])\n",
      "tensor([[-9.9253e-05,  1.3213e-01,  4.2464e-02,  ..., -8.6740e-05,\n",
      "         -1.7773e-01, -1.4262e-01]])\n",
      "tensor([[-9.9253e-05,  3.4161e-02,  1.8849e-01,  ..., -8.6740e-05,\n",
      "          1.9370e-01, -5.3688e-02]])\n",
      "tensor([[-9.9253e-05,  4.2136e-02,  8.4595e-02,  ..., -8.6740e-05,\n",
      "          1.2387e-01, -8.3470e-02]])\n",
      "tensor([[-9.9253e-05, -9.1195e-03,  8.2848e-02,  ..., -8.6740e-05,\n",
      "          5.0437e-02,  3.2435e-02]])\n",
      "tensor([[-9.9253e-05,  2.1667e-01,  4.3455e-04,  ..., -8.6740e-05,\n",
      "          1.1198e-01,  1.4682e-01]])\n",
      "tensor([[-9.9253e-05, -1.4513e-01,  5.1913e-02,  ..., -8.6740e-05,\n",
      "          1.2890e-01, -2.1271e-01]])\n",
      "tensor([[-9.9253e-05, -5.3562e-02,  8.7565e-02,  ..., -8.6740e-05,\n",
      "          2.1809e-01, -9.0320e-02]])\n",
      "tensor([[-9.9253e-05,  9.9881e-02, -9.6917e-02,  ..., -8.6740e-05,\n",
      "          1.0943e-01,  2.1705e-01]])\n",
      "tensor([[-9.9253e-05,  2.0240e-01, -2.9692e-02,  ..., -8.6740e-05,\n",
      "          1.8531e-02, -9.0970e-02]])\n",
      "tensor([[-9.9253e-05,  1.6232e-01, -8.8649e-02,  ..., -8.6740e-05,\n",
      "         -3.1293e-02,  1.8815e-01]])\n",
      "tensor([[-9.9253e-05,  4.4029e-02, -7.1454e-02,  ..., -8.6740e-05,\n",
      "         -8.7006e-02, -6.7792e-02]])\n",
      "tensor([[-9.9253e-05,  1.6361e-01,  2.1593e-01,  ..., -8.6740e-05,\n",
      "          3.6241e-02, -2.4716e-02]])\n",
      "tensor([[-9.9253e-05, -3.8456e-02,  6.5468e-02,  ..., -8.6740e-05,\n",
      "          8.5073e-02,  1.6216e-02]])\n",
      "tensor([[-9.9253e-05,  2.4759e-02,  7.5363e-02,  ..., -8.6740e-05,\n",
      "          1.4734e-01,  5.5249e-02]])\n",
      "tensor([[-9.9253e-05, -4.0246e-02, -1.2276e-01,  ..., -8.6740e-05,\n",
      "          7.1268e-02, -5.2990e-02]])\n",
      "tensor([[-9.9253e-05,  4.9932e-02,  1.0401e-01,  ..., -8.6740e-05,\n",
      "          2.9163e-02, -1.1007e-01]])\n",
      "tensor([[-9.9253e-05, -1.2959e-01,  3.9945e-02,  ..., -8.6740e-05,\n",
      "          4.6389e-03,  7.2971e-02]])\n",
      "tensor([[-9.9253e-05,  3.0532e-01,  1.8958e-01,  ..., -8.6740e-05,\n",
      "          7.2652e-02,  4.2744e-02]])\n",
      "tensor([[-9.9253e-05,  1.0464e-01,  2.0302e-01,  ..., -8.6740e-05,\n",
      "          4.8492e-02,  7.2289e-02]])\n",
      "tensor([[-9.9253e-05,  8.1374e-04,  6.9658e-03,  ..., -8.6740e-05,\n",
      "          1.2991e-01,  1.8038e-02]])\n",
      "tensor([[-9.9253e-05,  1.1356e-02,  2.8638e-01,  ..., -8.6740e-05,\n",
      "          2.9148e-02,  6.2837e-02]])\n",
      "tensor([[-9.9253e-05,  1.4381e-01,  1.3019e-01,  ..., -8.6740e-05,\n",
      "         -8.2813e-02, -5.5827e-02]])\n",
      "tensor([[-9.9253e-05, -6.2171e-02, -1.7927e-02,  ..., -8.6740e-05,\n",
      "          1.4392e-01,  2.5357e-02]])\n",
      "tensor([[-9.9253e-05, -2.4975e-02,  1.5351e-01,  ..., -8.6740e-05,\n",
      "          9.6743e-02,  1.0595e-01]])\n",
      "tensor([[-9.9253e-05, -2.6570e-02, -7.4574e-04,  ..., -8.6740e-05,\n",
      "          1.2492e-02,  5.3367e-02]])\n",
      "tensor([[-9.9253e-05, -3.7369e-02,  9.7972e-02,  ..., -8.6740e-05,\n",
      "          2.9449e-01,  5.6813e-02]])\n",
      "tensor([[-9.9253e-05,  8.2138e-02,  3.2294e-02,  ..., -8.6740e-05,\n",
      "          4.2476e-02,  5.7390e-02]])\n",
      "tensor([[-9.9253e-05,  2.0924e-01, -1.2296e-01,  ..., -8.6740e-05,\n",
      "          2.4033e-02,  1.1459e-01]])\n",
      "tensor([[-9.9253e-05, -2.3725e-03, -1.9775e-02,  ..., -8.6740e-05,\n",
      "         -1.5439e-01, -4.1863e-02]])\n",
      "tensor([[-9.9253e-05, -7.0479e-03,  1.4926e-01,  ..., -8.6740e-05,\n",
      "          2.1203e-01, -2.1791e-02]])\n",
      "tensor([[-9.9253e-05,  2.0851e-01, -1.5353e-01,  ..., -8.6740e-05,\n",
      "         -3.0594e-02,  3.9611e-02]])\n",
      "tensor([[-9.9253e-05,  1.1279e-01,  1.4594e-01,  ..., -8.6740e-05,\n",
      "          4.0646e-02,  1.2940e-01]])\n",
      "tensor([[-9.9253e-05,  5.5948e-02, -4.2794e-02,  ..., -8.6740e-05,\n",
      "          1.0903e-01, -4.0542e-02]])\n",
      "tensor([[-9.9253e-05,  3.0203e-02,  2.0850e-01,  ..., -8.6740e-05,\n",
      "          1.9707e-01, -1.8009e-01]])\n",
      "tensor([[-9.9253e-05, -4.8289e-02,  2.5755e-01,  ..., -8.6740e-05,\n",
      "         -1.7130e-02, -4.0067e-03]])\n",
      "tensor([[-9.9253e-05, -4.1527e-02, -1.6967e-01,  ..., -8.6740e-05,\n",
      "         -2.8873e-02,  7.8467e-02]])\n",
      "tensor([[-9.9253e-05, -1.4513e-01,  7.7457e-02,  ..., -8.6740e-05,\n",
      "         -4.4078e-02, -1.7617e-01]])\n",
      "tensor([[-9.9253e-05, -1.4513e-01,  3.8829e-02,  ..., -8.6740e-05,\n",
      "          1.5401e-02, -2.3337e-01]])\n",
      "tensor([[-9.9253e-05,  8.4348e-02,  1.2979e-02,  ..., -8.6740e-05,\n",
      "          3.0993e-01, -9.0590e-02]])\n",
      "tensor([[-9.9253e-05, -7.7466e-02,  3.0143e-03,  ..., -8.6740e-05,\n",
      "          9.4210e-02, -2.1750e-01]])\n",
      "tensor([[-9.9253e-05,  1.0743e-01, -2.3343e-02,  ..., -8.6740e-05,\n",
      "          1.9018e-02,  1.0444e-01]])\n",
      "tensor([[-9.9253e-05,  4.8515e-02,  1.1971e-01,  ..., -8.6740e-05,\n",
      "         -3.4609e-02,  8.6554e-02]])\n",
      "tensor([[-9.9253e-05,  1.1681e-01,  2.0360e-01,  ..., -8.6740e-05,\n",
      "          1.7918e-01,  2.7531e-01]])\n",
      "tensor([[-9.9253e-05,  1.5151e-01, -1.5387e-02,  ..., -8.6740e-05,\n",
      "          2.3934e-01, -1.3838e-01]])\n",
      "tensor([[-9.9253e-05, -2.6673e-02,  3.6031e-02,  ..., -8.6740e-05,\n",
      "          1.7869e-02,  9.0764e-03]])\n",
      "tensor([[-9.9253e-05,  5.5452e-02,  3.7133e-02,  ..., -8.6740e-05,\n",
      "          2.3013e-01, -2.5355e-02]])\n",
      "tensor([[-9.9253e-05, -1.0273e-01, -5.8825e-02,  ..., -8.6740e-05,\n",
      "         -7.2690e-03,  1.2825e-02]])\n",
      "tensor([[-9.9253e-05,  1.1356e-01,  1.1690e-01,  ..., -8.6740e-05,\n",
      "          6.2954e-02, -1.7547e-02]])\n",
      "tensor([[-9.9253e-05, -1.7844e-02,  1.5057e-01,  ..., -8.6740e-05,\n",
      "          6.3413e-02, -4.4049e-02]])\n",
      "tensor([[-9.9253e-05, -1.2238e-01,  2.5611e-01,  ..., -8.6740e-05,\n",
      "         -7.9124e-02, -1.1876e-01]])\n",
      "tensor([[-9.9253e-05,  2.8725e-02, -4.9590e-02,  ..., -8.6740e-05,\n",
      "         -8.8973e-02,  6.0053e-02]])\n",
      "tensor([[-9.9253e-05,  8.6960e-02,  6.7571e-02,  ..., -8.6740e-05,\n",
      "         -1.1368e-01,  5.9670e-02]])\n",
      "tensor([[-9.9253e-05,  7.7482e-02, -1.1394e-01,  ..., -8.6740e-05,\n",
      "          2.3705e-01,  2.2929e-01]])\n",
      "tensor([[-9.9253e-05,  2.3099e-03, -5.7859e-03,  ..., -8.6740e-05,\n",
      "         -4.7401e-02,  2.2559e-01]])\n",
      "tensor([[-9.9253e-05, -5.0127e-03,  5.5413e-02,  ..., -8.6740e-05,\n",
      "          2.8577e-01,  1.1880e-01]])\n",
      "tensor([[-9.9253e-05,  1.2350e-01, -1.2744e-01,  ..., -8.6740e-05,\n",
      "          1.0837e-01,  1.4248e-01]])\n",
      "tensor([[-9.9253e-05,  2.2394e-03,  7.4392e-02,  ..., -8.6740e-05,\n",
      "          1.6652e-01,  4.2059e-02]])\n",
      "tensor([[-9.9253e-05, -8.1296e-02,  1.1827e-01,  ..., -8.6740e-05,\n",
      "          1.3504e-01, -5.3662e-02]])\n",
      "tensor([[-9.9253e-05, -1.4513e-01, -2.6835e-02,  ..., -8.6740e-05,\n",
      "         -3.7998e-02, -2.2159e-02]])\n",
      "tensor([[-9.9253e-05,  3.8790e-02, -1.3092e-01,  ..., -8.6740e-05,\n",
      "         -4.9499e-02, -1.6389e-01]])\n",
      "tensor([[-9.9253e-05, -6.0752e-02, -7.5328e-02,  ..., -8.6740e-05,\n",
      "          1.5910e-01, -5.2328e-02]])\n",
      "tensor([[-9.9253e-05, -5.7732e-02, -1.8049e-02,  ..., -8.6740e-05,\n",
      "         -2.1567e-01,  1.8230e-01]])\n",
      "tensor([[-9.9253e-05, -7.6235e-02,  1.2864e-01,  ..., -8.6740e-05,\n",
      "          1.9173e-01,  8.2960e-03]])\n",
      "tensor([[-9.9253e-05, -4.4736e-02,  1.9890e-01,  ..., -8.6740e-05,\n",
      "          3.0515e-02, -7.1939e-02]])\n",
      "tensor([[-9.9253e-05,  8.6743e-04,  9.3264e-02,  ..., -8.6740e-05,\n",
      "          2.6617e-02,  2.1440e-01]])\n",
      "tensor([[-9.9253e-05,  9.2023e-02,  4.6631e-02,  ..., -8.6740e-05,\n",
      "         -1.9591e-02, -4.1804e-02]])\n",
      "tensor([[-9.9253e-05, -1.0160e-02,  1.8237e-01,  ..., -8.6740e-05,\n",
      "         -6.2271e-02, -1.5654e-01]])\n",
      "tensor([[-9.9253e-05, -4.6912e-02,  5.8500e-02,  ..., -8.6740e-05,\n",
      "         -7.6117e-02,  3.1387e-02]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.9253e-05, -1.1360e-01, -9.4421e-03,  ..., -8.6740e-05,\n",
      "          4.1856e-02, -2.3337e-01]])\n",
      "tensor([[-9.9253e-05,  1.0535e-01, -9.1422e-03,  ..., -8.6740e-05,\n",
      "          3.9061e-02,  2.6506e-02]])\n",
      "tensor([[-9.9253e-05, -1.9350e-02, -5.0569e-02,  ..., -8.6740e-05,\n",
      "          5.0290e-02,  5.9134e-03]])\n",
      "tensor([[-9.9253e-05, -2.9245e-02, -2.3930e-02,  ..., -8.6740e-05,\n",
      "         -8.5769e-02,  4.8011e-02]])\n",
      "tensor([[-9.9253e-05,  3.0404e-02, -1.1881e-01,  ..., -8.6740e-05,\n",
      "         -6.1775e-02,  9.4130e-02]])\n",
      "tensor([[-9.9253e-05, -1.4367e-02, -8.5737e-02,  ..., -8.6740e-05,\n",
      "          1.0898e-01, -2.0689e-01]])\n",
      "tensor([[-9.9253e-05, -1.7948e-02,  1.8571e-01,  ..., -8.6740e-05,\n",
      "          1.9556e-01, -1.6017e-02]])\n",
      "tensor([[-9.9253e-05,  1.0886e-01,  1.3377e-01,  ..., -8.6740e-05,\n",
      "          4.5214e-03, -6.2884e-02]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0b6cdc27ca29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mxprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_txts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membedded_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_output_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_txts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_output_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CMU/F20/777/TwoWayNets/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mreconstructed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_encoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_encoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mhidden_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mreconstructed_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_encoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CMU/F20/777/TwoWayNets/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_linear\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CMU/F20/777/TwoWayNets/layers/TiedLinear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#dataloader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "dataloader = DataLoader(train_set, batch_size=1, shuffle=False)\n",
    "# img_name -> (img_embedding, text embedding)\n",
    "all_embeddings = dict()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (img, txt), img_name in dataloader:\n",
    "        xprime, yprime, hidden_imgs, hidden_txts = model({\"x\": img, \"y\": txt})\n",
    "        embedded_x, embedded_y = hidden_imgs[model.hidden_output_layer], hidden_txts[model.hidden_output_layer]\n",
    "        all_embeddings[img_name[0]] = (embedded_x[0].tolist(), embedded_y[0].tolist())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"TwoWayNet_paired_embedding_result_{model_name}.json\", \"w\") as f:\n",
    "    json.dump(all_embeddings, f)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
