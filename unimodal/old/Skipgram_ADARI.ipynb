{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Skipgram_ADARI.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelladron/mmml_f20/blob/main/Skipgram_ADARI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMKtkR37w8NB",
        "outputId": "6739c180-4a27-490a-a6ff-b93aa7fa1d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn3j5SaMw-Oc"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021')\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/notebooks/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb_Dy2MPw32a",
        "outputId": "df4a741a-c07c-4aad-b076-a2d8f977cce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import json, pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time, sys, os, random, io\n",
        "from operator import itemgetter\n",
        "from IPython.display import clear_output\n",
        "from vocab4embeddings import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is True\n",
            "8 cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KjQliTbw32e",
        "outputId": "e4e525e5-64bc-44c5-ceed-caa406ce3dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Check if cuda is available\n",
        "cuda = torch.cuda.is_available()\n",
        "print('CUDA is', cuda)\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "num_workers = 8 if cuda else 0\n",
        "print(num_workers, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is True\n",
            "8 cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFoTv4bwRx5d"
      },
      "source": [
        "def update_progress(progress):\n",
        "    bar_length = 20\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "\n",
        "    block = int(round(bar_length * progress))\n",
        "    clear_output(wait = True)\n",
        "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JbKPku_w32i"
      },
      "source": [
        "### Glove embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjIFFRZ6w32i"
      },
      "source": [
        "glove_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/embeddings/glove.6B.50d.txt'\n",
        "with io.open(glove_path, 'r', encoding='utf8') as f:    \n",
        "    glove_file = f.read()\n",
        "    \n",
        "glove_sentences = glove_file.splitlines()\n",
        "glove_vocab = {}\n",
        "for sentence in glove_sentences:\n",
        "    word = sentence.split()[0]\n",
        "    embedding = np.array(sentence.split()[1:], dtype = float)\n",
        "    glove_vocab[word] = embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqp9rxlSw32l"
      },
      "source": [
        "### Our embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHrk5Axaw32v"
      },
      "source": [
        "### Train embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQooufDyrdeO"
      },
      "source": [
        "def load_vocabulary(file_path):\n",
        "    # open file\n",
        "    f = open(file_path, 'rb')\n",
        "    # dump info to that file\n",
        "    data = pickle.load(f)\n",
        "    # close file\n",
        "    f.close()\n",
        "    # return vocab\n",
        "    return data\n",
        "\n",
        "vocab = load_vocabulary('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/vocabulary/vocab4embeds_fur_c5sw.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXreppExDMVb",
        "outputId": "0e3aec26-5533-4436-80db-88b505e9fc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab.word2idx['angular'], vocab.word2idx_context['angular']\n",
        "vocab.last_target_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3458"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeBys-Dkhlsk",
        "outputId": "0d63f416-cfcd-4de5-e35c-3f2fbb35fbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c = 0\n",
        "for w, id in vocab.word2idx.items():\n",
        "    if w not in glove_vocab:\n",
        "      c += 1\n",
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxL44RoW5FS5",
        "outputId": "110eb922-214b-4753-d63c-6ca36dd53b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(len(vocab.data))\n",
        "print(len(vocab.word2idx))\n",
        "vocab.data[1000:1010]\n",
        "vocab.last_target_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46670\n",
            "26926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3458"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOrcMhkP0MpE"
      },
      "source": [
        "**Data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIwlZ4aZw322",
        "outputId": "3caa3520-11df-43c0-d692-3c23346af9aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def create_datasets(data_):  \n",
        "  print(data_.data[0])\n",
        "  # shuffle data\n",
        "  random.shuffle(data_.data)\n",
        "  # split\n",
        "  split = 0.8\n",
        "  training_number = int(len(data_.data)*split)\n",
        "  test_number = len(data_.data) - training_number\n",
        "  training_data = data_.data[:training_number]\n",
        "  test_data = data_.data[training_number:]\n",
        "  return training_data, test_data\n",
        "\n",
        "training_data, test_data = create_datasets(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['maple', 'plywood', 'boards', '.', 'objects', 'originally', 'designed', 'purpose'], '\"-conceptualizing')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCoKaxVW-Cne",
        "outputId": "441268bf-3556-42a4-9b21-1280962910c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(list(vocab.idx2word.keys())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmuLVxLXw324"
      },
      "source": [
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = X\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.X[index][1]\n",
        "        context = self.X[index][0]\n",
        "        context_ids = torch.tensor([vocab.word2idx[w] for w in context], dtype=torch.long)\n",
        "        print('context_ids')\n",
        "        print(context_ids)\n",
        "        # Change this to avoid having words belonging to real context \n",
        "        context_fakes = np.random.choice(list(vocab.idx2word.keys()), size=(10))\n",
        "        print('context fakes')\n",
        "        print(context_fakes)\n",
        "        context_fakes = torch.from_numpy(context_fakes) # same shape than ocntext_ids [10]        \n",
        "        target_id = torch.tensor([vocab.word2idx[target]], dtype=torch.long)\n",
        "  \n",
        "        return context_ids, target_id, context_fakes\n",
        "    \n",
        "def collate(sequence): # sequence is [context, target, fakes] times batch_size\n",
        "    \"\"\"\n",
        "    \"the input of this function is the output of function __getitem__\"\n",
        "    \"this gets BATCH_SIZE times GETITEM! \"\n",
        "    if batch_Size == 2 --> sequence is a list with length 2. \n",
        "    Each list is a tuple (contexts, target) = ([4], [1])\n",
        "    \"\"\"\n",
        "    #Concatenate all targets and contexts in the batch\n",
        "    contexts = []\n",
        "    for batch in sequence:\n",
        "        if len(batch[0]) != 10:\n",
        "            length = len(batch[0])\n",
        "            pad_length = 10 - length\n",
        "            context = F.pad(batch[0], (0,pad_length), 'constant', 0)\n",
        "            contexts.append(context.view(1, 10))\n",
        "        else:\n",
        "            contexts.append(batch[0].view(1, 10))\n",
        "    contexts = torch.cat(contexts, dim=0) # [batch, 10]\n",
        "    \n",
        "    # If using mean uncomment this line \n",
        "    # contexts = torch.cat(([batch_[0] for batch_ in sequence]), dim=0) # [batch]\n",
        "    # targets\n",
        "    targets = torch.cat(([batch_[1] for batch_ in sequence]), dim=0) # [batch]\n",
        "    context_fakes = torch.cat(([batch_[2] for batch_ in sequence]), dim=0) # [batch, 10]\n",
        "    context_fakes = context_fakes.view(targets.shape[0], -1)\n",
        "    \n",
        "    # contexts and context fakes are [batch, 10], targets is [batch]\n",
        "    return contexts, targets, context_fakes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSWE6xNrw327"
      },
      "source": [
        "batch_size = 64\n",
        "train_dataset = EmbeddingDataset(training_data)\n",
        "train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size, collate_fn = collate, drop_last=True)\n",
        "\n",
        "test_dataset = EmbeddingDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, shuffle = False, batch_size = batch_size, collate_fn = collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSt8xm6j3McY"
      },
      "source": [
        "def create_embedding_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.size()\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    if non_trainable:\n",
        "      emb_layer.requires_grad = False\n",
        "    \n",
        "    return emb_layer#, num_embeddings, embedding_dim\n",
        "\n",
        "# Fill vocabulary with glove vectors, if it exists. \n",
        "weights_matrix = torch.zeros((len(vocab.word2idx), 50))\n",
        "for w, id in vocab.word2idx.items():\n",
        "    try:\n",
        "        weights_matrix[id] = torch.from_numpy(glove_vocab[w])\n",
        "    except KeyError:\n",
        "        weights_matrix[id] = torch.from_numpy(np.random.normal(scale=0.6, size=(50,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fN-3sf_w32_"
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        # self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device) Creates random weights\n",
        "        self.u_embeddings = create_embedding_layer(weights_matrix, False) # init weights with glove vectors\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=False).to(device)\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        #self.u_embeddings.weight.data.uniform_(-initrange, initrange).to(device)\n",
        "        self.v_embeddings.weight.data.uniform_(-0, 0).to(device)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \n",
        "        emb_u = self.u_embeddings(pos_u) # target -> [batch, 300]\n",
        "        emb_v = self.v_embeddings(pos_v) # context -> [batch, 10, 300]\n",
        "\n",
        "        # Average context vector\n",
        "        emb_v = torch.mean(emb_v, dim=1) # avg context -> [batch, 300]\n",
        "\n",
        "        score = torch.mul(emb_u, emb_v).squeeze() # [batch, 300]\n",
        "        score = torch.sum(score, dim=1)\n",
        "        log_target = F.logsigmoid(score) # batch size \n",
        "\n",
        "        # for negative \n",
        "        neg_emb_v = self.v_embeddings(neg_v) # neg context -> [batch, 10, 300]\n",
        "        \n",
        "        # bmm between [batch, 10, 300] x [batch, 300, 1]\n",
        "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze() # [batch, 10]\n",
        "        neg_score = torch.sum(neg_score, dim=1)\n",
        "        sum_log_sampled = F.logsigmoid(-1*neg_score) # batch size\n",
        "\n",
        "        loss = log_target + sum_log_sampled\n",
        "        bs = emb_u.shape[0]\n",
        "        \n",
        "        return -1*loss.sum()/bs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHWq6LCJHird"
      },
      "source": [
        "def train_epoch_s(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total_predictions = 0.0\n",
        "    correct_predictions = 0.0\n",
        "    \n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for batch_idx, (pos, target, neg) in enumerate(train_loader):   \n",
        "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
        "        \n",
        "        pos = pos.cuda()  # [batch size, context_window] [2, 4]\n",
        "        target = target.cuda() # [batch size] [2]\n",
        "        neg = neg.cuda()\n",
        "        \n",
        "        loss = model(target, pos, neg)    # [batch, vocab_size]\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        # #calculating accuracy\n",
        "        # _, predicted = torch.max(log_probs.data, 1)\n",
        "        # total_predictions += target.size(0)\n",
        "        # correct_predictions += (predicted == target).sum().item()\n",
        "            \n",
        "        # #calculuating confusion matrix\n",
        "        # predictions += list(predicted.cpu().numpy())\n",
        "        # ground_truth += list(target.cpu().numpy())\n",
        "        \n",
        "        if batch_idx%50 == 0:\n",
        "            print('loss: ', loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    print('------ Training -----')\n",
        "    running_loss /= len(train_loader)\n",
        "    # acc = (correct_predictions/total_predictions)*100.0\n",
        "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
        "    # print('Training Accuracy: ', acc, '%')\n",
        "    return running_loss\n",
        "\n",
        "def validate_model_s(model, validate_loader, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        total_predictions = 0.0\n",
        "        correct_predictions = 0.0\n",
        "        \n",
        "        predictions = []\n",
        "        ground_truth = []\n",
        "\n",
        "        for batch_idx, (pos, target, neg) in enumerate(validate_loader):   \n",
        "            pos = pos.cuda()  # [batch size, context_window] [2, 4]\n",
        "            target = target.cuda() # [batch size] [2]\n",
        "            neg = neg.cuda()\n",
        "            \n",
        "            loss = model(target, pos, neg)    # [batch, vocab_size]\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # # Get sizes \n",
        "            # batch_size = log_probs.shape[0]\n",
        "            # vocab_size = log_probs.shape[1]\n",
        "\n",
        "            # _, predicted = torch.max(log_probs.data, 1)\n",
        "            # total_predictions += target.size(0)\n",
        "            # correct_predictions += (predicted == target).sum().item()\n",
        "            \n",
        "            # #calculuating confusion matrix\n",
        "            # predictions += list(predicted.cpu().numpy())\n",
        "            # ground_truth += list(target.cpu().numpy())\n",
        "\n",
        "            if batch_idx%50 == 0:\n",
        "                print('loss: ', loss.item())\n",
        "\n",
        "        print('------ Testing -----')\n",
        "        running_loss /= len(validate_loader)\n",
        "        #acc = (correct_predictions/total_predictions)*100.0\n",
        "        print('Testing Loss: ', running_loss)\n",
        "        #print('Testing Accuracy: ', acc, '%')\n",
        "        return running_loss #, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2sSVMisIwVW"
      },
      "source": [
        "def create_run_id(name):\n",
        "  run_id = str(int(time.time()))\n",
        "  if not os.path.exists('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments'):\n",
        "      os.mkdir('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments')\n",
        "  path_name = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments/{}_{}'.format(run_id, name)\n",
        "  os.mkdir(path_name)\n",
        "  print(\"Saving models, and predictions to {}\".format(path_name))\n",
        "  return path_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKYQAloew33D",
        "outputId": "5d83de2d-af5a-4f0b-f350-68438aafb31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "save_path = create_run_id('skipgram_6')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving models, and predictions to /content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments/1597271606_skipgram_6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G53-fuSvw33F",
        "outputId": "51eef58b-68fe-465f-8f64-68c00a0312e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "vocab_size = len(vocab.word2idx)\n",
        "embedding_dimension = 50\n",
        "context_size = 10\n",
        "\n",
        "model = SkipGramModel(vocab_size, embedding_dimension)\n",
        "model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "best_loss = 1e30\n",
        "\n",
        "for epoch in range(20):\n",
        "    print('\\nEpoch: ', epoch)\n",
        "    train_loss = train_epoch_s(model, train_loader, criterion, optimizer)\n",
        "    test_loss = validate_model_s(model, test_loader, criterion)\n",
        "    print('='*20)\n",
        " \n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        print(\"Saving model, predictions and generated output for epoch \" + str(epoch) + \" with Loss: \" + str(best_loss))\n",
        "            \n",
        "        torch.save(model, save_path + '/nlp_embed_' + str(epoch) + '.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  0\n",
            "loss:  1.3862943649291992\n",
            "loss:  0.9968018531799316\n",
            "loss:  0.5957505106925964\n",
            "loss:  0.4017864465713501\n",
            "loss:  0.3436018228530884\n",
            "loss:  0.1795228123664856\n",
            "loss:  0.28758037090301514\n",
            "loss:  0.27599671483039856\n",
            "loss:  0.19278834760189056\n",
            "loss:  0.0950952023267746\n",
            "loss:  0.21500767767429352\n",
            "loss:  0.15024398267269135\n",
            "------ Training -----\n",
            "Training Loss:  0.38883759080085706 Time:  84.07404232025146 s\n",
            "loss:  0.15371014177799225\n",
            "loss:  0.135403111577034\n",
            "loss:  0.18737682700157166\n",
            "------ Testing -----\n",
            "Testing Loss:  0.1407396647415749\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 0 with Loss: 0.1407396647415749\n",
            "\n",
            "Epoch:  1\n",
            "loss:  0.1107078567147255\n",
            "loss:  0.08864519000053406\n",
            "loss:  0.07819347083568573\n",
            "loss:  0.1453486680984497\n",
            "loss:  0.12177090346813202\n",
            "loss:  0.050636645406484604\n",
            "loss:  0.036164697259664536\n",
            "loss:  0.045713160187006\n",
            "loss:  0.09270841628313065\n",
            "loss:  0.07459942996501923\n",
            "loss:  0.1530609428882599\n",
            "loss:  0.025021545588970184\n",
            "------ Training -----\n",
            "Training Loss:  0.09959753589643928 Time:  83.97938513755798 s\n",
            "loss:  0.12847520411014557\n",
            "loss:  0.0955742821097374\n",
            "loss:  0.1648935228586197\n",
            "------ Testing -----\n",
            "Testing Loss:  0.10125907007859994\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 1 with Loss: 0.10125907007859994\n",
            "\n",
            "Epoch:  2\n",
            "loss:  0.07456192374229431\n",
            "loss:  0.054975785315036774\n",
            "loss:  0.08953114598989487\n",
            "loss:  0.09595021605491638\n",
            "loss:  0.139657124876976\n",
            "loss:  0.03990677744150162\n",
            "loss:  0.04981298744678497\n",
            "loss:  0.04480854794383049\n",
            "loss:  0.043273091316223145\n",
            "loss:  0.04917759448289871\n",
            "loss:  0.08355502039194107\n",
            "loss:  0.03913247585296631\n",
            "------ Training -----\n",
            "Training Loss:  0.06354587739349485 Time:  84.5948417186737 s\n",
            "loss:  0.11153901368379593\n",
            "loss:  0.37902960181236267\n",
            "loss:  0.11951957643032074\n",
            "------ Testing -----\n",
            "Testing Loss:  0.08090647353395207\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 2 with Loss: 0.08090647353395207\n",
            "\n",
            "Epoch:  3\n",
            "loss:  0.0363876111805439\n",
            "loss:  0.026100050657987595\n",
            "loss:  0.03428296744823456\n",
            "loss:  0.021135345101356506\n",
            "loss:  0.017660826444625854\n",
            "loss:  0.03549741208553314\n",
            "loss:  0.03961736708879471\n",
            "loss:  0.12993741035461426\n",
            "loss:  0.011039813049137592\n",
            "loss:  0.07339941710233688\n",
            "loss:  0.020232683047652245\n",
            "loss:  0.01564779505133629\n",
            "------ Training -----\n",
            "Training Loss:  0.04307693172418744 Time:  83.19324922561646 s\n",
            "loss:  0.10341326892375946\n",
            "loss:  0.1317410171031952\n",
            "loss:  0.09739385545253754\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07465339176777801\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 3 with Loss: 0.07465339176777801\n",
            "\n",
            "Epoch:  4\n",
            "loss:  0.02970099449157715\n",
            "loss:  0.015685221180319786\n",
            "loss:  0.08076553046703339\n",
            "loss:  0.030400479212403297\n",
            "loss:  0.01614079624414444\n",
            "loss:  0.2654370367527008\n",
            "loss:  0.026061829179525375\n",
            "loss:  0.019910957664251328\n",
            "loss:  0.010559659451246262\n",
            "loss:  0.010864537209272385\n",
            "loss:  0.015843922272324562\n",
            "loss:  0.01599762588739395\n",
            "------ Training -----\n",
            "Training Loss:  0.03228620556622462 Time:  82.18461203575134 s\n",
            "loss:  0.31787481904029846\n",
            "loss:  0.10286381840705872\n",
            "loss:  0.1145768016576767\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07653999655851967\n",
            "====================\n",
            "\n",
            "Epoch:  5\n",
            "loss:  0.009289337322115898\n",
            "loss:  0.016521306708455086\n",
            "loss:  0.015601467341184616\n",
            "loss:  0.032446496188640594\n",
            "loss:  0.020127687603235245\n",
            "loss:  0.03657318651676178\n",
            "loss:  0.08590468019247055\n",
            "loss:  0.022124867886304855\n",
            "loss:  0.013936186209321022\n",
            "loss:  0.2858172357082367\n",
            "loss:  0.01677222177386284\n",
            "loss:  0.011239847168326378\n",
            "------ Training -----\n",
            "Training Loss:  0.028839809244169533 Time:  81.92476630210876 s\n",
            "loss:  0.10841861367225647\n",
            "loss:  0.09344295412302017\n",
            "loss:  0.09395381808280945\n",
            "------ Testing -----\n",
            "Testing Loss:  0.06940219609373033\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 5 with Loss: 0.06940219609373033\n",
            "\n",
            "Epoch:  6\n",
            "loss:  0.0075333560816943645\n",
            "loss:  0.5077173113822937\n",
            "loss:  0.008567435666918755\n",
            "loss:  0.05188572034239769\n",
            "loss:  0.005827664397656918\n",
            "loss:  0.0035240529105067253\n",
            "loss:  0.007188939023762941\n",
            "loss:  0.00948147103190422\n",
            "loss:  0.011969284154474735\n",
            "loss:  0.015341773629188538\n",
            "loss:  0.0065654851496219635\n",
            "loss:  0.009588212706148624\n",
            "------ Training -----\n",
            "Training Loss:  0.025310495546835236 Time:  81.90092539787292 s\n",
            "loss:  0.08571901172399521\n",
            "loss:  0.07354992628097534\n",
            "loss:  0.12603308260440826\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07179938570902467\n",
            "====================\n",
            "\n",
            "Epoch:  7\n",
            "loss:  0.0059679606929421425\n",
            "loss:  0.012394726276397705\n",
            "loss:  0.07428090274333954\n",
            "loss:  0.010779375210404396\n",
            "loss:  0.005813003052026033\n",
            "loss:  0.009939195588231087\n",
            "loss:  0.009783530607819557\n",
            "loss:  0.007033619098365307\n",
            "loss:  0.008071204647421837\n",
            "loss:  0.008566552773118019\n",
            "loss:  0.03900735452771187\n",
            "loss:  0.014780279248952866\n",
            "------ Training -----\n",
            "Training Loss:  0.023831057692534494 Time:  81.97168922424316 s\n",
            "loss:  0.0950549766421318\n",
            "loss:  0.09548268467187881\n",
            "loss:  0.08231963217258453\n",
            "------ Testing -----\n",
            "Testing Loss:  0.0638100351557799\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 7 with Loss: 0.0638100351557799\n",
            "\n",
            "Epoch:  8\n",
            "loss:  0.13651980459690094\n",
            "loss:  0.09307978302240372\n",
            "loss:  0.007284065708518028\n",
            "loss:  0.009829187765717506\n",
            "loss:  0.005540094804018736\n",
            "loss:  0.006073420401662588\n",
            "loss:  0.005505808629095554\n",
            "loss:  0.009671822190284729\n",
            "loss:  0.007112813647836447\n",
            "loss:  0.0722254291176796\n",
            "loss:  0.005258857738226652\n",
            "loss:  0.004975331947207451\n",
            "------ Training -----\n",
            "Training Loss:  0.020464292054078948 Time:  81.76667857170105 s\n",
            "loss:  0.1662323772907257\n",
            "loss:  0.0910014808177948\n",
            "loss:  0.18219175934791565\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07172568334419638\n",
            "====================\n",
            "\n",
            "Epoch:  9\n",
            "loss:  0.028227942064404488\n",
            "loss:  0.007060309872031212\n",
            "loss:  0.005823004525154829\n",
            "loss:  0.008634732104837894\n",
            "loss:  0.10016278177499771\n",
            "loss:  0.0045847585424780846\n",
            "loss:  0.006204598117619753\n",
            "loss:  0.005851964931935072\n",
            "loss:  0.005361846648156643\n",
            "loss:  0.005977515131235123\n",
            "loss:  0.006806743331253529\n",
            "loss:  0.00992267020046711\n",
            "------ Training -----\n",
            "Training Loss:  0.02073753654947233 Time:  81.87636423110962 s\n",
            "loss:  0.12589028477668762\n",
            "loss:  0.08756881952285767\n",
            "loss:  0.09737969934940338\n",
            "------ Testing -----\n",
            "Testing Loss:  0.06254545315360166\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 9 with Loss: 0.06254545315360166\n",
            "\n",
            "Epoch:  10\n",
            "loss:  0.005169852636754513\n",
            "loss:  0.00735729094594717\n",
            "loss:  0.006645090878009796\n",
            "loss:  0.006094461772590876\n",
            "loss:  0.00496920058503747\n",
            "loss:  0.007448925636708736\n",
            "loss:  0.003595183603465557\n",
            "loss:  0.20908492803573608\n",
            "loss:  0.009689012542366982\n",
            "loss:  0.08574081212282181\n",
            "loss:  0.019473891705274582\n",
            "loss:  0.007465609349310398\n",
            "------ Training -----\n",
            "Training Loss:  0.015582882310183939 Time:  81.95028400421143 s\n",
            "loss:  0.141758993268013\n",
            "loss:  0.0954994410276413\n",
            "loss:  0.08182059973478317\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07015637751375262\n",
            "====================\n",
            "\n",
            "Epoch:  11\n",
            "loss:  0.0028432647231966257\n",
            "loss:  0.003778052981942892\n",
            "loss:  0.004808902274817228\n",
            "loss:  0.5853638648986816\n",
            "loss:  0.007015446666628122\n",
            "loss:  0.0758570060133934\n",
            "loss:  0.005319367628544569\n",
            "loss:  0.005138781853020191\n",
            "loss:  0.005198006052523851\n",
            "loss:  0.006036057136952877\n",
            "loss:  0.06806153804063797\n",
            "loss:  0.0046594589948654175\n",
            "------ Training -----\n",
            "Training Loss:  0.018208813912668112 Time:  81.71680569648743 s\n",
            "loss:  0.12866096198558807\n",
            "loss:  0.10909850895404816\n",
            "loss:  0.10545580089092255\n",
            "------ Testing -----\n",
            "Testing Loss:  0.06590086340661837\n",
            "====================\n",
            "\n",
            "Epoch:  12\n",
            "loss:  0.003939674235880375\n",
            "loss:  0.004697758238762617\n",
            "loss:  0.002360893413424492\n",
            "loss:  0.002681646030396223\n",
            "loss:  0.005543778650462627\n",
            "loss:  0.00363725982606411\n",
            "loss:  0.0051965732127428055\n",
            "loss:  0.005557648837566376\n",
            "loss:  0.003600679337978363\n",
            "loss:  0.0037918887101113796\n",
            "loss:  0.011985784396529198\n",
            "loss:  0.003778675338253379\n",
            "------ Training -----\n",
            "Training Loss:  0.015958352089303626 Time:  81.4341242313385 s\n",
            "loss:  0.1096905767917633\n",
            "loss:  0.060079846531152725\n",
            "loss:  0.16579538583755493\n",
            "------ Testing -----\n",
            "Testing Loss:  0.06398840500871102\n",
            "====================\n",
            "\n",
            "Epoch:  13\n",
            "loss:  0.002440415555611253\n",
            "loss:  0.0040217237547039986\n",
            "loss:  0.007499936036765575\n",
            "loss:  0.0038188870530575514\n",
            "loss:  0.005249639507383108\n",
            "loss:  0.0030611949041485786\n",
            "loss:  0.004259956069290638\n",
            "loss:  0.0048349639400839806\n",
            "loss:  0.013019024394452572\n",
            "loss:  0.004598555155098438\n",
            "loss:  0.0045047784224152565\n",
            "loss:  0.003561536082997918\n",
            "------ Training -----\n",
            "Training Loss:  0.017565147543492157 Time:  81.42422819137573 s\n",
            "loss:  0.16147248446941376\n",
            "loss:  0.2598966062068939\n",
            "loss:  0.10249222815036774\n",
            "------ Testing -----\n",
            "Testing Loss:  0.06758082922139805\n",
            "====================\n",
            "\n",
            "Epoch:  14\n",
            "loss:  0.003350555431097746\n",
            "loss:  0.003266064915806055\n",
            "loss:  0.0020400509238243103\n",
            "loss:  0.003841509809717536\n",
            "loss:  0.004679759033024311\n",
            "loss:  0.004684386309236288\n",
            "loss:  0.0035797704476863146\n",
            "loss:  0.13215041160583496\n",
            "loss:  0.0029288537334650755\n",
            "loss:  0.00262656481936574\n",
            "loss:  0.0037126722745597363\n",
            "loss:  0.0029854706954210997\n",
            "------ Training -----\n",
            "Training Loss:  0.015960757903783467 Time:  80.96926069259644 s\n",
            "loss:  0.13174432516098022\n",
            "loss:  0.09704581648111343\n",
            "loss:  0.12224352359771729\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07170846747323768\n",
            "====================\n",
            "\n",
            "Epoch:  15\n",
            "loss:  0.0016972225857898593\n",
            "loss:  0.002220413414761424\n",
            "loss:  0.003110667457804084\n",
            "loss:  0.00307416170835495\n",
            "loss:  0.0025152252055704594\n",
            "loss:  0.10400406271219254\n",
            "loss:  0.0018727376591414213\n",
            "loss:  0.05995088070631027\n",
            "loss:  0.0035000520292669535\n",
            "loss:  0.03493596985936165\n",
            "loss:  0.14038734138011932\n",
            "loss:  0.0034968163818120956\n",
            "------ Training -----\n",
            "Training Loss:  0.015541321346392507 Time:  80.80663752555847 s\n",
            "loss:  0.30614328384399414\n",
            "loss:  0.22312630712985992\n",
            "loss:  0.09133505076169968\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07257249870988196\n",
            "====================\n",
            "\n",
            "Epoch:  16\n",
            "loss:  0.0030002049170434475\n",
            "loss:  0.002240543020889163\n",
            "loss:  0.344666451215744\n",
            "loss:  0.0030950920190662146\n",
            "loss:  0.003179525723680854\n",
            "loss:  0.0018183281645178795\n",
            "loss:  0.0018818079261109233\n",
            "loss:  0.02896132320165634\n",
            "loss:  0.002749865874648094\n",
            "loss:  0.0018508611246943474\n",
            "loss:  0.011139003559947014\n",
            "loss:  0.001878814771771431\n",
            "------ Training -----\n",
            "Training Loss:  0.014500534771158844 Time:  81.31029891967773 s\n",
            "loss:  0.0728941410779953\n",
            "loss:  0.12779153883457184\n",
            "loss:  0.10444913059473038\n",
            "------ Testing -----\n",
            "Testing Loss:  0.07278735750699289\n",
            "====================\n",
            "\n",
            "Epoch:  17\n",
            "loss:  0.002134638838469982\n",
            "loss:  0.012507092207670212\n",
            "loss:  0.00161467376165092\n",
            "loss:  0.0025658225640654564\n",
            "loss:  0.005834289826452732\n",
            "loss:  0.002109367400407791\n",
            "loss:  0.06353424489498138\n",
            "loss:  0.002539577893912792\n",
            "loss:  0.0027982264291495085\n",
            "loss:  0.0031386511400341988\n",
            "loss:  0.014219828881323338\n",
            "loss:  0.0010537648340687156\n",
            "------ Training -----\n",
            "Training Loss:  0.01802925853494804 Time:  81.35491156578064 s\n",
            "loss:  0.11150085926055908\n",
            "loss:  0.07667051255702972\n",
            "loss:  0.09892909973859787\n",
            "------ Testing -----\n",
            "Testing Loss:  0.08133339348938061\n",
            "====================\n",
            "\n",
            "Epoch:  18\n",
            "loss:  0.001568647800013423\n",
            "loss:  0.0032723238691687584\n",
            "loss:  0.001838830066844821\n",
            "loss:  0.036522772163152695\n",
            "loss:  0.0034674471244215965\n",
            "loss:  0.002838071435689926\n",
            "loss:  0.0023183126468211412\n",
            "loss:  0.003265258390456438\n",
            "loss:  0.0020208421628922224\n",
            "loss:  0.003886963240802288\n",
            "loss:  0.012158960103988647\n",
            "loss:  0.0053255134262144566\n",
            "------ Training -----\n",
            "Training Loss:  0.01391665637033862 Time:  81.05649209022522 s\n",
            "loss:  0.12614399194717407\n",
            "loss:  0.22532540559768677\n",
            "loss:  0.13856740295886993\n",
            "------ Testing -----\n",
            "Testing Loss:  0.08333706378311634\n",
            "====================\n",
            "\n",
            "Epoch:  19\n",
            "loss:  0.001758480560965836\n",
            "loss:  0.08380492031574249\n",
            "loss:  0.003350189421325922\n",
            "loss:  0.13239368796348572\n",
            "loss:  0.002444217447191477\n",
            "loss:  0.0019401742611080408\n",
            "loss:  0.003561530727893114\n",
            "loss:  0.0018564133206382394\n",
            "loss:  0.00284696277230978\n",
            "loss:  0.002460761461406946\n",
            "loss:  0.0021226329263299704\n",
            "loss:  0.0017319790786132216\n",
            "------ Training -----\n",
            "Training Loss:  0.015642143430517376 Time:  81.16153240203857 s\n",
            "loss:  0.1968543976545334\n",
            "loss:  0.12390720099210739\n",
            "loss:  0.10998311638832092\n",
            "------ Testing -----\n",
            "Testing Loss:  0.08648275860189779\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLblDvF7E2hk"
      },
      "source": [
        "# Save embeddings\n",
        "\n",
        "load_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments/1597271606_skipgram_6/nlp_embed_9.pth'\n",
        "save_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/embeddings/fur_5c_50d_sk_glove_ft.json'\n",
        "class Embedding(object):\n",
        "    def __init__(self, file_name, load_file):\n",
        "        if load_file != None:\n",
        "            model = torch.load(load_file)\n",
        "        self.file_name = file_name\n",
        "        self.create_embedding()\n",
        "        self.save_json()\n",
        "\n",
        "\n",
        "    def create_embedding(self):\n",
        "        embeddings = model.u_embeddings.weight.cpu().data.numpy()\n",
        "        self.embedding = {}\n",
        "        for id, w in vocab.idx2word.items():\n",
        "            e = embeddings[id].tolist()\n",
        "            self.embedding[w] = e\n",
        "        \n",
        "    def save_json(self):\n",
        "        out_file = open(self.file_name, \"w\")\n",
        "        json.dump(self.embedding, out_file)\n",
        "        out_file.close()\n",
        "    \n",
        "E = Embedding(save_path, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EulHF-Z2KieG"
      },
      "source": [
        "def open_json(path):\n",
        "    f = open(path) \n",
        "    data = json.load(f) \n",
        "    f.close()\n",
        "    return data \n",
        "\n",
        "emb = open_json(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
