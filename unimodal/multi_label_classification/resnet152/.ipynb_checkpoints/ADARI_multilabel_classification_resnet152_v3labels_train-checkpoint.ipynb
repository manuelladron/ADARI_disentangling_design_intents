{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as dataF\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import re\n",
    "import io\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import pdb\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "def save_json(file_path, data):\n",
    "    out_file = open(file_path, \"w\")\n",
    "    json.dump(data, out_file)\n",
    "    out_file.close()\n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar to visualize progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES\n",
    "im_path_fur = '../../../../ADARI/images/ADARI_v2/furniture/full'\n",
    "\n",
    "# ONEHOTS\n",
    "onehots_vocab_p = '../../../../ADARI/json_files/cleaned/ADARI_v2/furniture/ADARI_furniture_onehots_vocab_3labels.json'\n",
    "onehots_w2i_p = '../../../../ADARI/json_files/cleaned/ADARI_v2/furniture/ADARI_furniture_onehots_w2i_3labels.json'\n",
    "onehots_i2w_p = '../../../../ADARI/json_files/cleaned/ADARI_v2/furniture/ADARI_furniture_onehots_i2w_3labels.json'\n",
    "onehots_p = '../../../../ADARI/json_files/cleaned/ADARI_v2/furniture/ADARI_furniture_onehots_3labels.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open json files with embeddings \n",
    "onehots = open_json(onehots_p)\n",
    "onehots_vocab = open_json(onehots_vocab_p)\n",
    "onehots_w2i = open_json(onehots_w2i_p)\n",
    "onehots_i2w = open_json(onehots_i2w_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4780"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onehots_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'danish': 243,\n",
       " 'recycled': 210,\n",
       " 'outdoor': 209,\n",
       " 'japanese': 189,\n",
       " 'swedish': 159,\n",
       " 'plastic': 141,\n",
       " 'public': 140,\n",
       " 'solid': 140,\n",
       " 'chinese': 138,\n",
       " 'concrete': 138,\n",
       " 'nordic': 137,\n",
       " 'norwegian': 127,\n",
       " 'urban': 122,\n",
       " 'bent': 121,\n",
       " 'modular': 118,\n",
       " 'traditional': 118,\n",
       " 'brazilian': 112,\n",
       " 'tubular': 111,\n",
       " 'digital': 108,\n",
       " 'flat-pack': 104,\n",
       " 'raw': 101,\n",
       " 'original': 100,\n",
       " 'coloured': 100,\n",
       " 'architectural': 95,\n",
       " 'red': 93,\n",
       " 'australian': 91,\n",
       " 'french': 88,\n",
       " 'korean': 86,\n",
       " 'local': 86,\n",
       " 'white': 85,\n",
       " 'pink': 84,\n",
       " 'industrial': 82,\n",
       " 'italian': 82,\n",
       " 'dutch': 81,\n",
       " 'domestic': 80,\n",
       " 'mexican': 80,\n",
       " 'ceramic': 79,\n",
       " 'unseen': 78,\n",
       " 'young': 78,\n",
       " 'african': 77,\n",
       " 'finnish': 77,\n",
       " 'mid-century': 77,\n",
       " 'black': 76,\n",
       " 'transparent': 76,\n",
       " 'lacquered': 76,\n",
       " 'yellow': 75,\n",
       " 'english': 75,\n",
       " 'offsite': 74,\n",
       " 'steam-bent': 74,\n",
       " 'human': 73,\n",
       " 'environmental': 72,\n",
       " 'contemporary': 69,\n",
       " 'textile': 69,\n",
       " 'perforated': 68,\n",
       " 'online': 67,\n",
       " 'grey': 65,\n",
       " 'future': 65,\n",
       " 'cantilevered': 65,\n",
       " 'polished': 64,\n",
       " 'german': 63,\n",
       " 'good': 63,\n",
       " 'padded': 63,\n",
       " 'social': 62,\n",
       " 'mass': 59,\n",
       " 'invisible': 59,\n",
       " 'modern': 58,\n",
       " 'collectible': 58,\n",
       " 'antique': 58,\n",
       " 'volcanic': 58,\n",
       " 'low': 58,\n",
       " 'ancient': 57,\n",
       " 'upholstered': 57,\n",
       " 'female': 56,\n",
       " 'south': 56,\n",
       " 'wooden': 56,\n",
       " 'american': 56,\n",
       " 'elastic': 56,\n",
       " 'early': 56,\n",
       " 'slim': 55,\n",
       " 'hot': 54,\n",
       " 'comfortable': 53,\n",
       " 'double': 53,\n",
       " 'beige': 53,\n",
       " 'useful': 52,\n",
       " 'recyclable': 52,\n",
       " 'compact': 52,\n",
       " 'stainless': 51,\n",
       " 'sustainable': 50,\n",
       " 'austrian': 50,\n",
       " 'portable': 50,\n",
       " 'permanent': 50,\n",
       " 'extra': 50,\n",
       " 'independent': 50,\n",
       " 'horizontal': 49,\n",
       " 'proper': 49,\n",
       " 'natural': 49,\n",
       " 'occasional': 49,\n",
       " 'joint': 49,\n",
       " 'old': 48,\n",
       " 'swiss': 48,\n",
       " 'irish': 48,\n",
       " 'classical': 47,\n",
       " 'molten': 47,\n",
       " 'ambient': 47,\n",
       " 'european': 47,\n",
       " 'active': 47,\n",
       " 'green': 47,\n",
       " 'british': 47,\n",
       " 'smart': 46,\n",
       " 'bad': 46,\n",
       " 'orange': 46,\n",
       " 'dark': 46,\n",
       " 'icelandic': 46,\n",
       " 'circular': 46,\n",
       " 'open-source': 45,\n",
       " 'medical': 45,\n",
       " 'informal': 45,\n",
       " 'inflated': 45,\n",
       " 'scandinavian': 44,\n",
       " 'organic': 44,\n",
       " 'belgian': 44,\n",
       " 'dynamic': 44,\n",
       " 'little': 44,\n",
       " 'laundry': 43,\n",
       " 'great': 43,\n",
       " 'personal': 43,\n",
       " 'acoustic': 42,\n",
       " 'safe': 42,\n",
       " 'three-dimensional': 42,\n",
       " 'square': 42,\n",
       " 'light': 42,\n",
       " 'hard': 42,\n",
       " 'metallic': 42,\n",
       " 'loose': 42,\n",
       " 'removable': 42,\n",
       " 'rotational': 42,\n",
       " 'composite': 41,\n",
       " 'miniature': 41,\n",
       " 'stellar': 41,\n",
       " 'swivel': 41,\n",
       " 'basic': 41,\n",
       " 'frosted': 41,\n",
       " 'mouth-blown': 41,\n",
       " 'slender': 41,\n",
       " 'synthetic': 40,\n",
       " 'hidden': 40,\n",
       " 'plain': 40,\n",
       " 'tiny': 40,\n",
       " 'continuous': 40,\n",
       " 'tough': 40,\n",
       " 'limited': 40,\n",
       " 'primitive': 39,\n",
       " 'split': 39,\n",
       " 'retail': 39,\n",
       " 'spanish': 39,\n",
       " 'wrong': 39,\n",
       " 'shallow': 39,\n",
       " 'interesting': 39,\n",
       " 'honest': 39,\n",
       " 'outer': 38,\n",
       " 'minimal': 38,\n",
       " 'classic': 38,\n",
       " 'private': 38,\n",
       " 'triangular': 38,\n",
       " 'sound': 38,\n",
       " 'utilitarian': 38,\n",
       " 'stackable': 38,\n",
       " 'geometrical': 38,\n",
       " 'religious': 37,\n",
       " 'powder-coated': 37,\n",
       " 'metre': 37,\n",
       " 'convenient': 37,\n",
       " 'luminous': 37,\n",
       " 'temporary': 36,\n",
       " 'sancal': 36,\n",
       " 'global': 36,\n",
       " 'standard': 36,\n",
       " 'cheap': 36,\n",
       " 'malleable': 36,\n",
       " 'tropical': 36,\n",
       " 'indoor': 36,\n",
       " 'petite': 36,\n",
       " 'stained': 36,\n",
       " 'redundant': 36,\n",
       " 'outstanding': 36,\n",
       " 'clear': 36,\n",
       " 'musical': 35,\n",
       " 'rural': 35,\n",
       " 'soft': 35,\n",
       " 'rigid': 35,\n",
       " 'nomadic': 35,\n",
       " 'irregular': 35,\n",
       " 'rough': 35,\n",
       " '3d': 35,\n",
       " 'long': 35,\n",
       " 'untreated': 35,\n",
       " 'enormous': 35,\n",
       " 'able': 34,\n",
       " 'vertical': 34,\n",
       " 'random': 34,\n",
       " 'normal': 34,\n",
       " 'middle': 34,\n",
       " 'particular': 34,\n",
       " 'everyday': 34,\n",
       " 'flexible': 34,\n",
       " 'victorian': 34,\n",
       " 'liquid': 34,\n",
       " 'critical': 34,\n",
       " 'smooth': 34,\n",
       " 'present': 34,\n",
       " 'charming': 34,\n",
       " 'solo': 34,\n",
       " 'efficient': 34,\n",
       " 'faceted': 33,\n",
       " 'opposite': 33,\n",
       " 'oval': 33,\n",
       " 'collaborative': 33,\n",
       " 'cultural': 33,\n",
       " 'elderly': 33,\n",
       " 'canadian': 33,\n",
       " 'heavy': 33,\n",
       " 'exterior': 33,\n",
       " 'matte': 33,\n",
       " 'pedestal': 33,\n",
       " 'residential': 33,\n",
       " 'durable': 33,\n",
       " 'corrugated-cardboard': 32,\n",
       " 'artistic': 32,\n",
       " 'half': 32,\n",
       " 'hand-turned': 32,\n",
       " 'big': 32,\n",
       " 'tall': 32,\n",
       " 'sexual': 32,\n",
       " 'hexagonal': 32,\n",
       " 'creative': 32,\n",
       " 'artificial': 32,\n",
       " 'rustic': 32,\n",
       " 'large': 32,\n",
       " 'multi-purpose': 32,\n",
       " 'popular': 32,\n",
       " 'strong': 32,\n",
       " 'solar': 31,\n",
       " 'perfect': 31,\n",
       " 'cool': 31,\n",
       " 'rectangular': 31,\n",
       " 'fake': 31,\n",
       " 'extendable': 31,\n",
       " 'important': 31,\n",
       " 'conventional': 31,\n",
       " 'corrugated': 31,\n",
       " 'single': 31,\n",
       " 'warm': 31,\n",
       " 'physical': 31,\n",
       " 'foldable': 31,\n",
       " 'curved': 31,\n",
       " 'striped': 31,\n",
       " 'actual': 31,\n",
       " 'maximum': 31,\n",
       " 'upper': 31,\n",
       " 'extraordinary': 31,\n",
       " 'timeless': 30,\n",
       " 'humble': 30,\n",
       " 'slow': 30,\n",
       " 'inflatable': 30,\n",
       " 'electric': 30,\n",
       " 'biodegradable': 30,\n",
       " 'closed': 30,\n",
       " 'busy': 30,\n",
       " 'adaptable': 30,\n",
       " 'historical': 30,\n",
       " 'lightweight': 30,\n",
       " 'stretchy': 30,\n",
       " 'ladder-': 30,\n",
       " 'thick': 30,\n",
       " 'flowing': 30,\n",
       " 'subdued': 30,\n",
       " 'asymmetric': 29,\n",
       " 'celestial': 29,\n",
       " 'precious': 29,\n",
       " 'space-saving': 29,\n",
       " 'exclusive': 29,\n",
       " 'twin': 29,\n",
       " 'hungarian': 29,\n",
       " 'intimate': 29,\n",
       " 'bright': 29,\n",
       " 'electrical': 29,\n",
       " 'fat': 29,\n",
       " 'innermost': 29,\n",
       " 'spatial': 29,\n",
       " 'swinging': 29,\n",
       " 'following': 29,\n",
       " 'tinted': 29,\n",
       " 'pet': 29,\n",
       " 'hand-drawn': 28,\n",
       " 'sophisticated': 28,\n",
       " 'mechanical': 28,\n",
       " 'open-plan': 28,\n",
       " 'regular': 28,\n",
       " 'northmodern': 28,\n",
       " 'reversible': 28,\n",
       " 'awkward': 28,\n",
       " 'willing': 28,\n",
       " 'computer-generated': 28,\n",
       " 'tapered': 28,\n",
       " 'expensive': 27,\n",
       " 'golden': 27,\n",
       " 'positive': 27,\n",
       " 'protective': 27,\n",
       " 'dirty': 27,\n",
       " 'open': 27,\n",
       " 'fast-growing': 27,\n",
       " 'close': 27,\n",
       " 'abstract': 27,\n",
       " 'possible': 27,\n",
       " 'cold': 27,\n",
       " 'thin': 27,\n",
       " 'skinny': 27,\n",
       " 'underground': 27,\n",
       " 'corporate': 26,\n",
       " 'scottish': 26,\n",
       " 'imaginary': 26,\n",
       " 'gentle': 26,\n",
       " 'tactile': 26,\n",
       " 'leftover': 26,\n",
       " 'geometric': 26,\n",
       " 'bold': 26,\n",
       " 'tensile': 26,\n",
       " 'right': 26,\n",
       " 'commercial': 26,\n",
       " 'feminine': 26,\n",
       " 'dovetail': 26,\n",
       " 'berlin-based': 26,\n",
       " '‘modus': 26,\n",
       " 'remorseful': 26,\n",
       " 'hedonistic': 26,\n",
       " 'seamless': 25,\n",
       " 'casual': 25,\n",
       " 'alternative': 25,\n",
       " 'cooperative': 25,\n",
       " 'two-dimensional': 25,\n",
       " 'high-end': 25,\n",
       " 'diagonal': 25,\n",
       " 'essential': 25,\n",
       " 'galvanised': 25,\n",
       " 'catalan': 25,\n",
       " 'upside-down': 25,\n",
       " 'viennese': 25,\n",
       " 'unknown': 25,\n",
       " 'interior': 25,\n",
       " 'unbalanced': 25,\n",
       " 'automotive': 25,\n",
       " 'functional': 24,\n",
       " 'deep': 24,\n",
       " 'fold-out': 24,\n",
       " 'fluorescent': 24,\n",
       " 'innovative': 24,\n",
       " 'sacred': 24,\n",
       " 'western': 24,\n",
       " 'curvy': 24,\n",
       " 'structural': 24,\n",
       " 'pleated': 24,\n",
       " 'experimental': 24,\n",
       " 'wall-mounted': 24,\n",
       " 'fictional': 24,\n",
       " 'quilted': 24,\n",
       " 'dedicated': 24,\n",
       " 'glass-topped': 24,\n",
       " 'masculine': 24,\n",
       " 'distorted': 24,\n",
       " 'metaphysical': 24,\n",
       " 'fourth': 24,\n",
       " 'authentic': 24,\n",
       " 'food-inspired': 24,\n",
       " 'climactic': 24,\n",
       " 'self-made': 24,\n",
       " 'southern': 24,\n",
       " 'separate': 24,\n",
       " 'northern': 24,\n",
       " 'international': 23,\n",
       " 'blue': 23,\n",
       " 'roman': 23,\n",
       " 'ergonomic': 23,\n",
       " 'remote': 23,\n",
       " 'undulating': 23,\n",
       " 'mobile': 23,\n",
       " 'height-adjustable': 23,\n",
       " 'interchangeable': 23,\n",
       " 'carbon-fibre': 23,\n",
       " 'twentieth-century': 23,\n",
       " 'well-known': 23,\n",
       " 'georgian': 23,\n",
       " 'pastel': 23,\n",
       " 'exciting': 23,\n",
       " 'dimensional': 23,\n",
       " 'horseshoe-shaped': 23,\n",
       " 'brooklyn-based': 23,\n",
       " 'angled': 23,\n",
       " 'high': 23,\n",
       " 'extruded': 23,\n",
       " 'optional': 23,\n",
       " 'gold': 23,\n",
       " 'faux': 23,\n",
       " 'collapsible': 23,\n",
       " 'odd': 23,\n",
       " 'laptopthe': 23,\n",
       " 'upsetting': 23,\n",
       " 'pure': 23,\n",
       " 'fine': 23,\n",
       " 'laser-cut': 23,\n",
       " 'lavic': 23,\n",
       " 'haunting': 23,\n",
       " 'sensual': 23,\n",
       " 'plump': 22,\n",
       " 'feline': 22,\n",
       " 'sharp': 22,\n",
       " 'absent': 22,\n",
       " 'sand-cast': 22,\n",
       " 'eastern': 22,\n",
       " 'worth': 22,\n",
       " 'tight': 22,\n",
       " 'postmodern': 22,\n",
       " 'limited-edition': 22,\n",
       " 'inherent': 22,\n",
       " 'minimalist': 22,\n",
       " 'visible': 22,\n",
       " 'soviet': 22,\n",
       " 'cast-iron': 22,\n",
       " 'engaging': 22,\n",
       " 'topographical': 22,\n",
       " 'scaly': 22,\n",
       " 'eco-friendly': 22,\n",
       " 'russian': 22,\n",
       " 'daring': 22,\n",
       " 'forthcoming': 22,\n",
       " 'connected': 22,\n",
       " 'one-piece': 22,\n",
       " 'sculptural': 22,\n",
       " 'silver': 22,\n",
       " 'boundless': 22,\n",
       " 'amish': 22,\n",
       " 'nice': 22,\n",
       " 'spontaneous': 22,\n",
       " 'full-length': 22,\n",
       " 'characteristic': 22,\n",
       " 'super': 22,\n",
       " 'undersized': 22,\n",
       " 'slight': 22,\n",
       " 'neat': 22,\n",
       " 'ornate': 22,\n",
       " 'gas-assisted': 22,\n",
       " 'tribal': 22,\n",
       " 'rich': 22,\n",
       " 'communal': 22,\n",
       " 'brown': 22,\n",
       " 'different': 22,\n",
       " 'thermoplastic': 22,\n",
       " 'mysterious': 22,\n",
       " 'narrative': 22,\n",
       " 'unrepeatable': 22,\n",
       " 'anodised': 22,\n",
       " 'versatile': 21,\n",
       " 'conceptual': 21,\n",
       " 'squishy': 21,\n",
       " 'customisable': 21,\n",
       " 'common': 21,\n",
       " 'rental': 21,\n",
       " 'practical': 21,\n",
       " 'latvian': 21,\n",
       " 'generative': 21,\n",
       " 'twisted': 21,\n",
       " 'tired': 21,\n",
       " 'mad': 21,\n",
       " 'fifth': 21,\n",
       " 'affordable': 21,\n",
       " 'rear': 21,\n",
       " 'ultra-minimal': 21,\n",
       " 'naive': 21,\n",
       " 'shaker-inspired': 21,\n",
       " 'nine-metre': 21,\n",
       " 'narrow': 21,\n",
       " 'dual-purpose': 21,\n",
       " 'injection-moulding': 21,\n",
       " 'psychological': 21,\n",
       " 'grid-like': 21,\n",
       " 'lazy': 21,\n",
       " 'pleased': 21,\n",
       " 'israeli': 21,\n",
       " 'pressed-felt': 21,\n",
       " 'adjustable': 21,\n",
       " 'witty': 21,\n",
       " 'comic': 21,\n",
       " 'secret': 20,\n",
       " 'resident': 20,\n",
       " 'flat': 20,\n",
       " 'reflective': 20,\n",
       " 'virtual': 20,\n",
       " 'detachable': 20,\n",
       " 'primary': 20,\n",
       " 'colourful': 20,\n",
       " 'aesthetic': 20,\n",
       " 'three-seater': 20,\n",
       " 'grand': 20,\n",
       " 'vintage': 20,\n",
       " 'standalone': 20,\n",
       " 'restless': 20,\n",
       " 'triple': 20,\n",
       " 'edible': 20,\n",
       " 'perfumed': 20,\n",
       " 'sloping': 20,\n",
       " 'varnished': 20,\n",
       " 'jagged': 20,\n",
       " 'plank-like': 20,\n",
       " 'multi-use': 20,\n",
       " 'spaced': 20,\n",
       " 'stained-black': 20,\n",
       " 'hammock-like': 20,\n",
       " 'dyed': 20,\n",
       " 'flex': 20,\n",
       " 'milled': 20,\n",
       " 'curated': 20,\n",
       " 'two-legged': 20,\n",
       " 'exaggerated': 20,\n",
       " 'shaker-style': 20,\n",
       " 'oiled': 20,\n",
       " 'inner': 20,\n",
       " 'upright': 20,\n",
       " 'slanted': 20,\n",
       " 'friendly': 20,\n",
       " 'laminated': 20,\n",
       " 'vietnamese': 20,\n",
       " 'high-ceilinged': 20,\n",
       " 'hand-carved': 20,\n",
       " 'rotation-moulded': 20,\n",
       " 'brilliant': 20,\n",
       " 'enchanting': 20,\n",
       " 'low-cost': 20,\n",
       " 'glossy': 19,\n",
       " 'chunky': 19,\n",
       " 'voluptuous': 19,\n",
       " 'futuristic': 19,\n",
       " 'seaside': 19,\n",
       " 'adequate': 19,\n",
       " 'handmade': 19,\n",
       " 'beech': 19,\n",
       " 'nearby': 19,\n",
       " 'translucent': 19,\n",
       " 'millennial': 19,\n",
       " 'glass-fronted': 19,\n",
       " 'felt-like': 19,\n",
       " 'ladder-backed': 19,\n",
       " 'hamburg-based': 19,\n",
       " 'parallel': 19,\n",
       " 'fluid': 19,\n",
       " 'oversized': 19,\n",
       " 'montreal-based': 19,\n",
       " 'ultra-thin': 19,\n",
       " 'bosnian': 19,\n",
       " 'teardrop-shaped': 19,\n",
       " 'stripy': 19,\n",
       " 'czech': 19,\n",
       " 'photogenic': 19,\n",
       " 'medium': 19,\n",
       " 'hard-boiled': 19,\n",
       " 'self-assembly': 19,\n",
       " 'wavy': 19,\n",
       " 'prehistoric': 19,\n",
       " 'voluntary': 19,\n",
       " 'unexpected': 19,\n",
       " 'inspired': 19,\n",
       " 'turquoise': 19,\n",
       " 'infinite': 19,\n",
       " 'robot-shaped': 19,\n",
       " 'childlike': 19,\n",
       " 'free': 19,\n",
       " 'entire': 19,\n",
       " 'prolonged': 19,\n",
       " 'obscure': 19,\n",
       " 'non-orientation': 19,\n",
       " 'alpine': 19,\n",
       " 'multi-section': 19,\n",
       " '́s': 19,\n",
       " 'graphic': 19,\n",
       " 'interested': 19,\n",
       " 'weatherproof': 18,\n",
       " 'flatpack': 18,\n",
       " 'york-based': 18,\n",
       " 'homeless': 18,\n",
       " 'modernist': 18,\n",
       " 'real': 18,\n",
       " 'multifunctional': 18,\n",
       " 'all-female': 18,\n",
       " 'off-the-shelf': 18,\n",
       " 'polish': 18,\n",
       " 'homogeneous': 18,\n",
       " 'speckled': 18,\n",
       " 'wing-shaped': 18,\n",
       " 'hand-sewn': 18,\n",
       " 'brasscloth': 18,\n",
       " 'nautical': 18,\n",
       " 'one-year': 18,\n",
       " 'x-shaped': 18,\n",
       " 'unfixed': 18,\n",
       " 'mosaic': 18,\n",
       " 'outward': 18,\n",
       " 'convinced': 18,\n",
       " 'flat-packed': 18,\n",
       " 'fond': 18,\n",
       " 'junior': 18,\n",
       " 'dismountable': 18,\n",
       " 'available': 18,\n",
       " 'integrated': 18,\n",
       " 'cut-crystal': 18,\n",
       " 'misty': 18,\n",
       " 'enamel-coated': 18,\n",
       " 'favorite': 18,\n",
       " 'sectional': 18,\n",
       " 'waterproof': 18,\n",
       " 'abundant': 18,\n",
       " 'horticultural': 18,\n",
       " '12mm': 18,\n",
       " 'one-of-a-kind': 18,\n",
       " 'distinct': 18,\n",
       " 'unsteady': 18,\n",
       " '--chatouthis': 18,\n",
       " 'programmatic': 18,\n",
       " 'strict': 18,\n",
       " 'naked': 18,\n",
       " 'kinked': 17,\n",
       " 'resilient': 17,\n",
       " 'tidal': 17,\n",
       " 'bare': 17,\n",
       " 'brutalist': 17,\n",
       " 'executive': 17,\n",
       " 'leather-upholstered': 17,\n",
       " 'mid': 17,\n",
       " 'radical': 17,\n",
       " 'conductive': 17,\n",
       " 'neutral': 17,\n",
       " 'built-in': 17,\n",
       " 'muted': 17,\n",
       " 'magnetic': 17,\n",
       " 'pale': 17,\n",
       " 'two-seater': 17,\n",
       " 'self-designed': 17,\n",
       " 'main': 17,\n",
       " 'rolling': 17,\n",
       " 'latin': 17,\n",
       " 'bespoke': 17,\n",
       " 'four-poster': 17,\n",
       " 'spindly': 17,\n",
       " 'round': 17,\n",
       " 'steady': 17,\n",
       " 'sentimental': 17,\n",
       " 'trapezium-shaped': 17,\n",
       " 'full-size': 17,\n",
       " 'cushioned': 17,\n",
       " 'pastel-coloured': 17,\n",
       " 'elongated': 17,\n",
       " 'islamic': 17,\n",
       " 'optical': 17,\n",
       " 'static': 17,\n",
       " 'solid-wood': 17,\n",
       " 'milky': 17,\n",
       " 'fascinating': 17,\n",
       " 'anglo-indian': 17,\n",
       " 'usable': 17,\n",
       " 'adaptive': 17,\n",
       " 'trestle-style': 17,\n",
       " 'pressure-moulded': 17,\n",
       " 'cnc-shaped': 17,\n",
       " 'true': 17,\n",
       " 'cooked': 17,\n",
       " 'evergreen': 17,\n",
       " 'semi-enclosed': 17,\n",
       " 'vague': 17,\n",
       " 'mixed': 17,\n",
       " 'one-off': 17,\n",
       " 'focused': 17,\n",
       " 'unobserved': 17,\n",
       " 'self-regenerating': 17,\n",
       " 'wood-like': 17,\n",
       " 'newspaperwood': 17,\n",
       " 'marginal': 17,\n",
       " 'hybrid': 17,\n",
       " 'vital': 17,\n",
       " 'specific': 17,\n",
       " 'chilean': 17,\n",
       " 'frayed': 17,\n",
       " 'two-shell': 17,\n",
       " 'quick-assembly': 17,\n",
       " 'well': 17,\n",
       " 'ornamental': 16,\n",
       " 'polar': 16,\n",
       " 'complicated': 16,\n",
       " 'replaceable': 16,\n",
       " 'aggressive': 16,\n",
       " 'lumpy': 16,\n",
       " 'indonesian': 16,\n",
       " 'beautiful': 16,\n",
       " 'mathematical': 16,\n",
       " 'explorative': 16,\n",
       " 'robotic': 16,\n",
       " '20th': 16,\n",
       " 'second': 16,\n",
       " 'bristly': 16,\n",
       " 'shimmering': 16,\n",
       " 'stone-shaped': 16,\n",
       " 'big-name': 16,\n",
       " 'curving': 16,\n",
       " 'restful': 16,\n",
       " 'glacial': 16,\n",
       " 'portuguese': 16,\n",
       " 'gridded': 16,\n",
       " 'splotchy': 16,\n",
       " 'late': 16,\n",
       " 'neocon': 16,\n",
       " 'brass-laminate': 16,\n",
       " 'lollipop-stick-inspired': 16,\n",
       " 'giant': 16,\n",
       " 'awful': 16,\n",
       " 'homogenous': 16,\n",
       " 'two-': 16,\n",
       " 'storycantilevered': 16,\n",
       " 'amazed': 16,\n",
       " 'uncommon': 16,\n",
       " 'passive': 16,\n",
       " 'trestle-like': 16,\n",
       " 'horse-shaped': 16,\n",
       " 'needed': 16,\n",
       " 'central': 16,\n",
       " 'cross-laminated': 16,\n",
       " 'high-density': 16,\n",
       " 'woollen': 16,\n",
       " 'dainty': 16,\n",
       " 'stacked': 16,\n",
       " 'ladder-like': 16,\n",
       " 'sail-shaped': 16,\n",
       " 'ascent': 16,\n",
       " 'asymmetrical': 16,\n",
       " 'patinated': 16,\n",
       " 'oriental': 16,\n",
       " 'incoming': 16,\n",
       " 'primordial': 16,\n",
       " 'layered': 16,\n",
       " 'seung-yong': 16,\n",
       " 'stereotyped': 16,\n",
       " 'epic': 16,\n",
       " '‘total': 16,\n",
       " 'transformable': 16,\n",
       " '‘transparent': 16,\n",
       " 'hand-made': 16,\n",
       " 'show-stopping': 16,\n",
       " 'prominent': 16,\n",
       " 'above-mentioned': 16,\n",
       " 'chair-shaped': 16,\n",
       " 'easy': 16,\n",
       " '13th': 16,\n",
       " 'tilted': 16,\n",
       " 'contrepoid': 16,\n",
       " 'captive': 16,\n",
       " 'hammered': 16,\n",
       " 'calming': 16,\n",
       " 'steam-bending': 16,\n",
       " 'martian': 16,\n",
       " 'soban': 15,\n",
       " 'stout': 15,\n",
       " 'five-centimetre': 15,\n",
       " 'two-toned': 15,\n",
       " 'london-based': 15,\n",
       " 'ordinary': 15,\n",
       " 'inward': 15,\n",
       " 'three-legged': 15,\n",
       " 'self-evident': 15,\n",
       " 'backr': 15,\n",
       " 'co-working': 15,\n",
       " 'hairy': 15,\n",
       " 'repeatable': 15,\n",
       " 'successive': 15,\n",
       " 'applied': 15,\n",
       " 'taiwanese': 15,\n",
       " 'disc-shaped': 15,\n",
       " 'shiny': 15,\n",
       " 'emotional': 15,\n",
       " 'appealing': 15,\n",
       " 'unified': 15,\n",
       " 'up-to-date': 15,\n",
       " 'stiff': 15,\n",
       " 'longterm': 15,\n",
       " 'longstanding': 15,\n",
       " 'comforting': 15,\n",
       " 'retro-style': 15,\n",
       " 'seattle-based': 15,\n",
       " 'pole-backed': 15,\n",
       " 'bi-annual': 15,\n",
       " 'eurocucina': 15,\n",
       " 'significant': 15,\n",
       " 'storypitt-pollaro': 15,\n",
       " 'foxy': 15,\n",
       " 'dish-like': 15,\n",
       " 'selective': 15,\n",
       " 'potential': 15,\n",
       " 'orthogonal': 15,\n",
       " 'spun-metal': 15,\n",
       " 'forward-facing': 15,\n",
       " 'softened': 15,\n",
       " 'endgrain': 15,\n",
       " 'french-style': 15,\n",
       " 'supportive': 15,\n",
       " 'cross-shaped': 15,\n",
       " 'lasercut': 15,\n",
       " 'upturned': 15,\n",
       " 'immobile': 15,\n",
       " 'bike-shaped': 15,\n",
       " 'articulated': 15,\n",
       " 'injection-moulded': 15,\n",
       " 'stainless-steel': 15,\n",
       " 'draped': 15,\n",
       " 'wing-like': 15,\n",
       " 'ever-changing': 15,\n",
       " 'elegant': 15,\n",
       " 'ultra': 15,\n",
       " 'problematic': 15,\n",
       " 'glue-coated': 15,\n",
       " 'familiar': 15,\n",
       " 'semi-private': 15,\n",
       " 'fibre-reinforced': 15,\n",
       " 'sootgood': 15,\n",
       " 'tiresome': 15,\n",
       " 'nude': 15,\n",
       " 'minimum': 15,\n",
       " 'paper-folding': 15,\n",
       " 'acceptable': 15,\n",
       " 'pliable': 15,\n",
       " 'rot-resistance': 15,\n",
       " 'emotive': 15,\n",
       " 'high-design': 15,\n",
       " 'fluent': 15,\n",
       " 'sturdy': 15,\n",
       " 'cocooned': 15,\n",
       " 'anodized': 15,\n",
       " 'heavy-duty': 15,\n",
       " 'rippled': 15,\n",
       " 'flat-bed': 15,\n",
       " 'eco-package': 14,\n",
       " 'venetian': 14,\n",
       " 'multiple': 14,\n",
       " 'satisfied': 14,\n",
       " 'deflated': 14,\n",
       " 'difficult': 14,\n",
       " 'vernacular': 14,\n",
       " 'iconic': 14,\n",
       " 'economic': 14,\n",
       " 'joyful': 14,\n",
       " 'emblematic': 14,\n",
       " 'scratch-resistant': 14,\n",
       " 'spiritual': 14,\n",
       " 'worldwide': 14,\n",
       " 'storyindustrial': 14,\n",
       " 'matching': 14,\n",
       " 'sci-fi': 14,\n",
       " 'imperfect': 14,\n",
       " 'figurative': 14,\n",
       " 'purple': 14,\n",
       " 'toronto-based': 14,\n",
       " 'citywide': 14,\n",
       " 'munich-based': 14,\n",
       " 'unprocessed': 14,\n",
       " 'rational': 14,\n",
       " 'blob-shaped': 14,\n",
       " 'monochrome': 14,\n",
       " 'high-street': 14,\n",
       " 'bean-shaped': 14,\n",
       " 'fancy': 14,\n",
       " 'line-drawn': 14,\n",
       " 'conspicuous': 14,\n",
       " 'criss-crossed': 14,\n",
       " 'sweden-based': 14,\n",
       " 'valencia-based': 14,\n",
       " 'writable': 14,\n",
       " 'writeable': 14,\n",
       " 'sit-down': 14,\n",
       " 'bulky': 14,\n",
       " 'suited': 14,\n",
       " 'post-world': 14,\n",
       " 'eight-legged': 14,\n",
       " 'untouched': 14,\n",
       " 'relevant': 14,\n",
       " 'lumbar': 14,\n",
       " 'bent-steel': 14,\n",
       " 'loud': 14,\n",
       " 'crumpled': 14,\n",
       " 'ragged': 14,\n",
       " 'copenhagen-based': 14,\n",
       " 'pedagogical': 14,\n",
       " 'skeletal': 14,\n",
       " 'modest': 14,\n",
       " 'multi-coloured': 14,\n",
       " 'consequent': 14,\n",
       " 'two-tier': 14,\n",
       " 'bio-based': 14,\n",
       " 'indigenous': 14,\n",
       " 'royal': 14,\n",
       " 'unused': 14,\n",
       " 'smoky': 14,\n",
       " 'shy': 14,\n",
       " 'connect': 14,\n",
       " 'threadlike': 14,\n",
       " 'shower-light': 14,\n",
       " 'box-type': 14,\n",
       " 'every-day': 14,\n",
       " 'restricted': 14,\n",
       " 'backlit': 14,\n",
       " 'house-shaped': 14,\n",
       " 'semi-transparent': 14,\n",
       " 'life-cycle': 14,\n",
       " 'sloppy': 14,\n",
       " 'mild': 14,\n",
       " 'aberrant': 14,\n",
       " 'mortice': 14,\n",
       " 'world-class': 14,\n",
       " 'ceaseless': 14,\n",
       " 'up-and-down': 14,\n",
       " 'well-': 14,\n",
       " '8mm': 14,\n",
       " 'fireproof': 14,\n",
       " 'secondary': 14,\n",
       " 'innate': 14,\n",
       " 'fresh': 14,\n",
       " 'instant': 14,\n",
       " 'slastic': 14,\n",
       " 'cylindrical': 14,\n",
       " 'cantilever': 14,\n",
       " 'intentional': 14,\n",
       " 'mechanised': 14,\n",
       " '30th': 14,\n",
       " 'exacting': 14,\n",
       " 'handy': 14,\n",
       " 'intensive': 14,\n",
       " 'extraneous': 14,\n",
       " 'lacquerer': 14,\n",
       " 'koloro-desk': 14,\n",
       " 'polyester-coated': 14,\n",
       " 'miscellaneous': 14,\n",
       " 'heat-resistant': 14,\n",
       " 're-usable': 14,\n",
       " 'brass-clad': 14,\n",
       " 'favourite': 14,\n",
       " 'baggy': 14,\n",
       " 'haphazardous': 14,\n",
       " 'down-scaled': 14,\n",
       " 'plant-based': 14,\n",
       " 'archaeological': 14,\n",
       " 'austere': 14,\n",
       " 'throne-like': 14,\n",
       " 'beirut-based': 14,\n",
       " 'healthy': 13,\n",
       " 'streamlined': 13,\n",
       " 'angular': 13,\n",
       " 'sloped': 13,\n",
       " 'ukrainian': 13,\n",
       " 'immersive': 13,\n",
       " 'handwoven': 13,\n",
       " 'unwanted': 13,\n",
       " 'lilac': 13,\n",
       " 'resistant': 13,\n",
       " 'mass-manufactured': 13,\n",
       " 'incomplete': 13,\n",
       " 'multicoloured': 13,\n",
       " 'dead': 13,\n",
       " 'surrealist': 13,\n",
       " 'select': 13,\n",
       " 'elliptical': 13,\n",
       " 'snug': 13,\n",
       " 'gabled': 13,\n",
       " 'furry': 13,\n",
       " 'oval-shaped': 13,\n",
       " 'white-painted': 13,\n",
       " 'electronic': 13,\n",
       " 'age-old': 13,\n",
       " 'stubby': 13,\n",
       " 'enduring': 13,\n",
       " 'oma-designed': 13,\n",
       " 'five-storey': 13,\n",
       " 'rival': 13,\n",
       " 'intimidating': 13,\n",
       " 'radial': 13,\n",
       " 'marine': 13,\n",
       " 'spacious': 13,\n",
       " 'surreal': 13,\n",
       " 'mushroom-shaped': 13,\n",
       " 'deadly': 13,\n",
       " 'teetering': 13,\n",
       " 'three-part': 13,\n",
       " 'disabled': 13,\n",
       " 'majestic': 13,\n",
       " 'tongue-in-cheek': 13,\n",
       " 'iridescent': 13,\n",
       " 'acid-stained': 13,\n",
       " 'polyamide': 13,\n",
       " 'interwoven': 13,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehots_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'well-priced': 0,\n",
       " 'modern': 1,\n",
       " 'country-friendly': 2,\n",
       " 'work-from-home': 3,\n",
       " 'glossy': 4,\n",
       " 'green-stained': 5,\n",
       " 'modulus': 6,\n",
       " 'fermented': 7,\n",
       " 'predominant': 8,\n",
       " 'able': 9,\n",
       " 'lesser-used': 10,\n",
       " 'baked': 11,\n",
       " 'mirrored': 12,\n",
       " 'high-quality': 13,\n",
       " 'asymmetric': 14,\n",
       " 'outdoor': 15,\n",
       " 'versatile': 16,\n",
       " 'weatherproof': 17,\n",
       " 'chinese': 18,\n",
       " 'international': 19,\n",
       " 'fair': 20,\n",
       " 'eco-package': 21,\n",
       " 'corrugated-cardboard': 22,\n",
       " 'corporate': 23,\n",
       " 'boundary': 24,\n",
       " 'ornamental': 25,\n",
       " 'conceptual': 26,\n",
       " 'environmental': 27,\n",
       " 'non-recyclable': 28,\n",
       " 'charitable': 29,\n",
       " 'healthy': 30,\n",
       " 'comfortable': 31,\n",
       " 'professional': 32,\n",
       " 'black-dyed': 33,\n",
       " 'dog-themed': 34,\n",
       " 'hollywood-style': 35,\n",
       " 'classical': 36,\n",
       " 'musical': 37,\n",
       " 'pandemic-friendly': 38,\n",
       " 'hand-drawn': 39,\n",
       " 'hyper-real': 40,\n",
       " 'soft-open': 41,\n",
       " 'functional': 42,\n",
       " 'placid': 43,\n",
       " 'exquisite': 44,\n",
       " 'post-industrialised': 45,\n",
       " 'custom-dyed': 46,\n",
       " 'wholesome': 47,\n",
       " 'air-dried': 48,\n",
       " 'environmentally-friendly': 49,\n",
       " 'antibacterial': 50,\n",
       " 'smart': 51,\n",
       " 'antiviral': 52,\n",
       " 'flat-pack': 53,\n",
       " 'chrome-based': 54,\n",
       " 'polluting': 55,\n",
       " 'wry': 56,\n",
       " 'venetian': 57,\n",
       " 'floaty': 58,\n",
       " 'worried': 59,\n",
       " 'aren': 60,\n",
       " 'kinked': 61,\n",
       " 'archival': 62,\n",
       " 'cubic': 63,\n",
       " 'zoomorphic': 64,\n",
       " 'rural': 65,\n",
       " 'rapid': 66,\n",
       " 'temporary': 67,\n",
       " 'charismatic': 68,\n",
       " 'clay-like': 69,\n",
       " 'affective': 70,\n",
       " 'doughy': 71,\n",
       " 'oil-coated': 72,\n",
       " 'frontal': 73,\n",
       " 'danish': 74,\n",
       " 'sustainable': 75,\n",
       " 'fibre-based': 76,\n",
       " 'ukurant': 77,\n",
       " 'blind': 78,\n",
       " 'squishy': 79,\n",
       " 'asian': 80,\n",
       " 'nordic': 81,\n",
       " 'happy': 82,\n",
       " 'blue': 83,\n",
       " 'elaborate': 84,\n",
       " 'deep': 85,\n",
       " 'virginal': 86,\n",
       " \"'stable\": 87,\n",
       " 'sophisticated': 88,\n",
       " 'fungi-like': 89,\n",
       " 'brick-red': 90,\n",
       " 'fungi-inspired': 91,\n",
       " 'concrete': 92,\n",
       " 'glass-fibre': 93,\n",
       " 'pigmenting': 94,\n",
       " 'primitive': 95,\n",
       " 'pixellated': 96,\n",
       " 'artistic': 97,\n",
       " 'japanese': 98,\n",
       " 'cotton-filled': 99,\n",
       " 'table-like': 100,\n",
       " 'resilient': 101,\n",
       " 'streamlined': 102,\n",
       " 'mechanical': 103,\n",
       " 'angular': 104,\n",
       " 'foam-like': 105,\n",
       " 'matte-black': 106,\n",
       " 'polar': 107,\n",
       " 'faceted': 108,\n",
       " 'multiple': 109,\n",
       " 'timber-lined': 110,\n",
       " 'double': 111,\n",
       " 'timeless': 112,\n",
       " 'flatpack': 113,\n",
       " 'seamless': 114,\n",
       " 'secret': 115,\n",
       " 'repeated': 116,\n",
       " 'complete': 117,\n",
       " 'poetic': 118,\n",
       " 'utrecht-based': 119,\n",
       " 'soft': 120,\n",
       " 'sancal': 121,\n",
       " 'unclear': 122,\n",
       " 'indoor-outdoor': 123,\n",
       " 'roman': 124,\n",
       " 'blocky': 125,\n",
       " 'soft-white': 126,\n",
       " 'tidal': 127,\n",
       " 'chunky': 128,\n",
       " 'hand-poured': 129,\n",
       " 'off-road': 130,\n",
       " 'drought-tolerant': 131,\n",
       " 'showcase': 132,\n",
       " 'sloped': 133,\n",
       " 'tufted': 134,\n",
       " 'ultimate': 135,\n",
       " 'ergonomic': 136,\n",
       " 'eurocentric': 137,\n",
       " 'black': 138,\n",
       " 'african': 139,\n",
       " 'remote': 140,\n",
       " 'soban': 141,\n",
       " 'emotionless': 142,\n",
       " 'cubist': 143,\n",
       " 'nuclear': 144,\n",
       " 'voluptuous': 145,\n",
       " 'contoured': 146,\n",
       " 'tubular': 147,\n",
       " 'ghanaian': 148,\n",
       " 'ethiopian-american': 149,\n",
       " 'social': 150,\n",
       " 'modular': 151,\n",
       " 'kiik': 152,\n",
       " 'uncompromised': 153,\n",
       " 'resident': 154,\n",
       " 'sole': 155,\n",
       " 'captioned': 156,\n",
       " 'post-lockdown': 157,\n",
       " 'customisable': 158,\n",
       " 'nature-based': 159,\n",
       " 'house-like': 160,\n",
       " 'stepped-level': 161,\n",
       " 'telar': 162,\n",
       " 'playful': 163,\n",
       " 'millimetre-square': 164,\n",
       " 'bare': 165,\n",
       " 'humble': 166,\n",
       " 'transparent': 167,\n",
       " 'positive/negative': 168,\n",
       " 'greenish': 169,\n",
       " 'low-tech': 170,\n",
       " 'solar': 171,\n",
       " 'light-weight': 172,\n",
       " 'casual': 173,\n",
       " 'austrian': 174,\n",
       " 'online': 175,\n",
       " 'non-standard': 176,\n",
       " 'chubby': 177,\n",
       " 'plump': 178,\n",
       " 'stout': 179,\n",
       " 'collectible': 180,\n",
       " 'half': 181,\n",
       " 'alternative': 182,\n",
       " 'stripped-down': 183,\n",
       " 'courtesy': 184,\n",
       " 'breakout': 185,\n",
       " 'two-week': 186,\n",
       " 'break-out': 187,\n",
       " 'l01': 188,\n",
       " 'l03': 189,\n",
       " 'low-rise': 190,\n",
       " 'fold-out': 191,\n",
       " 'public': 192,\n",
       " 'flat': 193,\n",
       " 'social-distancing': 194,\n",
       " 'covid-19': 195,\n",
       " 'global': 196,\n",
       " 'brief': 197,\n",
       " 'vertical': 198,\n",
       " 'brutalist': 199,\n",
       " 'clunky': 200,\n",
       " 'unorthodox': 201,\n",
       " 'bangkok-based': 202,\n",
       " 'depressed': 203,\n",
       " 'refractive': 204,\n",
       " 'reflective': 205,\n",
       " 'satisfied': 206,\n",
       " 'evolved': 207,\n",
       " 'renewed': 208,\n",
       " 'sparkly': 209,\n",
       " 'glassy': 210,\n",
       " 'molten': 211,\n",
       " 'offsite': 212,\n",
       " 'ambient': 213,\n",
       " 'unseen': 214,\n",
       " 'fabricated': 215,\n",
       " 'york-based': 216,\n",
       " 'fluorescent': 217,\n",
       " 'virtual': 218,\n",
       " 'common': 219,\n",
       " 'arched': 220,\n",
       " 'moisture-resistant': 221,\n",
       " 'feline': 222,\n",
       " 'detachable': 223,\n",
       " 'non-bpa': 224,\n",
       " 'storyarchitectural': 225,\n",
       " 'hands-free': 226,\n",
       " 'open-source': 227,\n",
       " 'medical': 228,\n",
       " 'sanctuary-like': 229,\n",
       " 'elasto-mechanical': 230,\n",
       " 'ukrainian': 231,\n",
       " 'ceramic': 232,\n",
       " 'female': 233,\n",
       " 'insulated': 234,\n",
       " 'reclusive': 235,\n",
       " 'double-width': 236,\n",
       " 'five-centimetre': 237,\n",
       " 'standard': 238,\n",
       " 'boro-embroidered': 239,\n",
       " 'cane-like': 240,\n",
       " 'rust-coloured': 241,\n",
       " 'innovative': 242,\n",
       " 'muutothis': 243,\n",
       " 'usb-c': 244,\n",
       " 'home-office': 245,\n",
       " 'remote-controlled': 246,\n",
       " 'cramped': 247,\n",
       " 'straight-edged': 248,\n",
       " 'prism-shaped': 249,\n",
       " 'undulating': 250,\n",
       " 'solid': 251,\n",
       " 'fleeting': 252,\n",
       " 'devious': 253,\n",
       " 'sumptuous': 254,\n",
       " 'rigid': 255,\n",
       " 'turned': 256,\n",
       " 'rococo': 257,\n",
       " 'finnish': 258,\n",
       " 'wiggly': 259,\n",
       " 'attractive': 260,\n",
       " 'norwegian': 261,\n",
       " 'distasteful': 262,\n",
       " 'pixel-like': 263,\n",
       " 'agile': 264,\n",
       " 'primary': 265,\n",
       " 'outfitted': 266,\n",
       " 'two-toned': 267,\n",
       " 'uv-filter': 268,\n",
       " 'scandinavian': 269,\n",
       " 'dependent': 270,\n",
       " 'random': 271,\n",
       " 'swedish': 272,\n",
       " 'unassuming': 273,\n",
       " 'sharp': 274,\n",
       " 'composite': 275,\n",
       " 'destructible': 276,\n",
       " 'outsized': 277,\n",
       " 'raffia-reminiscent': 278,\n",
       " 'institutional': 279,\n",
       " 'semicircular': 280,\n",
       " 'ashy': 281,\n",
       " 'emerald-coloured': 282,\n",
       " 'statuary': 283,\n",
       " 'empathetic': 284,\n",
       " 'mini-model': 285,\n",
       " 'unsuitable': 286,\n",
       " 'pillow-like': 287,\n",
       " 'sacred': 288,\n",
       " 'cheap': 289,\n",
       " 'mass': 290,\n",
       " 'plus-size': 291,\n",
       " 'futuristic': 292,\n",
       " 'outer': 293,\n",
       " 'polymorphic': 294,\n",
       " 'effusive': 295,\n",
       " 'slow': 296,\n",
       " 'expensive': 297,\n",
       " 'powder-coat': 298,\n",
       " 'non-conformist': 299,\n",
       " 'non-contextual': 300,\n",
       " 'self-propelled': 301,\n",
       " 'uptown': 302,\n",
       " 'beaux-arts': 303,\n",
       " '56th': 304,\n",
       " 'puffy': 305,\n",
       " 'immersive': 306,\n",
       " 'colourful': 307,\n",
       " 'sustainability-focussed': 308,\n",
       " 'london-based': 309,\n",
       " 'water-repellent': 310,\n",
       " 'handwoven': 311,\n",
       " 'rocky': 312,\n",
       " 'herkner-designed': 313,\n",
       " 'salute': 314,\n",
       " 'unwanted': 315,\n",
       " 'old': 316,\n",
       " 'rental': 317,\n",
       " 'aesthetic': 318,\n",
       " 'design-destination': 319,\n",
       " 'co-dependent': 320,\n",
       " 'odd-couple': 321,\n",
       " 'stilted': 322,\n",
       " 'geologic': 323,\n",
       " 'machine-driven': 324,\n",
       " 'fictive': 325,\n",
       " 'complicated': 326,\n",
       " 'metal-pressing': 327,\n",
       " 'useful': 328,\n",
       " 'inflatable': 329,\n",
       " 'chipboard': 330,\n",
       " 'deflated': 331,\n",
       " 'opposite': 332,\n",
       " 'light-strung': 333,\n",
       " 'self-titled': 334,\n",
       " 'nomadic': 335,\n",
       " 'mobile': 336,\n",
       " 'pre-furnished': 337,\n",
       " 'tasseled': 338,\n",
       " 'ellipse-shaped': 339,\n",
       " 'eyelash-like': 340,\n",
       " 'marbled': 341,\n",
       " 'single-use': 342,\n",
       " 'post-consumer': 343,\n",
       " 'ancient': 344,\n",
       " 'contemporary': 345,\n",
       " 'queer': 346,\n",
       " 'jubilant': 347,\n",
       " 'adorable': 348,\n",
       " 'rose-coloured': 349,\n",
       " 'excess': 350,\n",
       " 'domestic': 351,\n",
       " 'absent': 352,\n",
       " 'materialistic': 353,\n",
       " 'farewell': 354,\n",
       " 'homeless': 355,\n",
       " 'normal': 356,\n",
       " 'ordinary': 357,\n",
       " 'unstable': 358,\n",
       " 'affected': 359,\n",
       " 'wrongful': 360,\n",
       " 'korean': 361,\n",
       " 'western': 362,\n",
       " 'split': 363,\n",
       " 'polished': 364,\n",
       " 'mirrored-steel': 365,\n",
       " 'two-way': 366,\n",
       " 'swyft-lok': 367,\n",
       " 'grooved': 368,\n",
       " 'three-seater': 369,\n",
       " 'german': 370,\n",
       " 'difficult': 371,\n",
       " 'horizontal': 372,\n",
       " 'straight': 373,\n",
       " 'lilac': 374,\n",
       " 'urban': 375,\n",
       " 'graffiti-covered': 376,\n",
       " 'markerad': 377,\n",
       " 'grand': 378,\n",
       " 'subtile': 379,\n",
       " 'rundown': 380,\n",
       " 'politiken': 381,\n",
       " 'inward': 382,\n",
       " 'lesser-known': 383,\n",
       " 'tyne-based': 384,\n",
       " 'underserved': 385,\n",
       " 'bent-metal': 386,\n",
       " 'sand-cast': 387,\n",
       " 'eastern': 388,\n",
       " 'middle': 389,\n",
       " 'golden': 390,\n",
       " 'retail': 391,\n",
       " 'multifunction': 392,\n",
       " 'worn': 393,\n",
       " 'conscious': 394,\n",
       " 'unsellable': 395,\n",
       " 'worth': 396,\n",
       " 'angled-metal': 397,\n",
       " 'appreciative': 398,\n",
       " 'resistant': 399,\n",
       " \"'packed\": 400,\n",
       " 'three-man': 401,\n",
       " 'tight': 402,\n",
       " 'custom-milled': 403,\n",
       " 'friction-based': 404,\n",
       " 'hyped': 405,\n",
       " 'red-nail': 406,\n",
       " 'hand-turned': 407,\n",
       " 'movable': 408,\n",
       " 'bulbous': 409,\n",
       " 'persian': 410,\n",
       " 'synesthetic': 411,\n",
       " 'recycled': 412,\n",
       " 'plastic-packaging': 413,\n",
       " 'key': 414,\n",
       " 'dystopian': 415,\n",
       " 'beige': 416,\n",
       " 'sand-covered': 417,\n",
       " 'south': 418,\n",
       " 'convivial': 419,\n",
       " 'divergent': 420,\n",
       " 'club-style': 421,\n",
       " 'first-come-first-serve': 422,\n",
       " 'glad': 423,\n",
       " 'long-awaited': 424,\n",
       " 'woven-cane': 425,\n",
       " 'doodle-like': 426,\n",
       " 'one-armed': 427,\n",
       " 'undergraduate': 428,\n",
       " 'multi-sensory': 429,\n",
       " 'constituent': 430,\n",
       " 'original': 431,\n",
       " 'harvestable': 432,\n",
       " 'converted': 433,\n",
       " 'tree-based': 434,\n",
       " 'room-partition': 435,\n",
       " 'double-height': 436,\n",
       " 'mexican': 437,\n",
       " 'blush-coloured': 438,\n",
       " 'net-like': 439,\n",
       " 'cooperative': 440,\n",
       " 'fsc-certified': 441,\n",
       " 'plastic': 442,\n",
       " 'impatient': 443,\n",
       " 'worthy': 444,\n",
       " 'grungy': 445,\n",
       " 'gothic-inspired': 446,\n",
       " 'phone-charging': 447,\n",
       " 'wave-like': 448,\n",
       " 'cartoon-style': 449,\n",
       " 'pedestrianised': 450,\n",
       " 'big': 451,\n",
       " 'surprising': 452,\n",
       " 'oppressive': 453,\n",
       " 'sit-stand': 454,\n",
       " 'mid-century': 455,\n",
       " 'upstate': 456,\n",
       " 'australian': 457,\n",
       " 'european': 458,\n",
       " 'sewn': 459,\n",
       " 'peaked': 460,\n",
       " 'brick-clad': 461,\n",
       " 'full-floor': 462,\n",
       " 'grayish': 463,\n",
       " 'recyclable': 464,\n",
       " 'non-polluting': 465,\n",
       " 'celestial': 466,\n",
       " 'astrological': 467,\n",
       " 'rainbow-like': 468,\n",
       " 'washer-drier': 469,\n",
       " 'phenomenal': 470,\n",
       " 'replaceable': 471,\n",
       " 'sheet-like': 472,\n",
       " 'intact': 473,\n",
       " 'uninterrupted': 474,\n",
       " 'middle-class': 475,\n",
       " 'oval': 476,\n",
       " 'good': 477,\n",
       " 'proper': 478,\n",
       " 'textile': 479,\n",
       " 'dutch': 480,\n",
       " 'print-like': 481,\n",
       " 'sexist': 482,\n",
       " 'powerful': 483,\n",
       " 'imperative': 484,\n",
       " 'unfinished': 485,\n",
       " 'electric': 486,\n",
       " 'unregulated': 487,\n",
       " 'low-income': 488,\n",
       " 'practical': 489,\n",
       " 'vernacular': 490,\n",
       " 'iconic': 491,\n",
       " 'mass-manufactured': 492,\n",
       " 'walnut-veneered': 493,\n",
       " 'financial': 494,\n",
       " 'aggressive': 495,\n",
       " 'seaside': 496,\n",
       " 'lounge-based': 497,\n",
       " 'variable': 498,\n",
       " 'instantaneous': 499,\n",
       " 'impaired': 500,\n",
       " 'coloured-glass': 501,\n",
       " 'azure': 502,\n",
       " 'uncoloured-brass': 503,\n",
       " 'economic': 504,\n",
       " 'postwar': 505,\n",
       " 'pink': 506,\n",
       " 'thermo-polymer': 507,\n",
       " 'first-place': 508,\n",
       " 'bérénice': 509,\n",
       " 'three-legged': 510,\n",
       " 'red': 511,\n",
       " 'numerous': 512,\n",
       " 'enamel': 513,\n",
       " 'upholstered': 514,\n",
       " 'irregular': 515,\n",
       " 'all-women': 516,\n",
       " 'square-sized': 517,\n",
       " 'bad': 518,\n",
       " 'biodegradable': 519,\n",
       " 'bio-focused': 520,\n",
       " 'executive': 521,\n",
       " 'leather-upholstered': 522,\n",
       " 'industrial': 523,\n",
       " 'orange': 524,\n",
       " 'young': 525,\n",
       " 'arguable': 526,\n",
       " 'incomplete': 527,\n",
       " 'makeshift': 528,\n",
       " 'collaborative': 529,\n",
       " 'empathic': 530,\n",
       " 'joyful': 531,\n",
       " 'ecstatic': 532,\n",
       " 'bitchy': 533,\n",
       " 'ex-industrial': 534,\n",
       " 'antique': 535,\n",
       " 'hind': 536,\n",
       " 'time-worn': 537,\n",
       " 'multicoloured': 538,\n",
       " 'astrain': 539,\n",
       " 'heart-shape': 540,\n",
       " 'filipino': 541,\n",
       " 'imperial': 542,\n",
       " 'miniature': 543,\n",
       " 'exuberant': 544,\n",
       " 'whimsical': 545,\n",
       " 'hand-woven': 546,\n",
       " 'extraterrestrial': 547,\n",
       " 'antenna-like': 548,\n",
       " 'precious': 549,\n",
       " 'heartbroken': 550,\n",
       " 'half-heart': 551,\n",
       " 'wild': 552,\n",
       " 'adapted': 553,\n",
       " 'tame': 554,\n",
       " 'fétiche': 555,\n",
       " 'disposable': 556,\n",
       " 'sufficient': 557,\n",
       " 'pay-attention-to-me': 558,\n",
       " 'ridge-like': 559,\n",
       " 'one-eyed': 560,\n",
       " 'scottish': 561,\n",
       " 'aggregate': 562,\n",
       " 'single-line': 563,\n",
       " 'rope-like': 564,\n",
       " 'blue-room': 565,\n",
       " 'lounge-style': 566,\n",
       " 'dark-blue': 567,\n",
       " 'emblematic': 568,\n",
       " 'sentient': 569,\n",
       " 'cosmology-inspired': 570,\n",
       " 'local': 571,\n",
       " 'scratch-resistant': 572,\n",
       " 'brutal': 573,\n",
       " 'poppy': 574,\n",
       " 'captivating': 575,\n",
       " 'dysfunctional': 576,\n",
       " 'vitruvian': 577,\n",
       " 'blurry': 578,\n",
       " 'indeterminate': 579,\n",
       " 'eastern-style': 580,\n",
       " 'padded': 581,\n",
       " 'stellar': 582,\n",
       " 'height-adjustable': 583,\n",
       " 'military': 584,\n",
       " 'steel-cage': 585,\n",
       " 'brick-house': 586,\n",
       " 'wooden-constructed': 587,\n",
       " 'loaded': 588,\n",
       " 'perfect': 589,\n",
       " 'non-commissioned': 590,\n",
       " 'poisonous': 591,\n",
       " 'apartment-style': 592,\n",
       " 'architects-designed': 593,\n",
       " 'locally-based': 594,\n",
       " 'accelerated': 595,\n",
       " 'mid': 596,\n",
       " 'on-demand': 597,\n",
       " 'activity-based': 598,\n",
       " 'plenty': 599,\n",
       " 'mono-shell': 600,\n",
       " 'four-leg': 601,\n",
       " 'steam-bent': 602,\n",
       " 'mid-19th': 603,\n",
       " 'five-axis': 604,\n",
       " 'lumpy': 605,\n",
       " 'curvy': 606,\n",
       " 'direct': 607,\n",
       " 'generous': 608,\n",
       " 'light-fast': 609,\n",
       " 'homely': 610,\n",
       " 'lindencrone': 611,\n",
       " 'grandiose': 612,\n",
       " 'image-only': 613,\n",
       " 'collage-like': 614,\n",
       " 'arresting': 615,\n",
       " 'esoteric': 616,\n",
       " 'vaguely-japanese': 617,\n",
       " 'oxidised': 618,\n",
       " 'postmodern': 619,\n",
       " 'limited-edition': 620,\n",
       " 'storyitalian': 621,\n",
       " 'organic': 622,\n",
       " 'known': 623,\n",
       " 'positive': 624,\n",
       " 'spiritual': 625,\n",
       " 'mundane': 626,\n",
       " 'broken': 627,\n",
       " 'fragmentary': 628,\n",
       " 'marble-making': 629,\n",
       " 'political': 630,\n",
       " 'synthetic': 631,\n",
       " 'radical': 632,\n",
       " 'double-sided': 633,\n",
       " 'interchangeable': 634,\n",
       " 'belgian-american': 635,\n",
       " 'dead': 636,\n",
       " 'freelance': 637,\n",
       " 'decorative': 638,\n",
       " 'powder-like': 639,\n",
       " 'rudimental': 640,\n",
       " 'manipulative': 641,\n",
       " 'combinatory': 642,\n",
       " 'sixties-inspired': 643,\n",
       " 'editorial': 644,\n",
       " 'rectilinear': 645,\n",
       " 'worldwide': 646,\n",
       " 'protective': 647,\n",
       " 'latvian': 648,\n",
       " 'monolithic': 649,\n",
       " 'igneous': 650,\n",
       " 'grotesque': 651,\n",
       " 'large-grid': 652,\n",
       " 'forested': 653,\n",
       " 'economy-class': 654,\n",
       " 'self-evident': 655,\n",
       " 'unselected': 656,\n",
       " 'particular': 657,\n",
       " 'free-hand': 658,\n",
       " 'inherent': 659,\n",
       " 'storysun-like': 660,\n",
       " 'rough': 661,\n",
       " 'bamboo-stick': 662,\n",
       " 'unusual': 663,\n",
       " 'mythical': 664,\n",
       " 'linear': 665,\n",
       " 'understated': 666,\n",
       " 'insect-like': 667,\n",
       " 'research-oriented': 668,\n",
       " 'adequate': 669,\n",
       " 'storyindustrial': 670,\n",
       " 'under-frame': 671,\n",
       " 'informal': 672,\n",
       " 'menial': 673,\n",
       " 'notorious': 674,\n",
       " 'backward': 675,\n",
       " 'unfashionable': 676,\n",
       " 'swooping': 677,\n",
       " 'lunar-themed': 678,\n",
       " 'slung-leather': 679,\n",
       " 'polished-chrome': 680,\n",
       " 'wicker-effect': 681,\n",
       " 'competitive': 682,\n",
       " 'modernist': 683,\n",
       " 'irregular-shaped': 684,\n",
       " 'museum-like': 685,\n",
       " 'fuchsia': 686,\n",
       " 'camouflage-patterned': 687,\n",
       " 'holy': 688,\n",
       " 'four-star': 689,\n",
       " 'low-slung': 690,\n",
       " 'standing': 691,\n",
       " 'indonesian': 692,\n",
       " 'matching': 693,\n",
       " 'compara\\xadble': 694,\n",
       " 'consecutive': 695,\n",
       " 'growing-themed': 696,\n",
       " 'open-faced': 697,\n",
       " 'noise-cancelling': 698,\n",
       " 'active': 699,\n",
       " 'stress-free': 700,\n",
       " 'circuitous': 701,\n",
       " 'null': 702,\n",
       " 'nesting': 703,\n",
       " 'rubbish-clearing': 704,\n",
       " 'geothermal': 705,\n",
       " 'volcanic': 706,\n",
       " 'hidden': 707,\n",
       " 'uncontrollable': 708,\n",
       " 'blue-tinted': 709,\n",
       " 'walnut-wood': 710,\n",
       " 'tempered-glass': 711,\n",
       " 'metal-rich': 712,\n",
       " 'pessimistic': 713,\n",
       " 'speculative': 714,\n",
       " 'vintage': 715,\n",
       " 'unavailable': 716,\n",
       " 'surrealist': 717,\n",
       " 'dystopic': 718,\n",
       " 'pirarucu': 719,\n",
       " 'second-hand': 720,\n",
       " 'ego': 721,\n",
       " 'frothy': 722,\n",
       " 'homeware': 723,\n",
       " 'select': 724,\n",
       " 'hand-stitched': 725,\n",
       " 'thisable': 726,\n",
       " 'grabable': 727,\n",
       " 'slippery': 728,\n",
       " 'acoustic': 729,\n",
       " 'thermal': 730,\n",
       " 'generative': 731,\n",
       " 'noce': 732,\n",
       " 'forest-like': 733,\n",
       " 'world-famous': 734,\n",
       " 'digital': 735,\n",
       " 'real': 736,\n",
       " 'sinuous': 737,\n",
       " 'twisted': 738,\n",
       " 'townhouse-style': 739,\n",
       " 'wood-panelled': 740,\n",
       " 'samurai-inspired': 741,\n",
       " 'syrian': 742,\n",
       " 'safe': 743,\n",
       " 'less-fortunate': 744,\n",
       " 'rebellious': 745,\n",
       " 'stubborn': 746,\n",
       " 'endearing': 747,\n",
       " 'selfish': 748,\n",
       " 'human-size': 749,\n",
       " 'conductive': 750,\n",
       " 'mid-haul': 751,\n",
       " 'portable': 752,\n",
       " 'unfurnished': 753,\n",
       " 'deserved': 754,\n",
       " 'terminal': 755,\n",
       " 'standalone': 756,\n",
       " 'chromatic': 757,\n",
       " 'medium-density-foam': 758,\n",
       " 'savvy': 759,\n",
       " 'sci-fi': 760,\n",
       " 'lebanese-american': 761,\n",
       " 'galactic': 762,\n",
       " 'backr': 763,\n",
       " 'wooden': 764,\n",
       " 'aged': 765,\n",
       " 'user-friendly': 766,\n",
       " 'sensational': 767,\n",
       " 'raw': 768,\n",
       " 'elliptical': 769,\n",
       " 'restrained': 770,\n",
       " 'utopian': 771,\n",
       " 'dirty': 772,\n",
       " 'imaginary': 773,\n",
       " 'two-dimensional': 774,\n",
       " 'three-dimensional': 775,\n",
       " 'imperfect': 776,\n",
       " 'handmade': 777,\n",
       " 'antiqued': 778,\n",
       " 'white-dyed': 779,\n",
       " 'rocking': 780,\n",
       " 'chrome-plated': 781,\n",
       " 'highly-skilled': 782,\n",
       " 'beautiful': 783,\n",
       " 'gender-fluid': 784,\n",
       " 'multifunctional': 785,\n",
       " 'overlaid': 786,\n",
       " 'wooden-slatted': 787,\n",
       " 'swivel': 788,\n",
       " 'correct': 789,\n",
       " 'square': 790,\n",
       " 'early-20th-century': 791,\n",
       " 'symmetric': 792,\n",
       " 'brazilian': 793,\n",
       " 'assyrian': 794,\n",
       " 'palm-wood': 795,\n",
       " 'daybe': 796,\n",
       " 'copper-coated': 797,\n",
       " 'archetypal': 798,\n",
       " 'mathematical': 799,\n",
       " 'pile': 800,\n",
       " 'paulo-based': 801,\n",
       " 'cuddly': 802,\n",
       " 'identifiable': 803,\n",
       " 'valuable': 804,\n",
       " 'chemical': 805,\n",
       " 'scissor-style': 806,\n",
       " 'pre-approved': 807,\n",
       " 'backless': 808,\n",
       " 'sheet-plastic': 809,\n",
       " 'short-hair': 810,\n",
       " 'reclined': 811,\n",
       " 'beech': 812,\n",
       " 'greco-roman-inspired': 813,\n",
       " 'kuwaiti': 814,\n",
       " 'laptop-based': 815,\n",
       " 'desk-shy': 816,\n",
       " 'monotonous': 817,\n",
       " 'tired': 818,\n",
       " 'neutral': 819,\n",
       " 'stressed': 820,\n",
       " 'snug': 821,\n",
       " 'barcelonaa': 822,\n",
       " 'off-cut': 823,\n",
       " 'minimalist': 824,\n",
       " 'gabled': 825,\n",
       " 'american': 826,\n",
       " 'cuban-american': 827,\n",
       " 'grainy': 828,\n",
       " 'bio-resin': 829,\n",
       " 'mocha-looking': 830,\n",
       " 'open-plan': 831,\n",
       " 'fluttua': 832,\n",
       " 'stratified': 833,\n",
       " 'book-filled': 834,\n",
       " 'one-step': 835,\n",
       " 'robust': 836,\n",
       " 'california-based': 837,\n",
       " 'carbon-fibre': 838,\n",
       " 'research-led': 839,\n",
       " 'coalesse': 840,\n",
       " 'city-based': 841,\n",
       " 'dark': 842,\n",
       " 'permanent': 843,\n",
       " 'co-working': 844,\n",
       " 'monochromatic': 845,\n",
       " 'white': 846,\n",
       " 'furry': 847,\n",
       " 'hairy': 848,\n",
       " 'laundry': 849,\n",
       " 'space-saving': 850,\n",
       " 'visible': 851,\n",
       " 'degradable': 852,\n",
       " 'seven-year': 853,\n",
       " 'amakusa': 854,\n",
       " 'built-in': 855,\n",
       " 'brass-framed': 856,\n",
       " 'veiny': 857,\n",
       " 'cushion-covered': 858,\n",
       " 'surgical': 859,\n",
       " 'moon-inspired': 860,\n",
       " 'super-talented': 861,\n",
       " 'androgynous': 862,\n",
       " 'blackened-oak': 863,\n",
       " 'elliptic': 864,\n",
       " 'cotton-covered': 865,\n",
       " 'surf-rounded': 866,\n",
       " 'skeletal-like': 867,\n",
       " 'theoretical': 868,\n",
       " 'year-old': 869,\n",
       " 'square-shaped': 870,\n",
       " 'tall': 871,\n",
       " 'powerless': 872,\n",
       " 'afrofuturist': 873,\n",
       " 'carribean': 874,\n",
       " 'sexual': 875,\n",
       " 'late-night': 876,\n",
       " 'cloistered': 877,\n",
       " 'hand-blackened': 878,\n",
       " 'galleryirish': 879,\n",
       " 'restless': 880,\n",
       " 'explorative': 881,\n",
       " 'mad': 882,\n",
       " 'earnest': 883,\n",
       " 'long-necked': 884,\n",
       " 'slip-casting': 885,\n",
       " 'robotic': 886,\n",
       " 'cognitive': 887,\n",
       " 'overwhelmed': 888,\n",
       " 'cultural': 889,\n",
       " 'responsive': 890,\n",
       " 'everyday': 891,\n",
       " 'spring-like': 892,\n",
       " 'cluttered': 893,\n",
       " 'náhuatl': 894,\n",
       " 'desirable': 895,\n",
       " 'hand-operated': 896,\n",
       " 'grey': 897,\n",
       " 'thoughtful': 898,\n",
       " 'easy-to-construct': 899,\n",
       " 'tripodal': 900,\n",
       " 'line-judge': 901,\n",
       " 'cool': 902,\n",
       " 'open': 903,\n",
       " 'high-back': 904,\n",
       " 'outline': 905,\n",
       " 'six-axis': 906,\n",
       " 'oscillating': 907,\n",
       " 'soy-based': 908,\n",
       " 'big-designed': 909,\n",
       " 'angeles-based': 910,\n",
       " 'perennial': 911,\n",
       " 'elastic': 912,\n",
       " 'curtain-like': 913,\n",
       " 'centimetre-tall': 914,\n",
       " 'high-rise': 915,\n",
       " 'mixed-use': 916,\n",
       " 'amazing': 917,\n",
       " 'nearby': 918,\n",
       " 'anodised-aluminum': 919,\n",
       " 'defiant': 920,\n",
       " 'antwerp-based': 921,\n",
       " 'figurative': 922,\n",
       " 'knit-covered': 923,\n",
       " 'enriching': 924,\n",
       " 'intermediary': 925,\n",
       " 'lopsided': 926,\n",
       " 'post-internet': 927,\n",
       " '20th': 928,\n",
       " 'remarkable': 929,\n",
       " 'forefront': 930,\n",
       " 'atomic': 931,\n",
       " 'californian': 932,\n",
       " 'monthly': 933,\n",
       " \"'neutral\": 934,\n",
       " 'harmonic': 935,\n",
       " 'oblique': 936,\n",
       " 'pool-shaped': 937,\n",
       " 'silvered': 938,\n",
       " 'dumbell-shaped': 939,\n",
       " 'unborn': 940,\n",
       " 'audacious': 941,\n",
       " 'gracious': 942,\n",
       " 'french-lebanese': 943,\n",
       " 'patented': 944,\n",
       " 'varied': 945,\n",
       " 'structural': 946,\n",
       " 'sydney-based': 947,\n",
       " 'lurvig': 948,\n",
       " 'replenished': 949,\n",
       " 'greedy': 950,\n",
       " 'framemust': 951,\n",
       " 'ultra-violet': 952,\n",
       " 'communicative': 953,\n",
       " 'lebanese': 954,\n",
       " 'second': 955,\n",
       " 'hostile': 956,\n",
       " 'anti-homeless': 957,\n",
       " 'curvy-shaped': 958,\n",
       " 'simple-shaped': 959,\n",
       " 'malleable': 960,\n",
       " 'orange-coloured': 961,\n",
       " \"'side\": 962,\n",
       " 'human': 963,\n",
       " 'pop-art-inspired': 964,\n",
       " 'slogan-covered': 965,\n",
       " 'compact': 966,\n",
       " 'high-end': 967,\n",
       " 'high-top': 968,\n",
       " '3d': 969,\n",
       " 'soviet': 970,\n",
       " 'oval-shaped': 971,\n",
       " 'teenage': 972,\n",
       " 'maximalist': 973,\n",
       " 'muted': 974,\n",
       " 'minimal': 975,\n",
       " 'aqueous': 976,\n",
       " 'paper-like': 977,\n",
       " 'multiple-layered': 978,\n",
       " 'white-painted': 979,\n",
       " 'buttery': 980,\n",
       " 'yellow-hued': 981,\n",
       " 'all-encompassing': 982,\n",
       " 'grey-brown': 983,\n",
       " 'sought-after': 984,\n",
       " 'customary': 985,\n",
       " 'carpet-covered': 986,\n",
       " 'office-focussed': 987,\n",
       " 'pull-less': 988,\n",
       " 'ongoing': 989,\n",
       " 'u-shaped': 990,\n",
       " 'purple': 991,\n",
       " 'deceased': 992,\n",
       " 'studded': 993,\n",
       " 'repeatable': 994,\n",
       " 'whale-shaped': 995,\n",
       " 'white-fringe': 996,\n",
       " 'nonchalant': 997,\n",
       " 'illusionary': 998,\n",
       " 'holistic': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehots_w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unbalanced_weights(vocab, w2i):\n",
    "    all_labels = sum(vocab.values())\n",
    "    weights = [0]*len(w2i)\n",
    "    for k, val in w2i.items():\n",
    "        weights[val] = vocab[k] / all_labels\n",
    "    return torch.FloatTensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_WEIGHTS = create_unbalanced_weights(onehots_vocab, onehots_w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_dict(d):\n",
    "    l = list(d.items())\n",
    "    random.shuffle(l)\n",
    "    d = dict(l)\n",
    "    return d\n",
    "\n",
    "def splitDict(d_img_words, percent, val_number):\n",
    "\n",
    "    val_n = val_number\n",
    "    train_test_size = len(d_img_words) - val_n\n",
    "    train_n = int(train_test_size*percent)\n",
    "    test_n = train_test_size - train_n\n",
    "    \n",
    "    d_img_words = shuffle_dict(d_img_words)\n",
    "    im_words = iter(d_img_words.items())      \n",
    "    \n",
    "    # Image - words\n",
    "    dtrain_imw = dict(itertools.islice(im_words, train_n))  \n",
    "    dtest_imw = dict(itertools.islice(im_words, test_n))   \n",
    "    dval_imw = dict(itertools.islice(im_words, val_n))\n",
    "    \n",
    "    \n",
    "    print('trainset size: ', len(dtrain_imw), 'test dataset size: ',len(dtest_imw), 'val set size: ', len(dval_imw))\n",
    "    return dtrain_imw, dtest_imw, dval_imw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size:  13225 test dataset size:  3307 val set size:  1000\n"
     ]
    }
   ],
   "source": [
    "dtrain_w, dtest_w, dval_w = splitDict(onehots, .8, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save train test and val sets in json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_json('./train_test_val_multilabel/trainset_3l.json', dtrain_w)\n",
    "# save_json('./train_test_val_multilabel/testset_3l.json', dtest_w)\n",
    "# save_json('./train_test_val_multilabel/valset_3l.json', dval_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_w = open_json('./train_test_val_dataset_splits/trainset_3l.json')\n",
    "dtest_w = open_json('./train_test_val_dataset_splits/testset_3l.json')\n",
    "dval_w = open_json('./train_test_val_dataset_splits/valset_3l.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, onehots, w2i, i2w, image_path):\n",
    "        self.onehots_d = onehots\n",
    "        self.w2i = w2i\n",
    "        self.i2w = i2w\n",
    "        self.images_names = list(self.onehots_d.keys()) # names\n",
    "        self.onehots = list(self.onehots_d.values()) # onehots\n",
    "        self.image_path = image_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.onehots)\n",
    "    \n",
    "    def get_image_tensor(self, image_name):\n",
    "        \"\"\"\n",
    "        Gets image name and returns a tensor\n",
    "        \"\"\"\n",
    "        name = self.image_path + \"/\" + image_name\n",
    "        img = Image.open(name)\n",
    "        img = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor()])(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name_image = self.images_names[index]\n",
    "        img = self.get_image_tensor(name_image)\n",
    "        onehot = torch.FloatTensor(self.onehots[index])\n",
    "        \n",
    "        return img, onehot, name_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataset(dtrain_w, onehots_w2i, onehots_i2w, im_path_fur)\n",
    "dataset_test = MyDataset(dtest_w, onehots_w2i, onehots_i2w,im_path_fur)\n",
    "dataset_val = MyDataset(dval_w, onehots_w2i, onehots_i2w,  im_path_fur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 8 if cuda else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.9647, 0.9647, 0.9647,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.9569, 0.9647, 0.9608,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.9608, 0.9529, 0.9451,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          ...,\n",
      "          [0.9725, 0.9725, 0.9725,  ..., 0.9137, 0.9059, 0.8902],\n",
      "          [0.9725, 0.9686, 0.9686,  ..., 0.9137, 0.9059, 0.8863],\n",
      "          [0.9686, 0.9725, 0.9686,  ..., 0.9137, 0.9059, 0.8824]],\n",
      "\n",
      "         [[0.9647, 0.9647, 0.9647,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.9686, 0.9647, 0.9647,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.9255, 0.8980, 0.8902,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          ...,\n",
      "          [0.9725, 0.9725, 0.9686,  ..., 0.9059, 0.9020, 0.8431],\n",
      "          [0.9725, 0.9686, 0.9647,  ..., 0.9059, 0.9020, 0.8353],\n",
      "          [0.9686, 0.9725, 0.9647,  ..., 0.9059, 0.9059, 0.8314]],\n",
      "\n",
      "         [[0.9647, 0.9647, 0.9608,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.9608, 0.9647, 0.9725,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.8745, 0.8275, 0.8157,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          ...,\n",
      "          [0.9725, 0.9725, 0.9686,  ..., 0.8902, 0.8863, 0.8000],\n",
      "          [0.9725, 0.9686, 0.9647,  ..., 0.8902, 0.8941, 0.7882],\n",
      "          [0.9686, 0.9725, 0.9647,  ..., 0.8902, 0.8941, 0.7804]]],\n",
      "\n",
      "\n",
      "        [[[0.7843, 0.7882, 0.7882,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          [0.7882, 0.7922, 0.7882,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          [0.7882, 0.7922, 0.7922,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          ...,\n",
      "          [0.8196, 0.8196, 0.8235,  ..., 0.9137, 0.9137, 0.9137],\n",
      "          [0.8196, 0.8235, 0.8235,  ..., 0.9059, 0.9059, 0.9059],\n",
      "          [0.8235, 0.8275, 0.8275,  ..., 0.9059, 0.9059, 0.9059]],\n",
      "\n",
      "         [[0.7843, 0.7882, 0.7882,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          [0.7882, 0.7922, 0.7882,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          [0.7882, 0.7922, 0.7922,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          ...,\n",
      "          [0.8196, 0.8196, 0.8235,  ..., 0.9176, 0.9176, 0.9176],\n",
      "          [0.8196, 0.8235, 0.8235,  ..., 0.9098, 0.9098, 0.9098],\n",
      "          [0.8235, 0.8275, 0.8275,  ..., 0.9098, 0.9098, 0.9098]],\n",
      "\n",
      "         [[0.7765, 0.7804, 0.7804,  ..., 0.8706, 0.8706, 0.8706],\n",
      "          [0.7804, 0.7843, 0.7804,  ..., 0.8706, 0.8706, 0.8706],\n",
      "          [0.7804, 0.7843, 0.7843,  ..., 0.8706, 0.8706, 0.8706],\n",
      "          ...,\n",
      "          [0.8275, 0.8275, 0.8314,  ..., 0.9333, 0.9333, 0.9333],\n",
      "          [0.8275, 0.8314, 0.8314,  ..., 0.9255, 0.9255, 0.9255],\n",
      "          [0.8314, 0.8353, 0.8353,  ..., 0.9255, 0.9255, 0.9255]]],\n",
      "\n",
      "\n",
      "        [[[0.8353, 0.8431, 0.8392,  ..., 0.7451, 0.7451, 0.7529],\n",
      "          [0.8314, 0.8392, 0.8392,  ..., 0.7490, 0.7490, 0.7569],\n",
      "          [0.8314, 0.8392, 0.8392,  ..., 0.7529, 0.7529, 0.7608],\n",
      "          ...,\n",
      "          [0.5961, 0.5961, 0.5804,  ..., 0.8118, 0.8235, 0.8275],\n",
      "          [0.6000, 0.5961, 0.5725,  ..., 0.8078, 0.8196, 0.8275],\n",
      "          [0.5961, 0.5686, 0.5647,  ..., 0.8039, 0.8157, 0.8235]],\n",
      "\n",
      "         [[0.8314, 0.8392, 0.8353,  ..., 0.7451, 0.7451, 0.7529],\n",
      "          [0.8275, 0.8353, 0.8353,  ..., 0.7490, 0.7490, 0.7569],\n",
      "          [0.8275, 0.8353, 0.8353,  ..., 0.7529, 0.7529, 0.7608],\n",
      "          ...,\n",
      "          [0.5451, 0.5451, 0.5294,  ..., 0.8353, 0.8392, 0.8431],\n",
      "          [0.5529, 0.5490, 0.5255,  ..., 0.8314, 0.8392, 0.8431],\n",
      "          [0.5412, 0.5137, 0.5137,  ..., 0.8275, 0.8314, 0.8392]],\n",
      "\n",
      "         [[0.8157, 0.8235, 0.8196,  ..., 0.7373, 0.7412, 0.7529],\n",
      "          [0.8118, 0.8196, 0.8196,  ..., 0.7490, 0.7490, 0.7569],\n",
      "          [0.8118, 0.8196, 0.8196,  ..., 0.7529, 0.7529, 0.7608],\n",
      "          ...,\n",
      "          [0.4824, 0.4863, 0.4824,  ..., 0.8824, 0.8863, 0.8902],\n",
      "          [0.4941, 0.4902, 0.4627,  ..., 0.8863, 0.8863, 0.8863],\n",
      "          [0.4824, 0.4510, 0.4510,  ..., 0.8824, 0.8902, 0.8902]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.9961, 0.9961, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9961, 0.9961, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.9725, 0.9725, 0.9725,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9804, 0.9804, 0.9804,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9020, 0.9020, 0.9059,  ..., 0.9255, 0.9255, 0.9294],\n",
      "          [0.9020, 0.9020, 0.9059,  ..., 0.9255, 0.9255, 0.9294],\n",
      "          [0.9020, 0.9020, 0.9059,  ..., 0.9255, 0.9255, 0.9294],\n",
      "          ...,\n",
      "          [0.9020, 0.9020, 0.9020,  ..., 0.9647, 0.9647, 0.9647],\n",
      "          [0.9059, 0.9059, 0.9059,  ..., 0.9686, 0.9647, 0.9647],\n",
      "          [0.9059, 0.9059, 0.9098,  ..., 0.9647, 0.9647, 0.9608]],\n",
      "\n",
      "         [[0.8980, 0.8980, 0.9020,  ..., 0.9255, 0.9255, 0.9255],\n",
      "          [0.8980, 0.8980, 0.9020,  ..., 0.9255, 0.9255, 0.9255],\n",
      "          [0.8980, 0.8980, 0.9020,  ..., 0.9255, 0.9255, 0.9255],\n",
      "          ...,\n",
      "          [0.8941, 0.8941, 0.8941,  ..., 0.9647, 0.9608, 0.9608],\n",
      "          [0.8980, 0.8980, 0.8980,  ..., 0.9647, 0.9608, 0.9608],\n",
      "          [0.8980, 0.8980, 0.9020,  ..., 0.9608, 0.9608, 0.9569]],\n",
      "\n",
      "         [[0.8902, 0.8902, 0.8941,  ..., 0.9176, 0.9176, 0.9176],\n",
      "          [0.8902, 0.8902, 0.8941,  ..., 0.9176, 0.9176, 0.9176],\n",
      "          [0.8902, 0.8902, 0.8941,  ..., 0.9176, 0.9176, 0.9176],\n",
      "          ...,\n",
      "          [0.8941, 0.8941, 0.8941,  ..., 0.9569, 0.9529, 0.9529],\n",
      "          [0.9020, 0.9020, 0.9020,  ..., 0.9569, 0.9529, 0.9529],\n",
      "          [0.9020, 0.9020, 0.9059,  ..., 0.9529, 0.9529, 0.9490]]],\n",
      "\n",
      "\n",
      "        [[[0.1765, 0.1686, 0.1647,  ..., 0.2392, 0.2314, 0.2196],\n",
      "          [0.1725, 0.1647, 0.1647,  ..., 0.2471, 0.2314, 0.2235],\n",
      "          [0.1725, 0.1647, 0.1647,  ..., 0.2667, 0.2431, 0.2275],\n",
      "          ...,\n",
      "          [0.0471, 0.0471, 0.0471,  ..., 0.2196, 0.1647, 0.2078],\n",
      "          [0.0471, 0.0431, 0.0431,  ..., 0.2471, 0.1843, 0.1882],\n",
      "          [0.0392, 0.0392, 0.0392,  ..., 0.2157, 0.1373, 0.1216]],\n",
      "\n",
      "         [[0.1725, 0.1686, 0.1647,  ..., 0.2353, 0.2196, 0.2118],\n",
      "          [0.1725, 0.1647, 0.1647,  ..., 0.2431, 0.2235, 0.2157],\n",
      "          [0.1725, 0.1647, 0.1647,  ..., 0.2745, 0.2431, 0.2235],\n",
      "          ...,\n",
      "          [0.0471, 0.0471, 0.0471,  ..., 0.2157, 0.1843, 0.2000],\n",
      "          [0.0471, 0.0431, 0.0431,  ..., 0.2235, 0.1725, 0.1765],\n",
      "          [0.0392, 0.0392, 0.0392,  ..., 0.1961, 0.1333, 0.1098]],\n",
      "\n",
      "         [[0.1725, 0.1686, 0.1647,  ..., 0.2588, 0.2471, 0.2353],\n",
      "          [0.1725, 0.1647, 0.1647,  ..., 0.2706, 0.2471, 0.2392],\n",
      "          [0.1725, 0.1647, 0.1647,  ..., 0.3059, 0.2667, 0.2471],\n",
      "          ...,\n",
      "          [0.0471, 0.0471, 0.0471,  ..., 0.0824, 0.0784, 0.0667],\n",
      "          [0.0471, 0.0431, 0.0431,  ..., 0.0784, 0.0784, 0.0745],\n",
      "          [0.0392, 0.0392, 0.0392,  ..., 0.0784, 0.0706, 0.0706]]]]), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "# to test dataloader\n",
    "it = iter(test_dataloader)\n",
    "print(next(it))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extract = False # so we update the whole model \n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract) # This line is for finetuning vs feature extraction\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    input_size = 64\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4780, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(onehots_w2i)\n",
    "model, input_size = initialize_model(num_classes, feature_extract)\n",
    "model.fc.weight.shape\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, f1_score, accuracy_score, label_ranking_average_precision_score, average_precision_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Without label weights\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# With label weights\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight=LABEL_WEIGHTS)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "sgdr_partial = lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=0.005)\n",
    "plateau = lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, scheduler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    running_corrects = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    running_auc = 0.0\n",
    "    \n",
    "    running_corrects_ = 0.0\n",
    "    running_precision_ = 0.0\n",
    "    running_accuracy_ = 0.0\n",
    "    running_auc_ = 0.0\n",
    "    \n",
    "    result = []\n",
    "    result_ = []\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (image, target) in enumerate(loader):   \n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target) # Averaging losses from all vector components. \n",
    "\n",
    "        # We use sigmoid for prediction. Sigmoid is applied to each individual class in the 2700 classes. \n",
    "        # Therefore, if it's more than 0.5, we predict it as True. \n",
    "        preds = torch.sigmoid(output).data > 0.5\n",
    "        preds = preds.to(torch.float32)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        running_loss += loss.item() * image.shape[0]\n",
    "        \n",
    "        # Metrics \n",
    "        SAMPLE_WEIGHT = compute_sample_weight('balanced', target.to(\"cpu\"))\n",
    "        \n",
    "        f1 = f1_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\", sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "        precision = precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\",sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "        accuracy = accuracy_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(),sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "        auc = roc_auc_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\", sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "        \n",
    "        f1_ = f1_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\")*image.size(0)\n",
    "        precision_ = precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\")*image.size(0)\n",
    "        accuracy_ = accuracy_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy())*image.size(0)\n",
    "        auc_ = roc_auc_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\")*image.size(0)\n",
    "        \n",
    "        running_corrects += f1\n",
    "        running_precision += precision\n",
    "        running_accuracy += accuracy\n",
    "        running_auc += auc\n",
    "        \n",
    "        running_corrects_ += f1_\n",
    "        running_precision_ += precision_\n",
    "        running_accuracy_ += accuracy_\n",
    "        running_auc_ += auc_\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    \n",
    "    epoch_f1 = running_corrects / len(loader.dataset)\n",
    "    epoch_precision = running_precision / len(loader.dataset)\n",
    "    epoch_acc = running_accuracy / len(loader.dataset)\n",
    "    epoch_auc = running_auc / len(loader.dataset)\n",
    "    \n",
    "    epoch_f1_ = running_corrects_ / len(loader.dataset)\n",
    "    epoch_precision_ = running_precision_ / len(loader.dataset)\n",
    "    epoch_acc_ = running_accuracy_ / len(loader.dataset)\n",
    "    epoch_auc_ = running_auc_ / len(loader.dataset)\n",
    "    \n",
    "    result.append('WEIGHTED Training Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f}'.format(epoch_loss, epoch_f1, epoch_acc, epoch_precision, epoch_auc))\n",
    "    print(result)\n",
    "    result_.append('UNWEIGHTED Training Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f}'.format(epoch_loss, epoch_f1_, epoch_acc_, epoch_precision_, epoch_auc_))\n",
    "    print(result_)\n",
    "    \n",
    "    return epoch_loss, epoch_f1, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    running_auc = 0.0\n",
    "    \n",
    "    running_corrects_ = 0.0\n",
    "    running_precision_ = 0.0\n",
    "    running_accuracy_ = 0.0\n",
    "    running_auc_ = 0.0\n",
    "    \n",
    "    result = []\n",
    "    result_ = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, target) in enumerate(loader):   \n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            preds = torch.sigmoid(output).data > 0.5\n",
    "            preds = preds.to(torch.float32)\n",
    "\n",
    "            running_loss += loss.item() * image.shape[0]\n",
    "            \n",
    "            # Metrics \n",
    "            SAMPLE_WEIGHT = compute_sample_weight('balanced', target.to(\"cpu\"))\n",
    "            \n",
    "            f1 = f1_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\", sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "            precision = precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\",sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "            accuracy = accuracy_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(),sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "            auc = roc_auc_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\", sample_weight=SAMPLE_WEIGHT)*image.size(0)\n",
    "\n",
    "            f1_ = f1_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\")*image.size(0)\n",
    "            precision_ = precision_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\")*image.size(0)\n",
    "            accuracy_ = accuracy_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy())*image.size(0)\n",
    "            auc_ = roc_auc_score(target.to(\"cpu\").to(torch.int).numpy(), preds.to(\"cpu\").to(torch.int).numpy(), average=\"samples\")*image.size(0)\n",
    "             \n",
    "            running_corrects += f1\n",
    "            running_precision += precision\n",
    "            running_accuracy += accuracy\n",
    "            running_auc += auc\n",
    "            \n",
    "            running_corrects_ += f1_\n",
    "            running_precision_ += precision_\n",
    "            running_accuracy_ += accuracy_\n",
    "            running_auc_ += auc_\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_f1 = running_corrects / len(loader.dataset)\n",
    "    epoch_precision = running_precision / len(loader.dataset)\n",
    "    epoch_acc = running_accuracy / len(loader.dataset)\n",
    "    epoch_auc = running_auc / len(loader.dataset)\n",
    "    \n",
    "    epoch_f1_ = running_corrects_ / len(loader.dataset)\n",
    "    epoch_precision_ = running_precision_ / len(loader.dataset)\n",
    "    epoch_acc_ = running_accuracy_ / len(loader.dataset)\n",
    "    epoch_auc_ = running_auc_ / len(loader.dataset)\n",
    "    \n",
    "\n",
    "    result.append('WEIGHTED Testing Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f}'.format(epoch_loss, epoch_f1, epoch_acc, epoch_precision, epoch_auc))\n",
    "    print(result)\n",
    "    result_.append('UNWEIGHTED Testing Loss: {:.4f} F1: {:.4f} Acc: {:.4f} Prec: {:.4f} AUC: {:.4f}'.format(epoch_loss, epoch_f1_, epoch_acc_, epoch_precision_, epoch_auc_))\n",
    "    print(result_)\n",
    "    \n",
    "    return epoch_loss, epoch_f1, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13225, 3307)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader.dataset), len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCheckpoint(filename, batch_size, epoch):\n",
    "    checkpoint = {\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              \"batch_size\":batch_size,\n",
    "    } # save all important stuff\n",
    "    torch.save(checkpoint , filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './saved_models/multilabel_adam0.001_resnet_pretrained_3labels/' created\n",
      "-----Training epoch 0/49 --------\n",
      "['WEIGHTED Training Loss: 0.0116 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Training Loss: 0.0116 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5001']\n",
      "train epoch: 0, loss: 0.011577212941640942\n",
      "------Testing epoch 0/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Testing Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "test epoch: 0, loss: 0.005038321095524118\n",
      "\n",
      "-----Training epoch 1/49 --------\n",
      "['WEIGHTED Training Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Training Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "train epoch: 1, loss: 0.00499233262940875\n",
      "------Testing epoch 1/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Testing Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "test epoch: 1, loss: 0.005001877786719506\n",
      "\n",
      "-----Training epoch 2/49 --------\n",
      "['WEIGHTED Training Loss: 0.0049 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Training Loss: 0.0049 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "train epoch: 2, loss: 0.00492884799732434\n",
      "------Testing epoch 2/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Testing Loss: 0.0050 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "test epoch: 2, loss: 0.0049748741448520355\n",
      "\n",
      "-----Training epoch 3/49 --------\n",
      "['WEIGHTED Training Loss: 0.0048 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Training Loss: 0.0048 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "train epoch: 3, loss: 0.004830137106056531\n",
      "------Testing epoch 3/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0049 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Testing Loss: 0.0049 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "test epoch: 3, loss: 0.004879006736475038\n",
      "\n",
      "-----Training epoch 4/49 --------\n",
      "['WEIGHTED Training Loss: 0.0047 F1: 0.0002 Acc: 0.0001 Prec: 0.0001 AUC: 0.5001']\n",
      "['UNWEIGHTED Training Loss: 0.0047 F1: 0.0001 Acc: 0.0001 Prec: 0.0001 AUC: 0.5001']\n",
      "train epoch: 4, loss: 0.004655234582743313\n",
      "------Testing epoch 4/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0048 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "['UNWEIGHTED Testing Loss: 0.0048 F1: 0.0000 Acc: 0.0000 Prec: 0.0000 AUC: 0.5000']\n",
      "test epoch: 4, loss: 0.004778365704695921\n",
      "\n",
      "-----Training epoch 5/49 --------\n",
      "['WEIGHTED Training Loss: 0.0044 F1: 0.0016 Acc: 0.0009 Prec: 0.0019 AUC: 0.5007']\n",
      "['UNWEIGHTED Training Loss: 0.0044 F1: 0.0016 Acc: 0.0008 Prec: 0.0020 AUC: 0.5007']\n",
      "train epoch: 5, loss: 0.004387485088960097\n",
      "------Testing epoch 5/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0047 F1: 0.0006 Acc: 0.0000 Prec: 0.0011 AUC: 0.5002']\n",
      "['UNWEIGHTED Testing Loss: 0.0047 F1: 0.0005 Acc: 0.0000 Prec: 0.0009 AUC: 0.5002']\n",
      "test epoch: 5, loss: 0.004735359717875782\n",
      "\n",
      "-----Training epoch 6/49 --------\n",
      "['WEIGHTED Training Loss: 0.0041 F1: 0.0066 Acc: 0.0027 Prec: 0.0087 AUC: 0.5028']\n",
      "['UNWEIGHTED Training Loss: 0.0041 F1: 0.0066 Acc: 0.0026 Prec: 0.0089 AUC: 0.5028']\n",
      "train epoch: 6, loss: 0.004057080275933689\n",
      "------Testing epoch 6/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0046 F1: 0.0038 Acc: 0.0017 Prec: 0.0047 AUC: 0.5017']\n",
      "['UNWEIGHTED Testing Loss: 0.0046 F1: 0.0039 Acc: 0.0015 Prec: 0.0051 AUC: 0.5017']\n",
      "test epoch: 6, loss: 0.004597811558936383\n",
      "\n",
      "-----Training epoch 7/49 --------\n",
      "['WEIGHTED Training Loss: 0.0037 F1: 0.0196 Acc: 0.0085 Prec: 0.0256 AUC: 0.5086']\n",
      "['UNWEIGHTED Training Loss: 0.0037 F1: 0.0192 Acc: 0.0079 Prec: 0.0255 AUC: 0.5084']\n",
      "train epoch: 7, loss: 0.003697167463523172\n",
      "------Testing epoch 7/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0047 F1: 0.0085 Acc: 0.0044 Prec: 0.0109 AUC: 0.5038']\n",
      "['UNWEIGHTED Testing Loss: 0.0047 F1: 0.0083 Acc: 0.0042 Prec: 0.0108 AUC: 0.5037']\n",
      "test epoch: 7, loss: 0.004676396514835145\n",
      "\n",
      "-----Training epoch 8/49 --------\n",
      "['WEIGHTED Training Loss: 0.0033 F1: 0.0495 Acc: 0.0237 Prec: 0.0629 AUC: 0.5221']\n",
      "['UNWEIGHTED Training Loss: 0.0033 F1: 0.0498 Acc: 0.0234 Prec: 0.0638 AUC: 0.5221']\n",
      "train epoch: 8, loss: 0.0032957827991723226\n",
      "------Testing epoch 8/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0046 F1: 0.0203 Acc: 0.0084 Prec: 0.0263 AUC: 0.5090']\n",
      "['UNWEIGHTED Testing Loss: 0.0046 F1: 0.0193 Acc: 0.0079 Prec: 0.0251 AUC: 0.5086']\n",
      "test epoch: 8, loss: 0.004606071911416531\n",
      "\n",
      "-----Training epoch 9/49 --------\n",
      "['WEIGHTED Training Loss: 0.0029 F1: 0.1023 Acc: 0.0487 Prec: 0.1285 AUC: 0.5460']\n",
      "['UNWEIGHTED Training Loss: 0.0029 F1: 0.1005 Acc: 0.0466 Prec: 0.1268 AUC: 0.5450']\n",
      "train epoch: 9, loss: 0.002865306545253174\n",
      "------Testing epoch 9/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0055 F1: 0.0144 Acc: 0.0069 Prec: 0.0178 AUC: 0.5065']\n",
      "['UNWEIGHTED Testing Loss: 0.0055 F1: 0.0134 Acc: 0.0064 Prec: 0.0166 AUC: 0.5061']\n",
      "test epoch: 9, loss: 0.005541654663441501\n",
      "\n",
      "-----Training epoch 10/49 --------\n",
      "['WEIGHTED Training Loss: 0.0024 F1: 0.1709 Acc: 0.0831 Prec: 0.2114 AUC: 0.5778']\n",
      "['UNWEIGHTED Training Loss: 0.0024 F1: 0.1693 Acc: 0.0801 Prec: 0.2108 AUC: 0.5768']\n",
      "train epoch: 10, loss: 0.002443066095468537\n",
      "------Testing epoch 10/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0047 F1: 0.0344 Acc: 0.0160 Prec: 0.0422 AUC: 0.5157']\n",
      "['UNWEIGHTED Testing Loss: 0.0047 F1: 0.0342 Acc: 0.0154 Prec: 0.0424 AUC: 0.5155']\n",
      "test epoch: 10, loss: 0.004725074231070372\n",
      "\n",
      "-----Training epoch 11/49 --------\n",
      "['WEIGHTED Training Loss: 0.0020 F1: 0.2790 Acc: 0.1456 Prec: 0.3339 AUC: 0.6294']\n",
      "['UNWEIGHTED Training Loss: 0.0020 F1: 0.2746 Acc: 0.1403 Prec: 0.3313 AUC: 0.6268']\n",
      "train epoch: 11, loss: 0.002000611342528451\n",
      "------Testing epoch 11/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0049 F1: 0.0523 Acc: 0.0201 Prec: 0.0639 AUC: 0.5240']\n",
      "['UNWEIGHTED Testing Loss: 0.0049 F1: 0.0525 Acc: 0.0200 Prec: 0.0649 AUC: 0.5240']\n",
      "test epoch: 11, loss: 0.004872780193954634\n",
      "\n",
      "-----Training epoch 12/49 --------\n",
      "['WEIGHTED Training Loss: 0.0016 F1: 0.3987 Acc: 0.2210 Prec: 0.4639 AUC: 0.6876']\n",
      "['UNWEIGHTED Training Loss: 0.0016 F1: 0.3944 Acc: 0.2149 Prec: 0.4618 AUC: 0.6850']\n",
      "train epoch: 12, loss: 0.00156456330056973\n",
      "------Testing epoch 12/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0051 F1: 0.0533 Acc: 0.0231 Prec: 0.0640 AUC: 0.5248']\n",
      "['UNWEIGHTED Testing Loss: 0.0051 F1: 0.0530 Acc: 0.0221 Prec: 0.0644 AUC: 0.5245']\n",
      "test epoch: 12, loss: 0.005092557114139222\n",
      "\n",
      "-----Training epoch 13/49 --------\n",
      "['WEIGHTED Training Loss: 0.0012 F1: 0.5266 Acc: 0.3071 Prec: 0.6010 AUC: 0.7504']\n",
      "['UNWEIGHTED Training Loss: 0.0012 F1: 0.5205 Acc: 0.2991 Prec: 0.5975 AUC: 0.7467']\n",
      "train epoch: 13, loss: 0.0011986583621295729\n",
      "------Testing epoch 13/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0053 F1: 0.0519 Acc: 0.0191 Prec: 0.0605 AUC: 0.5250']\n",
      "['UNWEIGHTED Testing Loss: 0.0053 F1: 0.0518 Acc: 0.0178 Prec: 0.0614 AUC: 0.5247']\n",
      "test epoch: 13, loss: 0.005319327156143649\n",
      "\n",
      "-----Training epoch 14/49 --------\n",
      "['WEIGHTED Training Loss: 0.0009 F1: 0.6568 Acc: 0.4241 Prec: 0.7294 AUC: 0.8156']\n",
      "['UNWEIGHTED Training Loss: 0.0009 F1: 0.6524 Acc: 0.4156 Prec: 0.7288 AUC: 0.8125']\n",
      "train epoch: 14, loss: 0.0008734662203903404\n",
      "------Testing epoch 14/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0055 F1: 0.0764 Acc: 0.0337 Prec: 0.0903 AUC: 0.5363']\n",
      "['UNWEIGHTED Testing Loss: 0.0055 F1: 0.0751 Acc: 0.0327 Prec: 0.0891 AUC: 0.5356']\n",
      "test epoch: 14, loss: 0.005458536532225658\n",
      "\n",
      "-----Training epoch 15/49 --------\n",
      "['WEIGHTED Training Loss: 0.0006 F1: 0.7797 Acc: 0.5514 Prec: 0.8425 AUC: 0.8791']\n",
      "['UNWEIGHTED Training Loss: 0.0006 F1: 0.7747 Acc: 0.5406 Prec: 0.8408 AUC: 0.8757']\n",
      "train epoch: 15, loss: 0.0005896431869705734\n",
      "------Testing epoch 15/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0059 F1: 0.0527 Acc: 0.0224 Prec: 0.0653 AUC: 0.5241']\n",
      "['UNWEIGHTED Testing Loss: 0.0059 F1: 0.0519 Acc: 0.0209 Prec: 0.0651 AUC: 0.5235']\n",
      "test epoch: 15, loss: 0.00588294302964133\n",
      "\n",
      "-----Training epoch 16/49 --------\n",
      "['WEIGHTED Training Loss: 0.0004 F1: 0.8522 Acc: 0.6561 Prec: 0.9001 AUC: 0.9183']\n",
      "['UNWEIGHTED Training Loss: 0.0004 F1: 0.8490 Acc: 0.6479 Prec: 0.8999 AUC: 0.9161']\n",
      "train epoch: 16, loss: 0.00042526923028914174\n",
      "------Testing epoch 16/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0057 F1: 0.0923 Acc: 0.0464 Prec: 0.1088 AUC: 0.5435']\n",
      "['UNWEIGHTED Testing Loss: 0.0057 F1: 0.0919 Acc: 0.0457 Prec: 0.1089 AUC: 0.5432']\n",
      "test epoch: 16, loss: 0.005659731832116905\n",
      "\n",
      "-----Training epoch 17/49 --------\n",
      "['WEIGHTED Training Loss: 0.0003 F1: 0.9050 Acc: 0.7420 Prec: 0.9394 AUC: 0.9470']\n",
      "['UNWEIGHTED Training Loss: 0.0003 F1: 0.9025 Acc: 0.7347 Prec: 0.9390 AUC: 0.9452']\n",
      "train epoch: 17, loss: 0.0002997771829290981\n",
      "------Testing epoch 17/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0062 F1: 0.0716 Acc: 0.0334 Prec: 0.0858 AUC: 0.5335']\n",
      "['UNWEIGHTED Testing Loss: 0.0062 F1: 0.0722 Acc: 0.0327 Prec: 0.0877 AUC: 0.5335']\n",
      "test epoch: 17, loss: 0.006184061101939397\n",
      "\n",
      "-----Training epoch 18/49 --------\n",
      "['WEIGHTED Training Loss: 0.0002 F1: 0.9275 Acc: 0.7938 Prec: 0.9543 AUC: 0.9597']\n",
      "['UNWEIGHTED Training Loss: 0.0002 F1: 0.9259 Acc: 0.7881 Prec: 0.9547 AUC: 0.9584']\n",
      "train epoch: 18, loss: 0.00023818212308672685\n",
      "------Testing epoch 18/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0063 F1: 0.0890 Acc: 0.0514 Prec: 0.1050 AUC: 0.5414']\n",
      "['UNWEIGHTED Testing Loss: 0.0063 F1: 0.0879 Acc: 0.0499 Prec: 0.1038 AUC: 0.5409']\n",
      "test epoch: 18, loss: 0.006259513015475542\n",
      "\n",
      "-----Training epoch 19/49 --------\n",
      "['WEIGHTED Training Loss: 0.0003 F1: 0.9185 Acc: 0.7740 Prec: 0.9461 AUC: 0.9557']\n",
      "['UNWEIGHTED Training Loss: 0.0003 F1: 0.9174 Acc: 0.7688 Prec: 0.9470 AUC: 0.9546']\n",
      "train epoch: 19, loss: 0.0002578673357938879\n",
      "------Testing epoch 19/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0071 F1: 0.0479 Acc: 0.0188 Prec: 0.0592 AUC: 0.5219']\n",
      "['UNWEIGHTED Testing Loss: 0.0071 F1: 0.0475 Acc: 0.0181 Prec: 0.0594 AUC: 0.5216']\n",
      "test epoch: 19, loss: 0.007061537170299986\n",
      "\n",
      "-----Training epoch 20/49 --------\n",
      "['WEIGHTED Training Loss: 0.0004 F1: 0.8517 Acc: 0.6374 Prec: 0.8905 AUC: 0.9220']\n",
      "['UNWEIGHTED Training Loss: 0.0004 F1: 0.8501 Acc: 0.6318 Prec: 0.8914 AUC: 0.9204']\n",
      "train epoch: 20, loss: 0.00043290514189095035\n",
      "------Testing epoch 20/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0067 F1: 0.0712 Acc: 0.0302 Prec: 0.0812 AUC: 0.5345']\n",
      "['UNWEIGHTED Testing Loss: 0.0067 F1: 0.0724 Acc: 0.0299 Prec: 0.0828 AUC: 0.5351']\n",
      "test epoch: 20, loss: 0.006677925910283638\n",
      "\n",
      "-----Training epoch 21/49 --------\n",
      "['WEIGHTED Training Loss: 0.0008 F1: 0.7315 Acc: 0.4603 Prec: 0.7853 AUC: 0.8619']\n",
      "['UNWEIGHTED Training Loss: 0.0008 F1: 0.7285 Acc: 0.4545 Prec: 0.7851 AUC: 0.8596']\n",
      "train epoch: 21, loss: 0.0007626720025320317\n",
      "------Testing epoch 21/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0070 F1: 0.0457 Acc: 0.0164 Prec: 0.0548 AUC: 0.5213']\n",
      "['UNWEIGHTED Testing Loss: 0.0070 F1: 0.0473 Acc: 0.0166 Prec: 0.0570 AUC: 0.5220']\n",
      "test epoch: 21, loss: 0.0070138025436339724\n",
      "\n",
      "-----Training epoch 22/49 --------\n",
      "['WEIGHTED Training Loss: 0.0004 F1: 0.8507 Acc: 0.6494 Prec: 0.8903 AUC: 0.9214']\n",
      "['UNWEIGHTED Training Loss: 0.0004 F1: 0.8494 Acc: 0.6443 Prec: 0.8909 AUC: 0.9202']\n",
      "train epoch: 22, loss: 0.0004264155972796529\n",
      "------Testing epoch 22/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0063 F1: 0.0911 Acc: 0.0534 Prec: 0.1026 AUC: 0.5439']\n",
      "['UNWEIGHTED Testing Loss: 0.0063 F1: 0.0897 Acc: 0.0517 Prec: 0.1021 AUC: 0.5429']\n",
      "test epoch: 22, loss: 0.006285298188896537\n",
      "\n",
      "-----Training epoch 23/49 --------\n",
      "['WEIGHTED Training Loss: 0.0002 F1: 0.9531 Acc: 0.8659 Prec: 0.9679 AUC: 0.9749']\n",
      "['UNWEIGHTED Training Loss: 0.0002 F1: 0.9527 Acc: 0.8625 Prec: 0.9684 AUC: 0.9744']\n",
      "train epoch: 23, loss: 0.00016149128129439947\n",
      "------Testing epoch 23/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0063 F1: 0.1034 Acc: 0.0629 Prec: 0.1197 AUC: 0.5486']\n",
      "['UNWEIGHTED Testing Loss: 0.0063 F1: 0.1023 Acc: 0.0617 Prec: 0.1187 AUC: 0.5480']\n",
      "test epoch: 23, loss: 0.0062766546298450385\n",
      "\n",
      "-----Training epoch 24/49 --------\n",
      "['WEIGHTED Training Loss: 0.0001 F1: 0.9839 Acc: 0.9528 Prec: 0.9886 AUC: 0.9915']\n",
      "['UNWEIGHTED Training Loss: 0.0001 F1: 0.9839 Acc: 0.9521 Prec: 0.9889 AUC: 0.9914']\n",
      "train epoch: 24, loss: 6.884939330117249e-05\n",
      "------Testing epoch 24/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0065 F1: 0.1030 Acc: 0.0703 Prec: 0.1185 AUC: 0.5485']\n",
      "['UNWEIGHTED Testing Loss: 0.0065 F1: 0.1008 Acc: 0.0677 Prec: 0.1167 AUC: 0.5472']\n",
      "test epoch: 24, loss: 0.0065260745305032895\n",
      "\n",
      "-----Training epoch 25/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9938 Acc: 0.9851 Prec: 0.9949 AUC: 0.9968']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9939 Acc: 0.9850 Prec: 0.9950 AUC: 0.9968']\n",
      "train epoch: 25, loss: 3.675435276859724e-05\n",
      "------Testing epoch 25/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0066 F1: 0.1061 Acc: 0.0747 Prec: 0.1214 AUC: 0.5499']\n",
      "['UNWEIGHTED Testing Loss: 0.0066 F1: 0.1046 Acc: 0.0723 Prec: 0.1202 AUC: 0.5491']\n",
      "test epoch: 25, loss: 0.006577299211071739\n",
      "\n",
      "-----Training epoch 26/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9943 Acc: 0.9870 Prec: 0.9952 AUC: 0.9971']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9942 Acc: 0.9866 Prec: 0.9952 AUC: 0.9971']\n",
      "train epoch: 26, loss: 3.052910153235468e-05\n",
      "------Testing epoch 26/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0067 F1: 0.1033 Acc: 0.0698 Prec: 0.1179 AUC: 0.5487']\n",
      "['UNWEIGHTED Testing Loss: 0.0067 F1: 0.1020 Acc: 0.0674 Prec: 0.1178 AUC: 0.5479']\n",
      "test epoch: 26, loss: 0.006691519305473833\n",
      "\n",
      "-----Training epoch 27/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9950 Acc: 0.9891 Prec: 0.9960 AUC: 0.9975']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9952 Acc: 0.9893 Prec: 0.9962 AUC: 0.9976']\n",
      "train epoch: 27, loss: 2.615259134740787e-05\n",
      "------Testing epoch 27/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0070 F1: 0.0935 Acc: 0.0664 Prec: 0.1081 AUC: 0.5437']\n",
      "['UNWEIGHTED Testing Loss: 0.0070 F1: 0.0926 Acc: 0.0650 Prec: 0.1072 AUC: 0.5432']\n",
      "test epoch: 27, loss: 0.0070089530558833674\n",
      "\n",
      "-----Training epoch 28/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9947 Acc: 0.9878 Prec: 0.9954 AUC: 0.9974']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9944 Acc: 0.9872 Prec: 0.9953 AUC: 0.9973']\n",
      "train epoch: 28, loss: 2.806692902550686e-05\n",
      "------Testing epoch 28/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0070 F1: 0.1041 Acc: 0.0723 Prec: 0.1200 AUC: 0.5488']\n",
      "['UNWEIGHTED Testing Loss: 0.0070 F1: 0.1031 Acc: 0.0705 Prec: 0.1192 AUC: 0.5482']\n",
      "test epoch: 28, loss: 0.006988706345728221\n",
      "\n",
      "-----Training epoch 29/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9935 Acc: 0.9820 Prec: 0.9944 AUC: 0.9968']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9932 Acc: 0.9811 Prec: 0.9943 AUC: 0.9967']\n",
      "train epoch: 29, loss: 3.320953207337412e-05\n",
      "------Testing epoch 29/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0073 F1: 0.0876 Acc: 0.0603 Prec: 0.0999 AUC: 0.5414']\n",
      "['UNWEIGHTED Testing Loss: 0.0073 F1: 0.0871 Acc: 0.0581 Prec: 0.0999 AUC: 0.5410']\n",
      "test epoch: 29, loss: 0.007341349673732588\n",
      "\n",
      "-----Training epoch 30/49 --------\n",
      "['WEIGHTED Training Loss: 0.0002 F1: 0.9564 Acc: 0.8829 Prec: 0.9666 AUC: 0.9779']\n",
      "['UNWEIGHTED Training Loss: 0.0002 F1: 0.9554 Acc: 0.8795 Prec: 0.9664 AUC: 0.9772']\n",
      "train epoch: 30, loss: 0.0001515268185131942\n",
      "------Testing epoch 30/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0104 F1: 0.0354 Acc: 0.0121 Prec: 0.0444 AUC: 0.5165']\n",
      "['UNWEIGHTED Testing Loss: 0.0104 F1: 0.0345 Acc: 0.0109 Prec: 0.0435 AUC: 0.5160']\n",
      "test epoch: 30, loss: 0.0104430043613096\n",
      "\n",
      "-----Training epoch 31/49 --------\n",
      "['WEIGHTED Training Loss: 0.0016 F1: 0.4966 Acc: 0.2343 Prec: 0.5534 AUC: 0.7450']\n",
      "['UNWEIGHTED Training Loss: 0.0016 F1: 0.4947 Acc: 0.2293 Prec: 0.5538 AUC: 0.7433']\n",
      "train epoch: 31, loss: 0.0015996118991359336\n",
      "------Testing epoch 31/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0061 F1: 0.0597 Acc: 0.0249 Prec: 0.0693 AUC: 0.5287']\n",
      "['UNWEIGHTED Testing Loss: 0.0061 F1: 0.0582 Acc: 0.0233 Prec: 0.0680 AUC: 0.5279']\n",
      "test epoch: 31, loss: 0.006126663504528304\n",
      "\n",
      "-----Training epoch 32/49 --------\n",
      "['WEIGHTED Training Loss: 0.0003 F1: 0.8995 Acc: 0.7575 Prec: 0.9307 AUC: 0.9459']\n",
      "['UNWEIGHTED Training Loss: 0.0003 F1: 0.8984 Acc: 0.7526 Prec: 0.9311 AUC: 0.9449']\n",
      "train epoch: 32, loss: 0.0002966683766662032\n",
      "------Testing epoch 32/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0062 F1: 0.0847 Acc: 0.0496 Prec: 0.0996 AUC: 0.5396']\n",
      "['UNWEIGHTED Testing Loss: 0.0062 F1: 0.0849 Acc: 0.0484 Prec: 0.1010 AUC: 0.5395']\n",
      "test epoch: 32, loss: 0.0061843950252969855\n",
      "\n",
      "-----Training epoch 33/49 --------\n",
      "['WEIGHTED Training Loss: 0.0001 F1: 0.9829 Acc: 0.9509 Prec: 0.9880 AUC: 0.9909']\n",
      "['UNWEIGHTED Training Loss: 0.0001 F1: 0.9825 Acc: 0.9486 Prec: 0.9883 AUC: 0.9904']\n",
      "train epoch: 33, loss: 7.63649889234971e-05\n",
      "------Testing epoch 33/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0064 F1: 0.0955 Acc: 0.0617 Prec: 0.1118 AUC: 0.5445']\n",
      "['UNWEIGHTED Testing Loss: 0.0064 F1: 0.0951 Acc: 0.0599 Prec: 0.1125 AUC: 0.5441']\n",
      "test epoch: 33, loss: 0.006373187524103482\n",
      "\n",
      "-----Training epoch 34/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9947 Acc: 0.9857 Prec: 0.9961 AUC: 0.9972']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9946 Acc: 0.9854 Prec: 0.9960 AUC: 0.9971']\n",
      "train epoch: 34, loss: 3.0990146992726794e-05\n",
      "------Testing epoch 34/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0065 F1: 0.1045 Acc: 0.0713 Prec: 0.1199 AUC: 0.5489']\n",
      "['UNWEIGHTED Testing Loss: 0.0065 F1: 0.1043 Acc: 0.0702 Prec: 0.1199 AUC: 0.5488']\n",
      "test epoch: 34, loss: 0.0064895224512763385\n",
      "\n",
      "-----Training epoch 35/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9968 Acc: 0.9948 Prec: 0.9968 AUC: 0.9985']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9968 Acc: 0.9946 Prec: 0.9969 AUC: 0.9984']\n",
      "train epoch: 35, loss: 1.915098856984935e-05\n",
      "------Testing epoch 35/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0067 F1: 0.1071 Acc: 0.0769 Prec: 0.1227 AUC: 0.5503']\n",
      "['UNWEIGHTED Testing Loss: 0.0067 F1: 0.1063 Acc: 0.0750 Prec: 0.1225 AUC: 0.5498']\n",
      "test epoch: 35, loss: 0.006704610256500157\n",
      "\n",
      "-----Training epoch 36/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9972 Acc: 0.9958 Prec: 0.9973 AUC: 0.9987']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9971 Acc: 0.9957 Prec: 0.9972 AUC: 0.9986']\n",
      "train epoch: 36, loss: 1.4602308331448267e-05\n",
      "------Testing epoch 36/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0068 F1: 0.1057 Acc: 0.0783 Prec: 0.1202 AUC: 0.5498']\n",
      "['UNWEIGHTED Testing Loss: 0.0068 F1: 0.1051 Acc: 0.0771 Prec: 0.1201 AUC: 0.5495']\n",
      "test epoch: 36, loss: 0.006816736340527272\n",
      "\n",
      "-----Training epoch 37/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9969 Acc: 0.9959 Prec: 0.9970 AUC: 0.9985']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9970 Acc: 0.9960 Prec: 0.9971 AUC: 0.9985']\n",
      "train epoch: 37, loss: 1.2889806145321514e-05\n",
      "------Testing epoch 37/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0069 F1: 0.1049 Acc: 0.0768 Prec: 0.1196 AUC: 0.5494']\n",
      "['UNWEIGHTED Testing Loss: 0.0069 F1: 0.1038 Acc: 0.0756 Prec: 0.1190 AUC: 0.5488']\n",
      "test epoch: 37, loss: 0.006936406013213654\n",
      "\n",
      "-----Training epoch 38/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9971 Acc: 0.9956 Prec: 0.9973 AUC: 0.9986']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9970 Acc: 0.9956 Prec: 0.9972 AUC: 0.9985']\n",
      "train epoch: 38, loss: 1.1506264030125515e-05\n",
      "------Testing epoch 38/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0071 F1: 0.1002 Acc: 0.0735 Prec: 0.1129 AUC: 0.5473']\n",
      "['UNWEIGHTED Testing Loss: 0.0071 F1: 0.0998 Acc: 0.0720 Prec: 0.1135 AUC: 0.5469']\n",
      "test epoch: 38, loss: 0.007145372681665545\n",
      "\n",
      "-----Training epoch 39/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9971 Acc: 0.9959 Prec: 0.9971 AUC: 0.9986']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9970 Acc: 0.9958 Prec: 0.9972 AUC: 0.9985']\n",
      "train epoch: 39, loss: 1.1187477213584084e-05\n",
      "------Testing epoch 39/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0072 F1: 0.1034 Acc: 0.0758 Prec: 0.1177 AUC: 0.5487']\n",
      "['UNWEIGHTED Testing Loss: 0.0072 F1: 0.1032 Acc: 0.0747 Prec: 0.1183 AUC: 0.5484']\n",
      "test epoch: 39, loss: 0.007155623484238428\n",
      "\n",
      "-----Training epoch 40/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9974 Acc: 0.9964 Prec: 0.9975 AUC: 0.9987']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9974 Acc: 0.9964 Prec: 0.9975 AUC: 0.9987']\n",
      "train epoch: 40, loss: 1.0012432323426917e-05\n",
      "------Testing epoch 40/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0073 F1: 0.1041 Acc: 0.0774 Prec: 0.1168 AUC: 0.5493']\n",
      "['UNWEIGHTED Testing Loss: 0.0073 F1: 0.1028 Acc: 0.0756 Prec: 0.1161 AUC: 0.5485']\n",
      "test epoch: 40, loss: 0.00726427294039045\n",
      "\n",
      "-----Training epoch 41/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9976 Acc: 0.9966 Prec: 0.9976 AUC: 0.9988']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9975 Acc: 0.9964 Prec: 0.9975 AUC: 0.9988']\n",
      "train epoch: 41, loss: 9.452837232930765e-06\n",
      "------Testing epoch 41/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0074 F1: 0.1017 Acc: 0.0747 Prec: 0.1154 AUC: 0.5479']\n",
      "['UNWEIGHTED Testing Loss: 0.0074 F1: 0.1014 Acc: 0.0738 Prec: 0.1157 AUC: 0.5477']\n",
      "test epoch: 41, loss: 0.007351385689579065\n",
      "\n",
      "-----Training epoch 42/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9977 Acc: 0.9963 Prec: 0.9977 AUC: 0.9990']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9977 Acc: 0.9964 Prec: 0.9977 AUC: 0.9989']\n",
      "train epoch: 42, loss: 9.054077535230377e-06\n",
      "------Testing epoch 42/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0074 F1: 0.0990 Acc: 0.0766 Prec: 0.1098 AUC: 0.5471']\n",
      "['UNWEIGHTED Testing Loss: 0.0074 F1: 0.0980 Acc: 0.0747 Prec: 0.1097 AUC: 0.5465']\n",
      "test epoch: 42, loss: 0.007429521459475552\n",
      "\n",
      "-----Training epoch 43/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9976 Acc: 0.9964 Prec: 0.9977 AUC: 0.9988']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9975 Acc: 0.9964 Prec: 0.9976 AUC: 0.9988']\n",
      "train epoch: 43, loss: 9.9676136681415e-06\n",
      "------Testing epoch 43/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0076 F1: 0.0992 Acc: 0.0756 Prec: 0.1117 AUC: 0.5470']\n",
      "['UNWEIGHTED Testing Loss: 0.0076 F1: 0.0990 Acc: 0.0744 Prec: 0.1119 AUC: 0.5468']\n",
      "test epoch: 43, loss: 0.0076001239456134975\n",
      "\n",
      "-----Training epoch 44/49 --------\n",
      "['WEIGHTED Training Loss: 0.0001 F1: 0.9725 Acc: 0.9376 Prec: 0.9770 AUC: 0.9863']\n",
      "['UNWEIGHTED Training Loss: 0.0001 F1: 0.9721 Acc: 0.9362 Prec: 0.9770 AUC: 0.9859']\n",
      "train epoch: 44, loss: 8.556752959275107e-05\n",
      "------Testing epoch 44/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0099 F1: 0.0216 Acc: 0.0062 Prec: 0.0267 AUC: 0.5102']\n",
      "['UNWEIGHTED Testing Loss: 0.0099 F1: 0.0221 Acc: 0.0057 Prec: 0.0272 AUC: 0.5104']\n",
      "test epoch: 44, loss: 0.009933480317866311\n",
      "\n",
      "-----Training epoch 45/49 --------\n",
      "['WEIGHTED Training Loss: 0.0015 F1: 0.4937 Acc: 0.2543 Prec: 0.5542 AUC: 0.7410']\n",
      "['UNWEIGHTED Training Loss: 0.0015 F1: 0.4925 Acc: 0.2501 Prec: 0.5552 AUC: 0.7398']\n",
      "train epoch: 45, loss: 0.0015251709695021658\n",
      "------Testing epoch 45/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0062 F1: 0.0683 Acc: 0.0327 Prec: 0.0791 AUC: 0.5327']\n",
      "['UNWEIGHTED Testing Loss: 0.0062 F1: 0.0682 Acc: 0.0336 Prec: 0.0795 AUC: 0.5326']\n",
      "test epoch: 45, loss: 0.006150452655410303\n",
      "\n",
      "-----Training epoch 46/49 --------\n",
      "['WEIGHTED Training Loss: 0.0002 F1: 0.9203 Acc: 0.8053 Prec: 0.9445 AUC: 0.9570']\n",
      "['UNWEIGHTED Training Loss: 0.0002 F1: 0.9198 Acc: 0.8011 Prec: 0.9455 AUC: 0.9563']\n",
      "train epoch: 46, loss: 0.00023322904976753832\n",
      "------Testing epoch 46/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0064 F1: 0.0850 Acc: 0.0490 Prec: 0.0999 AUC: 0.5399']\n",
      "['UNWEIGHTED Testing Loss: 0.0064 F1: 0.0861 Acc: 0.0496 Prec: 0.1013 AUC: 0.5404']\n",
      "test epoch: 46, loss: 0.006381102797209468\n",
      "\n",
      "-----Training epoch 47/49 --------\n",
      "['WEIGHTED Training Loss: 0.0001 F1: 0.9870 Acc: 0.9626 Prec: 0.9907 AUC: 0.9932']\n",
      "['UNWEIGHTED Training Loss: 0.0001 F1: 0.9865 Acc: 0.9609 Prec: 0.9903 AUC: 0.9928']\n",
      "train epoch: 47, loss: 6.0591596525150575e-05\n",
      "------Testing epoch 47/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0065 F1: 0.1110 Acc: 0.0821 Prec: 0.1245 AUC: 0.5530']\n",
      "['UNWEIGHTED Testing Loss: 0.0065 F1: 0.1109 Acc: 0.0807 Prec: 0.1248 AUC: 0.5529']\n",
      "test epoch: 47, loss: 0.006495405369983732\n",
      "\n",
      "-----Training epoch 48/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9957 Acc: 0.9910 Prec: 0.9965 AUC: 0.9978']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9956 Acc: 0.9906 Prec: 0.9964 AUC: 0.9977']\n",
      "train epoch: 48, loss: 2.385067636418264e-05\n",
      "------Testing epoch 48/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0067 F1: 0.1044 Acc: 0.0776 Prec: 0.1177 AUC: 0.5494']\n",
      "['UNWEIGHTED Testing Loss: 0.0067 F1: 0.1038 Acc: 0.0759 Prec: 0.1177 AUC: 0.5490']\n",
      "test epoch: 48, loss: 0.00672332056053715\n",
      "\n",
      "-----Training epoch 49/49 --------\n",
      "['WEIGHTED Training Loss: 0.0000 F1: 0.9970 Acc: 0.9954 Prec: 0.9972 AUC: 0.9985']\n",
      "['UNWEIGHTED Training Loss: 0.0000 F1: 0.9969 Acc: 0.9951 Prec: 0.9972 AUC: 0.9984']\n",
      "train epoch: 49, loss: 1.531045667303563e-05\n",
      "------Testing epoch 49/49 --------\n",
      "['WEIGHTED Testing Loss: 0.0070 F1: 0.1080 Acc: 0.0809 Prec: 0.1203 AUC: 0.5515']\n",
      "['UNWEIGHTED Testing Loss: 0.0070 F1: 0.1074 Acc: 0.0789 Prec: 0.1206 AUC: 0.5510']\n",
      "test epoch: 49, loss: 0.006999709500218982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "train_results = []\n",
    "test_results = []\n",
    "diff_train_test_losses = [1000]\n",
    "\n",
    "save_model_path = './saved_models/multilabel_adam0.001_resnet_pretrained_3labels/'\n",
    "if os.path.isdir(save_model_path) == False:\n",
    "    os.makedirs(save_model_path)\n",
    "    print(\"Directory '%s' created\" %save_model_path)\n",
    "epochs=50\n",
    "best_loss = 1000.0\n",
    "for i in range(epochs):\n",
    "    print('-----Training epoch {}/{} --------'.format(i,epochs-1))\n",
    "    tr_loss, tr_acc, tr_res = train_epoch(model, train_dataloader, optimizer, criterion, sgdr_partial)\n",
    "    print('train epoch: {}, loss: {}'.format(i, tr_loss))\n",
    "    print('------Testing epoch {}/{} --------'.format(i,epochs-1))\n",
    "    tst_loss, tst_acc, tst_res = test_epoch(model, test_dataloader, criterion)\n",
    "    print('test epoch: {}, loss: {}'.format(i, tst_loss))\n",
    "    print()\n",
    "    \n",
    "    train_losses.append(tr_loss)\n",
    "    train_accs.append(tr_acc)\n",
    "    test_losses.append(tst_loss)\n",
    "    test_accs.append(tst_acc)\n",
    "    \n",
    "    train_results += tr_res\n",
    "    test_results += tst_res\n",
    "    \n",
    "    diff_train_test = np.absolute(tr_loss-tst_loss)\n",
    "    \n",
    "    if i > 10:\n",
    "        # if loss decreases\n",
    "        if tst_loss < best_loss:\n",
    "            best_loss = tst_loss\n",
    "            save_path_resnet = save_model_path + 'resnet_{}.pt'.format(i)\n",
    "            createCheckpoint(save_path_resnet, 64, i)\n",
    "\n",
    "        # if difference between train and test is smaller, save\n",
    "        elif diff_train_test < diff_train_test_losses[-1]:\n",
    "            save_path_resnet = save_model_path + 'resnet_{}.pt'.format(i)\n",
    "            createCheckpoint(save_path_resnet, 64, i)\n",
    "    \n",
    "    diff_train_test_losses.append(diff_train_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_path = './saved_models/multilabel_adam0.001_resnet_pretrained_3labels/'\n",
    "# save_path_resnet = save_model_path + 'resnet_last_epoch.pt'\n",
    "# createCheckpoint(save_path_resnet, 64, 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './pretrained/saved_models/v3_3labels/'\n",
    "checkpoint = torch.load(save_model_path + \"resnet_28.pt\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "batch_size = checkpoint['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(epochs, train, test, train_name, val_name, name_long, name_short):\n",
    "    plt.plot(epochs, train, 'g', label=train_name, c=\"mediumvioletred\")\n",
    "    plt.plot(epochs, test, 'b', label=val_name, c=\"darkturquoise\")\n",
    "    plt.title(name_long)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(name_short)\n",
    "    plt.legend()\n",
    "    plt.savefig('ADARI_resnet152_pretrained_adam0001.pdf', dpi=300)\n",
    "    plt.savefig('ADARI_resnet152_pretrained_adam0001.png', dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [e for e in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_graph(epochs_list, train_losses, test_losses, 'train', 'test', 'train_test_loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_losses\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(str(item) for item in train_losses))\n",
    "with open(\"test_losses\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(str(item) for item in test_losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
