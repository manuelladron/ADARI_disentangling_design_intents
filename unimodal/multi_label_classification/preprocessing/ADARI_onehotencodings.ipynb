{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import io\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import pdb\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for file dset_dataloader.json\n",
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar to visualize progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILES FOR DATALOADER\n",
    "dset_words_p = \"/Users/manuelladron/iCloud_archive/Documents/_CMU/PHD-CD/PHD-CD_Research/ADARI/json_files/cleaned/ADARI_v2/furniture/ADARI_furniture_words.json\"\n",
    "vocab_p = \"/Users/manuelladron/iCloud_archive/Documents/_CMU/PHD-CD/PHD-CD_Research/ADARI/json_files/cleaned/ADARI_v2/furniture/ADARI_furniture_vocab_adjs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeOneHots(object):\n",
    "    def __init__(self, image_words_path, vocab_path, out_path, freq_voc=2):\n",
    "\n",
    "        self.out_path = out_path\n",
    "        self.freq_vocab = freq_voc\n",
    "        self.vocab = open_json(vocab_path)\n",
    "        self.image_words = open_json(image_words_path)\n",
    "    \n",
    "    def save_json(self, file_path, data):\n",
    "        out_file = open(file_path, \"w\")\n",
    "        json.dump(data, out_file)\n",
    "        out_file.close()\n",
    "    \n",
    "    def chop_vocabulary(self, verbose=True):\n",
    "        chopped = dict()\n",
    "        w2i = dict()\n",
    "        i2w = dict()\n",
    "        i = 0\n",
    "        \n",
    "        for k,v in self.vocab.items():\n",
    "            if v > self.freq_vocab:\n",
    "                chopped[k] = v\n",
    "                w2i[k] = i\n",
    "                i2w[i] = k\n",
    "                i += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print('Original dictionary length: {}\\nCropped dictionary length:{}'.format(len(self.vocab), len(chopped)))\n",
    "        sorted_vocab = {k: v for k, v in sorted(chopped.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        return sorted_vocab, w2i, i2w\n",
    "    \n",
    "    \n",
    "    def sampling_rate_and_negative_sample(self, vocab, w2i):\n",
    "        # Returns sampling rate of word (prob of keeping the word ) and negative sampling rate\n",
    "        # 1) variables for sampling_rate\n",
    "\n",
    "        frequencies_ids = dict()\n",
    "        frequencies = dict()\n",
    "        total_number_words = sum(vocab.values())\n",
    "        threshold = 1e-5\n",
    "        for word, count in vocab.items():\n",
    "            # for sampling rate \n",
    "            z_w = count / total_number_words # this all add up to 1\n",
    "            frequencies[word] = z_w\n",
    "            w_id = w2i[word]\n",
    "            frequencies_ids[w_id] = z_w\n",
    "\n",
    "        # Noise_dist\n",
    "        noise_dist = {key:val**(3/4) for key, val in frequencies.items()}\n",
    "\n",
    "        # Frequency of dropping\n",
    "        p_drop = {word: 1 - np.sqrt(threshold/frequencies[word]) for word in vocab}\n",
    "\n",
    "        # Noise dist normalized \n",
    "        Z = sum(noise_dist.values())\n",
    "        neg_sampling = dict()\n",
    "        neg_sampling_ids = dict()\n",
    "\n",
    "        for k, v in noise_dist.items():\n",
    "            k_id = w2i[k]\n",
    "            n_s_value = v/Z\n",
    "            neg_sampling[k] = n_s_value\n",
    "            neg_sampling_ids[k_id] = n_s_value\n",
    "\n",
    "        return frequencies, frequencies_ids \n",
    "    \n",
    "    \n",
    "    #### Get 10 most relevant \n",
    "    def sort_list_by_sample_rate(self, idx_list, i2w, s_rate_idx, verbose=False):\n",
    "        \"\"\"\n",
    "        Receives a list of indexes, sorts this list according to a dictionary of sample rates, and selects \n",
    "        the 10 more common labels\n",
    "        \"\"\"\n",
    "        sorted_idxs = sorted(idx_list, key=lambda x: s_rate_idx[x], reverse=True)\n",
    "        if verbose:\n",
    "            for idx in idx_list:\n",
    "                print('idx: {} corresponds to word: {} and has s_rate: {}'.format(idx, i2w[idx], s_rate_idx[idx]))\n",
    "            print('sorted_idxs')\n",
    "        \n",
    "        return sorted_idxs[:10]\n",
    "    \n",
    "    def get_one_hot(self, w2i, i2w, s_rate_idx):\n",
    "        images_onehot = dict()\n",
    "        vocab_len = len(w2i)\n",
    "        ii = 0\n",
    "        dic_length = len(self.image_words)\n",
    "        for k, v in self.image_words.items():\n",
    "            update_progress(ii/dic_length)\n",
    "            onehot = [0] * vocab_len\n",
    "            has_label = False\n",
    "            idxs = []\n",
    "            for label in v:\n",
    "                if label in w2i.keys():\n",
    "                    idx = w2i[label]\n",
    "                    idxs.append(idx)\n",
    "                    #onehot[idx] = 1\n",
    "                    has_label = True\n",
    "            \n",
    "            # Get list of max 10 most common labels \n",
    "            most_common = self.sort_list_by_sample_rate(idxs, i2w, s_rate_idx)\n",
    "            \n",
    "            for x in most_common:\n",
    "                onehot[x] = 1\n",
    "            images_onehot[k] = onehot\n",
    "            ii += 1\n",
    "        \n",
    "        return images_onehot\n",
    "    \n",
    "    def run(self):\n",
    "        sorted_vocab, w2i, i2w = self.chop_vocabulary()\n",
    "        s_rate, s_rate_idxs = self.sampling_rate_and_negative_sample(sorted_vocab, w2i)\n",
    "        images_onehot = self.get_one_hot(w2i, i2w, s_rate_idxs)\n",
    "        \n",
    "        self.save_json(self.out_path + '/ADARI_furniture_onehots.json', images_onehot)\n",
    "        self.save_json(self.out_path + '/ADARI_furniture_onehots_vocab.json', sorted_vocab)\n",
    "        self.save_json(self.out_path + '/ADARI_furniture_onehots_w2i.json', w2i)\n",
    "        self.save_json(self.out_path + '/ADARI_furniture_onehots_i2w.json', i2w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/manuelladron/iCloud_archive/Documents/_CMU/PHD-CD/PHD-CD_Research/ADARI/json_files/cleaned/ADARI_v2/furniture'\n",
    "M = MakeOneHots(dset_words_p, vocab_p, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n"
     ]
    }
   ],
   "source": [
    "M.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
