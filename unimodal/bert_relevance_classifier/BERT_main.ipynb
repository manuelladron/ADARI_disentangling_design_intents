{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import * # for pad_sequence and whatnot\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import json\n",
    "import pickle \n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from BERT_train_test import train, test\n",
    "from BERT_preprocessing_text import PreprocessedData\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_tagged_p ='./data/design_dz-cleaned-tagged.json'\n",
    "arch_tagged_p ='./data/architecture_dz-cleaned-tagged.json'\n",
    "tech_tagged_p ='./data/technology_dz-cleaned-tagged.json'\n",
    "\n",
    "design_ntag_p = '.data/design_dz-cleaned.json'\n",
    "arch_ntag_p = './data/architecture_dz-cleaned.json'\n",
    "tech_ntag_p = './data/technology_dz-cleaned.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(epochs, plots, name_long, name_short, save_name):\n",
    "    colors = ['darkturquoise', 'mediumvioletred', 'beige', 'brown', 'chartreuse', 'chocolate', \n",
    "              'coral', 'crimson', 'fusia', 'goldenrod', 'green', 'indigo', 'lavender', 'lightgreen', \n",
    "              'orange', 'orchid']\n",
    "    print('pp')\n",
    "    BCK = (1, 1, 1)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.set_facecolor(BCK)\n",
    "    print('oo')\n",
    "    for idx, p in enumerate(plots):\n",
    "        data = p[0]\n",
    "        name = p[1]\n",
    "        color = colors[idx]\n",
    "        ax.plot(epochs, data, label=name, c=color)\n",
    "        \n",
    "    ax.set_title(name_long)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(name_short)\n",
    "    ax.legend()\n",
    "    print('ii')\n",
    "    plt.savefig(save_name, dpi=300)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    plt.cla()\n",
    "    print('hh')\n",
    "    return\n",
    "\n",
    "def save_json(file_path, data):\n",
    "    out_file = open(file_path, \"w\")\n",
    "    json.dump(data, out_file)\n",
    "    out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(model, train_dataloader, validation_dataloader, optimizer, scheduler, epochs, b, m, eq_name):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_losses, train_accs, train_precs, train_recalls, train_f1s = [], [], [], [], []\n",
    "    test_losses, test_accs, test_precs, test_recalls, test_f1s = [], [], [], [], []\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('======== Epoch %d ========' % epoch)\n",
    "        train_loss, train_acc, train_prec, train_recall, train_f1 = train(model, train_dataloader, optimizer, scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_precs.append(train_prec)\n",
    "        train_recalls.append(train_recall)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        test_loss, test_acc, test_prec, test_recall, test_f1 = test(model, validation_dataloader)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        test_precs.append(test_prec)\n",
    "        test_recalls.append(test_recall)\n",
    "        test_f1s.append(test_f1)\n",
    "        \n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            name = 'best_epoch_{}_batch_size_{}_max_length_{}.pt'.format(eq_name, str(b), str(m))\n",
    "            torch.save(model.state_dict(), name)\n",
    "    \n",
    "    print(\"Total training took %.2f seconds\" % (time.time()-start_time))\n",
    "    \n",
    "    return train_losses, train_accs, train_precs, train_recalls, train_f1s, test_losses, test_accs, test_precs, test_recalls, test_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    MODEL_NAME = 'bert-base-uncased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    batches = [64, 32, 16, 8]\n",
    "    max_lengths = [32, 64, 100]\n",
    "    equal = [True, False]\n",
    "    for eqs in equal:\n",
    "        eq_name = 'equal' if eqs else 'unequal'\n",
    "        results = dict()\n",
    "        for b in batches:\n",
    "            train_losses, train_accs, train_precs, train_recalls, train_f1s = [], [], [], [], []\n",
    "            test_losses, test_accs, test_precs, test_recalls, test_f1s = [], [], [], [], []\n",
    "            ms = []\n",
    "            \n",
    "            for m in max_lengths:\n",
    "                # For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "                batch_size = b\n",
    "                max_length = m\n",
    "\n",
    "                dataset = PreprocessedData([design_tagged_p, arch_tagged_p, tech_tagged_p], \n",
    "                                           [design_ntag_p, arch_ntag_p, tech_ntag_p], \n",
    "                                           tokenizer, max_length, eqs, './data')\n",
    "                \n",
    "                train_dataset = dataset.train_dataset\n",
    "                dev_dataset = dataset.dev_dataset\n",
    "                \n",
    "                # We'll take training samples in random order. \n",
    "                train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "                validation_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "                # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "                # linear classification layer on top. \n",
    "                model = BertForSequenceClassification.from_pretrained(\n",
    "                    \"bert-base-uncased\",          # Use the 12-layer BERT model, with an uncased vocab.\n",
    "                    num_labels = 2,               # The number of output labels--2 for binary classification. \n",
    "                    output_attentions = False,    # Whether the model returns attentions weights.\n",
    "                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "                )\n",
    "\n",
    "                model.to(device)\n",
    "\n",
    "                optimizer = optim.AdamW(model.parameters(), \n",
    "                              lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                              eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                            )\n",
    "\n",
    "                # The BERT authors recommend between 2 and 4. \n",
    "                epochs = 4\n",
    "                \n",
    "                # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "                # (Note that this is not the same as the number of training samples).\n",
    "                total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "                # Create the learning rate scheduler.\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                            num_warmup_steps = 0,\n",
    "                                                            num_training_steps = total_steps)\n",
    "\n",
    "                print()\n",
    "                print('DISTRIBUTION: {}, BATCH_SIZE:{}, MAX_LENGTH:{}'.format(eq_name, b, m))\n",
    "                data = run_epochs(model, train_dataloader, validation_dataloader, optimizer, scheduler, epochs, b, m, eq_name)\n",
    "                print('------------------------')\n",
    "                \n",
    "                train_l, train_a, train_p, train_r, train_f1, test_l, test_a, test_p, test_r, test_f1 = data \n",
    "                # All these variables are list of metrics per 4 epochs. \n",
    "                \n",
    "                train_losses.append(train_l)\n",
    "                train_accs.append(train_a)\n",
    "                train_precs.append(train_p)\n",
    "                train_recalls.append(train_r)\n",
    "                train_f1s.append(train_f1)\n",
    "                \n",
    "                test_losses.append(test_l)\n",
    "                test_accs.append(test_a)\n",
    "                test_precs.append(test_p)\n",
    "                test_recalls.append(test_r)\n",
    "                test_f1s.append(test_f1)\n",
    "                ms.append(m)\n",
    "\n",
    "                # Delete model and empty memory\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # Store data in dictionary\n",
    "                name = 'BS_{}_ML_{}'.format(b, m)\n",
    "\n",
    "                par_res = dict()\n",
    "                par_res['test_accs'] = test_accs\n",
    "                par_res['test_precs'] = test_precs\n",
    "                par_res['test_recalls'] = test_recalls\n",
    "                par_res['test_f1s'] = test_f1s\n",
    "                par_res['test_loss'] = test_losses\n",
    "\n",
    "                par_res['train_accs'] = train_accs\n",
    "                par_res['train_precs'] = train_precs\n",
    "                par_res['train_recalls'] = train_recalls\n",
    "                par_res['train_f1s'] = train_f1s\n",
    "                par_res['train_loss'] = train_losses\n",
    "                \n",
    "            # Add dictionary to dictionary \n",
    "            results[name] = par_res\n",
    "            \n",
    "            # Make plot data\n",
    "            plots = []\n",
    "            for i in range(len(ms)):\n",
    "                data = train_losses[i]\n",
    "                name = 'Training, Max Length: %d' % ms[i]\n",
    "                plots.append([data, name])\n",
    "                data = test_losses[i]\n",
    "                name = 'Testing, Max Length: %d' % ms[i]\n",
    "                plots.append([data, name])\n",
    "            name = './graphs/loss-b%d-%s.pkl' % (b, eq_name)\n",
    "            with open(name, 'wb') as f:\n",
    "                pickle.dump(plots, f)\n",
    "\n",
    "            plots = []\n",
    "            for i in range(len(ms)):\n",
    "                data = train_accs[i]\n",
    "                name = 'Training, Max Length: %d' % ms[i]\n",
    "                plots.append([data, name])\n",
    "                data = test_accs[i]\n",
    "                name = 'Testing, Max Length: %d' % ms[i]\n",
    "                plots.append([data, name])\n",
    "            name = './graphs/accs-b%d-%s.pkl' % (b, eq_name)\n",
    "            with open(name, 'wb') as f:\n",
    "                pickle.dump(plots, f)\n",
    "                \n",
    "        save_json('BERT_results_{}'.format(eq_name), results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:64, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.61\n",
      "  Average training acc: 0.66\n",
      "  Average training prec: 0.68\n",
      "  Average training recall: 0.65\n",
      "  Average training f1: 0.65\n",
      "  Training took: 34.11 seconds\n",
      "  Validation Loss: 0.56\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.77\n",
      "  Validation Recall: 0.61\n",
      "  Validation F1: 0.68\n",
      "  Validation took: 2.67 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.53\n",
      "  Average training acc: 0.73\n",
      "  Average training prec: 0.78\n",
      "  Average training recall: 0.67\n",
      "  Average training f1: 0.71\n",
      "  Training took: 34.38 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.69\n",
      "  Validation Recall: 0.72\n",
      "  Validation F1: 0.70\n",
      "  Validation took: 2.70 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.47\n",
      "  Average training acc: 0.78\n",
      "  Average training prec: 0.81\n",
      "  Average training recall: 0.73\n",
      "  Average training f1: 0.76\n",
      "  Training took: 34.53 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.71\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.70\n",
      "  Validation took: 2.68 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.41\n",
      "  Average training acc: 0.82\n",
      "  Average training prec: 0.85\n",
      "  Average training recall: 0.77\n",
      "  Average training f1: 0.80\n",
      "  Training took: 34.64 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.71\n",
      "  Validation Recall: 0.68\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 2.71 seconds\n",
      "Total training took 153.59 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:64, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.61\n",
      "  Average training acc: 0.66\n",
      "  Average training prec: 0.69\n",
      "  Average training recall: 0.61\n",
      "  Average training f1: 0.63\n",
      "  Training took: 61.85 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.69\n",
      "  Validation Recall: 0.72\n",
      "  Validation F1: 0.70\n",
      "  Validation took: 5.28 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.51\n",
      "  Average training acc: 0.75\n",
      "  Average training prec: 0.78\n",
      "  Average training recall: 0.69\n",
      "  Average training f1: 0.73\n",
      "  Training took: 63.20 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.69\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 5.08 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.44\n",
      "  Average training acc: 0.80\n",
      "  Average training prec: 0.83\n",
      "  Average training recall: 0.74\n",
      "  Average training f1: 0.78\n",
      "  Training took: 60.80 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.74\n",
      "  Validation Recall: 0.61\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 5.03 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.39\n",
      "  Average training acc: 0.83\n",
      "  Average training prec: 0.87\n",
      "  Average training recall: 0.78\n",
      "  Average training f1: 0.82\n",
      "  Training took: 60.88 seconds\n",
      "  Validation Loss: 0.65\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.68\n",
      "  Validation Recall: 0.70\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 5.09 seconds\n",
      "Total training took 269.80 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:64, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.61\n",
      "  Average training acc: 0.67\n",
      "  Average training prec: 0.71\n",
      "  Average training recall: 0.62\n",
      "  Average training f1: 0.65\n",
      "  Training took: 94.32 seconds\n",
      "  Validation Loss: 0.56\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.70\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 8.56 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.52\n",
      "  Average training acc: 0.75\n",
      "  Average training prec: 0.79\n",
      "  Average training recall: 0.68\n",
      "  Average training f1: 0.73\n",
      "  Training took: 94.63 seconds\n",
      "  Validation Loss: 0.56\n",
      "  Validation Accuracy: 0.72\n",
      "  Validation Precision: 0.73\n",
      "  Validation Recall: 0.67\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 8.04 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.46\n",
      "  Average training acc: 0.79\n",
      "  Average training prec: 0.84\n",
      "  Average training recall: 0.74\n",
      "  Average training f1: 0.78\n",
      "  Training took: 92.53 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.71\n",
      "  Validation Recall: 0.67\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 8.17 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.41\n",
      "  Average training acc: 0.82\n",
      "  Average training prec: 0.86\n",
      "  Average training recall: 0.76\n",
      "  Average training f1: 0.81\n",
      "  Training took: 95.14 seconds\n",
      "  Validation Loss: 0.61\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.69\n",
      "  Validation Recall: 0.71\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 8.53 seconds\n",
      "Total training took 412.50 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:32, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.61\n",
      "  Average training acc: 0.66\n",
      "  Average training prec: 0.69\n",
      "  Average training recall: 0.62\n",
      "  Average training f1: 0.63\n",
      "  Training took: 41.77 seconds\n",
      "  Validation Loss: 0.55\n",
      "  Validation Accuracy: 0.72\n",
      "  Validation Precision: 0.74\n",
      "  Validation Recall: 0.66\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 2.80 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.50\n",
      "  Average training acc: 0.75\n",
      "  Average training prec: 0.78\n",
      "  Average training recall: 0.70\n",
      "  Average training f1: 0.73\n",
      "  Training took: 40.71 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.70\n",
      "  Validation Recall: 0.70\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 2.79 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.40\n",
      "  Average training acc: 0.81\n",
      "  Average training prec: 0.84\n",
      "  Average training recall: 0.77\n",
      "  Average training f1: 0.80\n",
      "  Training took: 40.59 seconds\n",
      "  Validation Loss: 0.63\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.72\n",
      "  Validation Recall: 0.66\n",
      "  Validation F1: 0.68\n",
      "  Validation took: 2.79 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.31\n",
      "  Average training acc: 0.87\n",
      "  Average training prec: 0.89\n",
      "  Average training recall: 0.84\n",
      "  Average training f1: 0.86\n",
      "  Training took: 40.77 seconds\n",
      "  Validation Loss: 0.68\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.71\n",
      "  Validation Recall: 0.67\n",
      "  Validation F1: 0.68\n",
      "  Validation took: 2.83 seconds\n",
      "Total training took 177.95 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:32, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training acc: 0.68\n",
      "  Average training prec: 0.70\n",
      "  Average training recall: 0.65\n",
      "  Average training f1: 0.66\n",
      "  Training took: 68.63 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.73\n",
      "  Validation Recall: 0.60\n",
      "  Validation F1: 0.65\n",
      "  Validation took: 5.52 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.50\n",
      "  Average training acc: 0.76\n",
      "  Average training prec: 0.79\n",
      "  Average training recall: 0.72\n",
      "  Average training f1: 0.75\n",
      "  Training took: 70.29 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.66\n",
      "  Validation Precision: 0.65\n",
      "  Validation Recall: 0.68\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 5.38 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.40\n",
      "  Average training acc: 0.82\n",
      "  Average training prec: 0.84\n",
      "  Average training recall: 0.79\n",
      "  Average training f1: 0.81\n",
      "  Training took: 67.73 seconds\n",
      "  Validation Loss: 0.69\n",
      "  Validation Accuracy: 0.66\n",
      "  Validation Precision: 0.64\n",
      "  Validation Recall: 0.70\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 5.29 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training acc: 0.87\n",
      "  Average training prec: 0.89\n",
      "  Average training recall: 0.85\n",
      "  Average training f1: 0.87\n",
      "  Training took: 67.90 seconds\n",
      "  Validation Loss: 0.75\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.66\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.67\n",
      "  Validation took: 5.40 seconds\n",
      "Total training took 304.18 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:32, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.67\n",
      "  Average training prec: 0.70\n",
      "  Average training recall: 0.65\n",
      "  Average training f1: 0.65\n",
      "  Training took: 104.65 seconds\n",
      "  Validation Loss: 0.56\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.70\n",
      "  Validation Recall: 0.74\n",
      "  Validation F1: 0.71\n",
      "  Validation took: 9.06 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.50\n",
      "  Average training acc: 0.75\n",
      "  Average training prec: 0.80\n",
      "  Average training recall: 0.69\n",
      "  Average training f1: 0.73\n",
      "  Training took: 102.44 seconds\n",
      "  Validation Loss: 0.56\n",
      "  Validation Accuracy: 0.72\n",
      "  Validation Precision: 0.73\n",
      "  Validation Recall: 0.71\n",
      "  Validation F1: 0.72\n",
      "  Validation took: 8.40 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.41\n",
      "  Average training acc: 0.82\n",
      "  Average training prec: 0.86\n",
      "  Average training recall: 0.76\n",
      "  Average training f1: 0.80\n",
      "  Training took: 102.41 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.69\n",
      "  Validation Recall: 0.73\n",
      "  Validation F1: 0.71\n",
      "  Validation took: 8.61 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.33\n",
      "  Average training acc: 0.86\n",
      "  Average training prec: 0.90\n",
      "  Average training recall: 0.81\n",
      "  Average training f1: 0.85\n",
      "  Training took: 105.55 seconds\n",
      "  Validation Loss: 0.65\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.72\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.70\n",
      "  Validation took: 8.42 seconds\n",
      "Total training took 452.46 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:16, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.68\n",
      "  Average training prec: 0.71\n",
      "  Average training recall: 0.60\n",
      "  Average training f1: 0.63\n",
      "  Training took: 52.07 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.74\n",
      "  Validation Recall: 0.62\n",
      "  Validation F1: 0.65\n",
      "  Validation took: 2.86 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.49\n",
      "  Average training acc: 0.76\n",
      "  Average training prec: 0.80\n",
      "  Average training recall: 0.72\n",
      "  Average training f1: 0.74\n",
      "  Training took: 52.13 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.73\n",
      "  Validation Recall: 0.66\n",
      "  Validation F1: 0.67\n",
      "  Validation took: 2.88 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.36\n",
      "  Average training acc: 0.84\n",
      "  Average training prec: 0.87\n",
      "  Average training recall: 0.82\n",
      "  Average training f1: 0.83\n",
      "  Training took: 52.61 seconds\n",
      "  Validation Loss: 0.71\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.67\n",
      "  Validation Recall: 0.70\n",
      "  Validation F1: 0.67\n",
      "  Validation took: 2.93 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training acc: 0.90\n",
      "  Average training prec: 0.92\n",
      "  Average training recall: 0.88\n",
      "  Average training f1: 0.89\n",
      "  Training took: 53.19 seconds\n",
      "  Validation Loss: 0.81\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.70\n",
      "  Validation Recall: 0.66\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 3.02 seconds\n",
      "Total training took 224.62 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:16, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.68\n",
      "  Average training prec: 0.71\n",
      "  Average training recall: 0.63\n",
      "  Average training f1: 0.65\n",
      "  Training took: 82.15 seconds\n",
      "  Validation Loss: 0.56\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.68\n",
      "  Validation Recall: 0.72\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 5.56 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.49\n",
      "  Average training acc: 0.77\n",
      "  Average training prec: 0.80\n",
      "  Average training recall: 0.71\n",
      "  Average training f1: 0.74\n",
      "  Training took: 80.74 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.72\n",
      "  Validation Precision: 0.75\n",
      "  Validation Recall: 0.63\n",
      "  Validation F1: 0.67\n",
      "  Validation took: 5.62 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.36\n",
      "  Average training acc: 0.85\n",
      "  Average training prec: 0.88\n",
      "  Average training recall: 0.81\n",
      "  Average training f1: 0.83\n",
      "  Training took: 81.62 seconds\n",
      "  Validation Loss: 0.70\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.72\n",
      "  Validation Recall: 0.63\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 5.77 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training acc: 0.90\n",
      "  Average training prec: 0.92\n",
      "  Average training recall: 0.87\n",
      "  Average training f1: 0.89\n",
      "  Training took: 83.53 seconds\n",
      "  Validation Loss: 0.81\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.68\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.67\n",
      "  Validation took: 5.65 seconds\n",
      "Total training took 350.98 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:16, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.67\n",
      "  Average training prec: 0.71\n",
      "  Average training recall: 0.61\n",
      "  Average training f1: 0.63\n",
      "  Training took: 112.59 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.72\n",
      "  Validation Recall: 0.62\n",
      "  Validation F1: 0.65\n",
      "  Validation took: 8.39 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.49\n",
      "  Average training acc: 0.76\n",
      "  Average training prec: 0.80\n",
      "  Average training recall: 0.71\n",
      "  Average training f1: 0.74\n",
      "  Training took: 114.21 seconds\n",
      "  Validation Loss: 0.60\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.68\n",
      "  Validation Recall: 0.73\n",
      "  Validation F1: 0.69\n",
      "  Validation took: 8.83 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.36\n",
      "  Average training acc: 0.85\n",
      "  Average training prec: 0.88\n",
      "  Average training recall: 0.81\n",
      "  Average training f1: 0.83\n",
      "  Training took: 113.81 seconds\n",
      "  Validation Loss: 0.71\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.68\n",
      "  Validation Recall: 0.65\n",
      "  Validation F1: 0.65\n",
      "  Validation took: 8.32 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.26\n",
      "  Average training acc: 0.89\n",
      "  Average training prec: 0.92\n",
      "  Average training recall: 0.87\n",
      "  Average training f1: 0.88\n",
      "  Training took: 113.15 seconds\n",
      "  Validation Loss: 0.84\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.67\n",
      "  Validation Recall: 0.71\n",
      "  Validation F1: 0.68\n",
      "  Validation took: 8.58 seconds\n",
      "Total training took 490.88 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:8, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.68\n",
      "  Average training prec: 0.70\n",
      "  Average training recall: 0.60\n",
      "  Average training f1: 0.61\n",
      "  Training took: 79.79 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.73\n",
      "  Validation Recall: 0.61\n",
      "  Validation F1: 0.63\n",
      "  Validation took: 3.10 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.48\n",
      "  Average training acc: 0.77\n",
      "  Average training prec: 0.80\n",
      "  Average training recall: 0.71\n",
      "  Average training f1: 0.72\n",
      "  Training took: 78.53 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.71\n",
      "  Validation Recall: 0.70\n",
      "  Validation F1: 0.68\n",
      "  Validation took: 3.04 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.35\n",
      "  Average training acc: 0.86\n",
      "  Average training prec: 0.88\n",
      "  Average training recall: 0.82\n",
      "  Average training f1: 0.82\n",
      "  Training took: 78.51 seconds\n",
      "  Validation Loss: 0.82\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.72\n",
      "  Validation Recall: 0.68\n",
      "  Validation F1: 0.67\n",
      "  Validation took: 3.07 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training acc: 0.91\n",
      "  Average training prec: 0.92\n",
      "  Average training recall: 0.88\n",
      "  Average training f1: 0.89\n",
      "  Training took: 78.65 seconds\n",
      "  Validation Loss: 1.12\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.70\n",
      "  Validation Recall: 0.68\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 3.15 seconds\n",
      "Total training took 330.77 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:8, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.67\n",
      "  Average training prec: 0.70\n",
      "  Average training recall: 0.60\n",
      "  Average training f1: 0.60\n",
      "  Training took: 104.91 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.80\n",
      "  Validation Recall: 0.42\n",
      "  Validation F1: 0.53\n",
      "  Validation took: 5.71 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.48\n",
      "  Average training acc: 0.78\n",
      "  Average training prec: 0.81\n",
      "  Average training recall: 0.72\n",
      "  Average training f1: 0.73\n",
      "  Training took: 103.64 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.73\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.68\n",
      "  Validation took: 5.79 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.34\n",
      "  Average training acc: 0.86\n",
      "  Average training prec: 0.89\n",
      "  Average training recall: 0.82\n",
      "  Average training f1: 0.84\n",
      "  Training took: 105.88 seconds\n",
      "  Validation Loss: 0.82\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.72\n",
      "  Validation Recall: 0.68\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 5.94 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.24\n",
      "  Average training acc: 0.91\n",
      "  Average training prec: 0.94\n",
      "  Average training recall: 0.89\n",
      "  Average training f1: 0.90\n",
      "  Training took: 103.39 seconds\n",
      "  Validation Loss: 1.15\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.70\n",
      "  Validation Recall: 0.69\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 5.71 seconds\n",
      "Total training took 443.89 seconds\n",
      "------------------------\n",
      "7944 7944\n",
      "6,355 training samples\n",
      "1,589 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: equal, BATCH_SIZE:8, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training acc: 0.68\n",
      "  Average training prec: 0.69\n",
      "  Average training recall: 0.62\n",
      "  Average training f1: 0.62\n",
      "  Training took: 139.41 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.66\n",
      "  Validation Recall: 0.67\n",
      "  Validation F1: 0.64\n",
      "  Validation took: 9.55 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.46\n",
      "  Average training acc: 0.79\n",
      "  Average training prec: 0.81\n",
      "  Average training recall: 0.74\n",
      "  Average training f1: 0.75\n",
      "  Training took: 139.85 seconds\n",
      "  Validation Loss: 0.65\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.68\n",
      "  Validation Recall: 0.67\n",
      "  Validation F1: 0.65\n",
      "  Validation took: 9.04 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training acc: 0.87\n",
      "  Average training prec: 0.89\n",
      "  Average training recall: 0.84\n",
      "  Average training f1: 0.85\n",
      "  Training took: 139.48 seconds\n",
      "  Validation Loss: 0.95\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.66\n",
      "  Validation Recall: 0.71\n",
      "  Validation F1: 0.66\n",
      "  Validation took: 9.59 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.22\n",
      "  Average training acc: 0.92\n",
      "  Average training prec: 0.93\n",
      "  Average training recall: 0.91\n",
      "  Average training f1: 0.91\n",
      "  Training took: 138.99 seconds\n",
      "  Validation Loss: 1.31\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.65\n",
      "  Validation Recall: 0.71\n",
      "  Validation F1: 0.65\n",
      "  Validation took: 9.03 seconds\n",
      "Total training took 600.46 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:64, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.69\n",
      "  Average training prec: 0.50\n",
      "  Average training recall: 0.22\n",
      "  Average training f1: 0.28\n",
      "  Training took: 53.26 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.71\n",
      "  Validation Recall: 0.17\n",
      "  Validation F1: 0.27\n",
      "  Validation took: 4.19 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.55\n",
      "  Average training acc: 0.73\n",
      "  Average training prec: 0.63\n",
      "  Average training recall: 0.41\n",
      "  Average training f1: 0.48\n",
      "  Training took: 54.02 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.55\n",
      "  Validation Recall: 0.44\n",
      "  Validation F1: 0.49\n",
      "  Validation took: 4.29 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.50\n",
      "  Average training acc: 0.77\n",
      "  Average training prec: 0.69\n",
      "  Average training recall: 0.52\n",
      "  Average training f1: 0.58\n",
      "  Training took: 55.39 seconds\n",
      "  Validation Loss: 0.60\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.55\n",
      "  Validation Recall: 0.45\n",
      "  Validation F1: 0.49\n",
      "  Validation took: 4.26 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.45\n",
      "  Average training acc: 0.80\n",
      "  Average training prec: 0.74\n",
      "  Average training recall: 0.60\n",
      "  Average training f1: 0.66\n",
      "  Training took: 53.37 seconds\n",
      "  Validation Loss: 0.63\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.54\n",
      "  Validation Recall: 0.48\n",
      "  Validation F1: 0.50\n",
      "  Validation took: 4.11 seconds\n",
      "Total training took 240.92 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:64, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.69\n",
      "  Average training prec: 0.49\n",
      "  Average training recall: 0.22\n",
      "  Average training f1: 0.28\n",
      "  Training took: 93.64 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.55\n",
      "  Validation Recall: 0.35\n",
      "  Validation F1: 0.42\n",
      "  Validation took: 7.89 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.53\n",
      "  Average training acc: 0.73\n",
      "  Average training prec: 0.63\n",
      "  Average training recall: 0.44\n",
      "  Average training f1: 0.51\n",
      "  Training took: 96.24 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.60\n",
      "  Validation Recall: 0.27\n",
      "  Validation F1: 0.37\n",
      "  Validation took: 8.34 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.47\n",
      "  Average training acc: 0.77\n",
      "  Average training prec: 0.70\n",
      "  Average training recall: 0.54\n",
      "  Average training f1: 0.60\n",
      "  Training took: 95.09 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.50\n",
      "  Validation Recall: 0.53\n",
      "  Validation F1: 0.51\n",
      "  Validation took: 7.76 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.41\n",
      "  Average training acc: 0.82\n",
      "  Average training prec: 0.75\n",
      "  Average training recall: 0.65\n",
      "  Average training f1: 0.69\n",
      "  Training took: 94.02 seconds\n",
      "  Validation Loss: 0.64\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.51\n",
      "  Validation Recall: 0.47\n",
      "  Validation F1: 0.48\n",
      "  Validation took: 7.85 seconds\n",
      "Total training took 413.75 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:64, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.68\n",
      "  Average training prec: 0.45\n",
      "  Average training recall: 0.22\n",
      "  Average training f1: 0.27\n",
      "  Training took: 146.50 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.59\n",
      "  Validation Recall: 0.27\n",
      "  Validation F1: 0.37\n",
      "  Validation took: 12.39 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.54\n",
      "  Average training acc: 0.73\n",
      "  Average training prec: 0.62\n",
      "  Average training recall: 0.45\n",
      "  Average training f1: 0.50\n",
      "  Training took: 143.06 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.72\n",
      "  Validation Precision: 0.60\n",
      "  Validation Recall: 0.37\n",
      "  Validation F1: 0.45\n",
      "  Validation took: 12.69 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.48\n",
      "  Average training acc: 0.77\n",
      "  Average training prec: 0.69\n",
      "  Average training recall: 0.54\n",
      "  Average training f1: 0.59\n",
      "  Training took: 146.74 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.55\n",
      "  Validation Recall: 0.47\n",
      "  Validation F1: 0.50\n",
      "  Validation took: 12.40 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.43\n",
      "  Average training acc: 0.80\n",
      "  Average training prec: 0.73\n",
      "  Average training recall: 0.64\n",
      "  Average training f1: 0.67\n",
      "  Training took: 143.55 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.53\n",
      "  Validation Recall: 0.49\n",
      "  Validation F1: 0.50\n",
      "  Validation took: 12.81 seconds\n",
      "Total training took 638.22 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:32, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.67\n",
      "  Average training prec: 0.45\n",
      "  Average training recall: 0.23\n",
      "  Average training f1: 0.27\n",
      "  Training took: 64.64 seconds\n",
      "  Validation Loss: 0.56\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.50\n",
      "  Validation Recall: 0.50\n",
      "  Validation F1: 0.49\n",
      "  Validation took: 4.39 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.53\n",
      "  Average training acc: 0.73\n",
      "  Average training prec: 0.64\n",
      "  Average training recall: 0.45\n",
      "  Average training f1: 0.51\n",
      "  Training took: 62.79 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.48\n",
      "  Validation Recall: 0.59\n",
      "  Validation F1: 0.51\n",
      "  Validation took: 4.28 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.43\n",
      "  Average training acc: 0.80\n",
      "  Average training prec: 0.73\n",
      "  Average training recall: 0.63\n",
      "  Average training f1: 0.66\n",
      "  Training took: 62.68 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.47\n",
      "  Validation Recall: 0.49\n",
      "  Validation F1: 0.47\n",
      "  Validation took: 4.35 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.33\n",
      "  Average training acc: 0.85\n",
      "  Average training prec: 0.80\n",
      "  Average training recall: 0.75\n",
      "  Average training f1: 0.76\n",
      "  Training took: 63.69 seconds\n",
      "  Validation Loss: 0.69\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.45\n",
      "  Validation Recall: 0.53\n",
      "  Validation F1: 0.47\n",
      "  Validation took: 4.51 seconds\n",
      "Total training took 274.21 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:32, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training acc: 0.69\n",
      "  Average training prec: 0.46\n",
      "  Average training recall: 0.23\n",
      "  Average training f1: 0.28\n",
      "  Training took: 109.87 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.60\n",
      "  Validation Recall: 0.24\n",
      "  Validation F1: 0.32\n",
      "  Validation took: 9.31 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.52\n",
      "  Average training acc: 0.74\n",
      "  Average training prec: 0.65\n",
      "  Average training recall: 0.47\n",
      "  Average training f1: 0.53\n",
      "  Training took: 115.83 seconds\n",
      "  Validation Loss: 0.60\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.54\n",
      "  Validation Recall: 0.44\n",
      "  Validation F1: 0.48\n",
      "  Validation took: 9.44 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.41\n",
      "  Average training acc: 0.81\n",
      "  Average training prec: 0.73\n",
      "  Average training recall: 0.64\n",
      "  Average training f1: 0.67\n",
      "  Training took: 116.23 seconds\n",
      "  Validation Loss: 0.69\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.51\n",
      "  Validation Recall: 0.42\n",
      "  Validation F1: 0.45\n",
      "  Validation took: 8.83 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training acc: 0.86\n",
      "  Average training prec: 0.81\n",
      "  Average training recall: 0.74\n",
      "  Average training f1: 0.76\n",
      "  Training took: 105.66 seconds\n",
      "  Validation Loss: 0.76\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.49\n",
      "  Validation Recall: 0.47\n",
      "  Validation F1: 0.47\n",
      "  Validation took: 8.23 seconds\n",
      "Total training took 486.32 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:32, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.68\n",
      "  Average training prec: 0.44\n",
      "  Average training recall: 0.20\n",
      "  Average training f1: 0.25\n",
      "  Training took: 160.81 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.53\n",
      "  Validation Recall: 0.43\n",
      "  Validation F1: 0.46\n",
      "  Validation took: 14.17 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.54\n",
      "  Average training acc: 0.73\n",
      "  Average training prec: 0.63\n",
      "  Average training recall: 0.43\n",
      "  Average training f1: 0.49\n",
      "  Training took: 174.49 seconds\n",
      "  Validation Loss: 0.57\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.53\n",
      "  Validation Recall: 0.48\n",
      "  Validation F1: 0.49\n",
      "  Validation took: 15.16 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.45\n",
      "  Average training acc: 0.79\n",
      "  Average training prec: 0.72\n",
      "  Average training recall: 0.58\n",
      "  Average training f1: 0.63\n",
      "  Training took: 169.30 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.52\n",
      "  Validation Recall: 0.48\n",
      "  Validation F1: 0.49\n",
      "  Validation took: 13.01 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.37\n",
      "  Average training acc: 0.84\n",
      "  Average training prec: 0.79\n",
      "  Average training recall: 0.70\n",
      "  Average training f1: 0.73\n",
      "  Training took: 158.52 seconds\n",
      "  Validation Loss: 0.67\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.51\n",
      "  Validation Recall: 0.47\n",
      "  Validation F1: 0.47\n",
      "  Validation took: 13.58 seconds\n",
      "Total training took 721.98 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:16, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training acc: 0.69\n",
      "  Average training prec: 0.43\n",
      "  Average training recall: 0.24\n",
      "  Average training f1: 0.28\n",
      "  Training took: 81.92 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.58\n",
      "  Validation Recall: 0.33\n",
      "  Validation F1: 0.38\n",
      "  Validation took: 4.43 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.50\n",
      "  Average training acc: 0.76\n",
      "  Average training prec: 0.65\n",
      "  Average training recall: 0.50\n",
      "  Average training f1: 0.53\n",
      "  Training took: 80.26 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation Precision: 0.58\n",
      "  Validation Recall: 0.34\n",
      "  Validation F1: 0.40\n",
      "  Validation took: 4.43 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.36\n",
      "  Average training acc: 0.84\n",
      "  Average training prec: 0.78\n",
      "  Average training recall: 0.69\n",
      "  Average training f1: 0.71\n",
      "  Training took: 80.96 seconds\n",
      "  Validation Loss: 0.71\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.51\n",
      "  Validation Recall: 0.48\n",
      "  Validation F1: 0.46\n",
      "  Validation took: 4.56 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training acc: 0.90\n",
      "  Average training prec: 0.85\n",
      "  Average training recall: 0.82\n",
      "  Average training f1: 0.82\n",
      "  Training took: 82.64 seconds\n",
      "  Validation Loss: 0.89\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.50\n",
      "  Validation Recall: 0.46\n",
      "  Validation F1: 0.44\n",
      "  Validation took: 4.52 seconds\n",
      "Total training took 349.24 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:16, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.70\n",
      "  Average training prec: 0.48\n",
      "  Average training recall: 0.25\n",
      "  Average training f1: 0.30\n",
      "  Training took: 124.51 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.53\n",
      "  Validation Recall: 0.43\n",
      "  Validation F1: 0.44\n",
      "  Validation took: 8.66 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.52\n",
      "  Average training acc: 0.75\n",
      "  Average training prec: 0.65\n",
      "  Average training recall: 0.46\n",
      "  Average training f1: 0.50\n",
      "  Training took: 127.09 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.52\n",
      "  Validation Recall: 0.46\n",
      "  Validation F1: 0.45\n",
      "  Validation took: 9.27 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.41\n",
      "  Average training acc: 0.82\n",
      "  Average training prec: 0.78\n",
      "  Average training recall: 0.64\n",
      "  Average training f1: 0.67\n",
      "  Training took: 125.29 seconds\n",
      "  Validation Loss: 0.69\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.50\n",
      "  Validation Recall: 0.49\n",
      "  Validation F1: 0.46\n",
      "  Validation took: 8.60 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.31\n",
      "  Average training acc: 0.88\n",
      "  Average training prec: 0.84\n",
      "  Average training recall: 0.76\n",
      "  Average training f1: 0.78\n",
      "  Training took: 126.28 seconds\n",
      "  Validation Loss: 0.79\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.49\n",
      "  Validation Recall: 0.47\n",
      "  Validation F1: 0.45\n",
      "  Validation took: 9.03 seconds\n",
      "Total training took 544.19 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:16, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training acc: 0.70\n",
      "  Average training prec: 0.42\n",
      "  Average training recall: 0.25\n",
      "  Average training f1: 0.29\n",
      "  Training took: 174.55 seconds\n",
      "  Validation Loss: 0.60\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.61\n",
      "  Validation Recall: 0.20\n",
      "  Validation F1: 0.28\n",
      "  Validation took: 12.97 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.52\n",
      "  Average training acc: 0.75\n",
      "  Average training prec: 0.65\n",
      "  Average training recall: 0.48\n",
      "  Average training f1: 0.51\n",
      "  Training took: 176.85 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.60\n",
      "  Validation Recall: 0.35\n",
      "  Validation F1: 0.41\n",
      "  Validation took: 12.89 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.41\n",
      "  Average training acc: 0.82\n",
      "  Average training prec: 0.75\n",
      "  Average training recall: 0.66\n",
      "  Average training f1: 0.68\n",
      "  Training took: 174.25 seconds\n",
      "  Validation Loss: 0.71\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.54\n",
      "  Validation Recall: 0.44\n",
      "  Validation F1: 0.46\n",
      "  Validation took: 13.34 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.30\n",
      "  Average training acc: 0.88\n",
      "  Average training prec: 0.85\n",
      "  Average training recall: 0.78\n",
      "  Average training f1: 0.79\n",
      "  Training took: 175.75 seconds\n",
      "  Validation Loss: 0.80\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.52\n",
      "  Validation Recall: 0.43\n",
      "  Validation F1: 0.45\n",
      "  Validation took: 12.87 seconds\n",
      "Total training took 758.98 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:8, MAX_LENGTH:32\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.69\n",
      "  Average training prec: 0.31\n",
      "  Average training recall: 0.20\n",
      "  Average training f1: 0.22\n",
      "  Training took: 121.69 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.49\n",
      "  Validation Recall: 0.36\n",
      "  Validation F1: 0.38\n",
      "  Validation took: 4.87 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.50\n",
      "  Average training acc: 0.76\n",
      "  Average training prec: 0.60\n",
      "  Average training recall: 0.51\n",
      "  Average training f1: 0.51\n",
      "  Training took: 121.85 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation Precision: 0.47\n",
      "  Validation Recall: 0.42\n",
      "  Validation F1: 0.41\n",
      "  Validation took: 4.68 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.36\n",
      "  Average training acc: 0.86\n",
      "  Average training prec: 0.75\n",
      "  Average training recall: 0.70\n",
      "  Average training f1: 0.70\n",
      "  Training took: 121.20 seconds\n",
      "  Validation Loss: 0.85\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.47\n",
      "  Validation Recall: 0.47\n",
      "  Validation F1: 0.44\n",
      "  Validation took: 4.79 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training acc: 0.92\n",
      "  Average training prec: 0.84\n",
      "  Average training recall: 0.82\n",
      "  Average training f1: 0.81\n",
      "  Training took: 121.82 seconds\n",
      "  Validation Loss: 1.35\n",
      "  Validation Accuracy: 0.66\n",
      "  Validation Precision: 0.46\n",
      "  Validation Recall: 0.46\n",
      "  Validation F1: 0.42\n",
      "  Validation took: 4.68 seconds\n",
      "Total training took 511.13 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:8, MAX_LENGTH:64\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training acc: 0.70\n",
      "  Average training prec: 0.36\n",
      "  Average training recall: 0.24\n",
      "  Average training f1: 0.26\n",
      "  Training took: 160.12 seconds\n",
      "  Validation Loss: 0.58\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.40\n",
      "  Validation Recall: 0.24\n",
      "  Validation F1: 0.28\n",
      "  Validation took: 9.05 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.49\n",
      "  Average training acc: 0.77\n",
      "  Average training prec: 0.61\n",
      "  Average training recall: 0.51\n",
      "  Average training f1: 0.52\n",
      "  Training took: 161.90 seconds\n",
      "  Validation Loss: 0.63\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.49\n",
      "  Validation Recall: 0.41\n",
      "  Validation F1: 0.41\n",
      "  Validation took: 8.79 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.34\n",
      "  Average training acc: 0.87\n",
      "  Average training prec: 0.78\n",
      "  Average training recall: 0.73\n",
      "  Average training f1: 0.72\n",
      "  Training took: 161.28 seconds\n",
      "  Validation Loss: 0.96\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation Precision: 0.47\n",
      "  Validation Recall: 0.45\n",
      "  Validation F1: 0.42\n",
      "  Validation took: 9.36 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.23\n",
      "  Average training acc: 0.93\n",
      "  Average training prec: 0.84\n",
      "  Average training recall: 0.82\n",
      "  Average training f1: 0.82\n",
      "  Training took: 159.64 seconds\n",
      "  Validation Loss: 1.39\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.46\n",
      "  Validation Recall: 0.45\n",
      "  Validation F1: 0.42\n",
      "  Validation took: 8.74 seconds\n",
      "Total training took 684.36 seconds\n",
      "------------------------\n",
      "12246 12246\n",
      "9,796 training samples\n",
      "2,450 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTION: unequal, BATCH_SIZE:8, MAX_LENGTH:100\n",
      "======== Epoch 0 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training acc: 0.69\n",
      "  Average training prec: 0.35\n",
      "  Average training recall: 0.24\n",
      "  Average training f1: 0.26\n",
      "  Training took: 211.77 seconds\n",
      "  Validation Loss: 0.59\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.45\n",
      "  Validation Recall: 0.29\n",
      "  Validation F1: 0.33\n",
      "  Validation took: 13.75 seconds\n",
      "======== Epoch 1 ========\n",
      "  Average training loss: 0.51\n",
      "  Average training acc: 0.76\n",
      "  Average training prec: 0.58\n",
      "  Average training recall: 0.47\n",
      "  Average training f1: 0.48\n",
      "  Training took: 211.81 seconds\n",
      "  Validation Loss: 0.62\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.53\n",
      "  Validation Recall: 0.46\n",
      "  Validation F1: 0.46\n",
      "  Validation took: 13.76 seconds\n",
      "======== Epoch 2 ========\n",
      "  Average training loss: 0.35\n",
      "  Average training acc: 0.86\n",
      "  Average training prec: 0.75\n",
      "  Average training recall: 0.70\n",
      "  Average training f1: 0.70\n",
      "  Training took: 211.89 seconds\n",
      "  Validation Loss: 0.84\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Precision: 0.49\n",
      "  Validation Recall: 0.42\n",
      "  Validation F1: 0.42\n",
      "  Validation took: 13.77 seconds\n",
      "======== Epoch 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training acc: 0.92\n",
      "  Average training prec: 0.84\n",
      "  Average training recall: 0.81\n",
      "  Average training f1: 0.81\n",
      "  Training took: 211.47 seconds\n",
      "  Validation Loss: 1.34\n",
      "  Validation Accuracy: 0.67\n",
      "  Validation Precision: 0.48\n",
      "  Validation Recall: 0.47\n",
      "  Validation F1: 0.44\n",
      "  Validation took: 14.03 seconds\n",
      "Total training took 905.17 seconds\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in listdir('./graphs/') if isfile(join('./graphs/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss-b16-equal.pkl',\n",
       " 'loss-b16-unequal.pkl',\n",
       " 'accs-b16-unequal.pkl',\n",
       " 'accs-b8-unequal.pkl',\n",
       " 'accs-b32-equal.pkl',\n",
       " 'loss-b8-equal.pkl',\n",
       " 'accs-b16-equal.pkl',\n",
       " 'loss-b64-equal.pkl',\n",
       " 'loss-b32-unequal.pkl',\n",
       " 'loss-b32-equal.pkl',\n",
       " 'accs-b64-unequal.pkl',\n",
       " 'accs-b8-equal.pkl',\n",
       " 'loss-b64-unequal.pkl',\n",
       " 'accs-b32-unequal.pkl',\n",
       " 'accs-b64-equal.pkl',\n",
       " 'loss-b8-unequal.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n",
      "pp\n",
      "oo\n",
      "ii\n",
      "hh\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in files:\n",
    "    f_full = join('./graphs/', file)\n",
    "    with open(f_full, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    name_long = file[:-4]\n",
    "    name_short = file[:4]\n",
    "    save_path = './graphs/plotted_graphs/{}.png'.format(name_long)\n",
    "    make_graph([0,1,2,3], data, name_long, name_short, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
