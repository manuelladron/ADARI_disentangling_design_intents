{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json, datetime\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "CLASS_LABEL_PATH = \"../../ADARI/furniture/ADARI_furniture_onehots.json\"\n",
    "IMAGE_FOLDER = \"../../ADARI/v2/full\"\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "class ADARIMultiHotDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, class_label_file, image_size):\n",
    "        super(ADARIMultiHotDataset).__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.image_folder = image_folder\n",
    "        self.class_label_file = class_label_file\n",
    "        self.transform = transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])\n",
    "        self.im_to_one_hots = open_json(self.class_label_file)\n",
    "        self.im_names = list(self.im_to_one_hots.keys())\n",
    "        self.num_classes = len(self.im_to_one_hots[self.im_names[0]])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.im_names)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        imname = self.im_names[idx]\n",
    "        \n",
    "        img = Image.open(self.image_folder + '/' + imname)\n",
    "        return self.transform(img), torch.tensor(self.im_to_one_hots[imname])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "data = ADARIMultiHotDataset(IMAGE_FOLDER, CLASS_LABEL_PATH, 64)\n",
    "vocab_size = data.num_classes\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(data, \n",
    "                                                    [int(.8 * len(data)), len(data) - int(.8 * len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def build_model():\n",
    "    vgg = torchvision.models.vgg16()\n",
    "    vgg.classifier[6] = torch.nn.Linear(4096, vocab_size)\n",
    "    return vgg\n",
    "vgg = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "def one_hot_to_multilabel(y):\n",
    "    # Assumes y is batched, shape (batch_size, vocab_size)\n",
    "    yhat = [[] for _ in range(y.shape[0])]\n",
    "    nonzeros = torch.nonzero(y)\n",
    "    for x in nonzeros:\n",
    "        yhat[x[0]].append(x[1])\n",
    "    for i in range(len(yhat)):\n",
    "        yhat[i].extend([-1] * (y.shape[1] - len(yhat[i])))\n",
    "    return torch.tensor(yhat)\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=True, \n",
    "                                            num_workers=num_workers)\n",
    "    optimizer = torch.optim.Adam(vgg.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for im, labels in dataloader:\n",
    "            im = im.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            l_hat = vgg(im)\n",
    "            loss = criterion(l_hat, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        print(f\"Avg Loss at Epoch {epoch}: {sum(losses) / len(losses)}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = datetime.datetime.now()\n",
    "try:\n",
    "    train(vgg)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "vgg.to('cpu')\n",
    "torch.save(vgg.state_dict(), f\"VGG16_ADARI_{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "\"\"\"\n",
    "im = Image.open(IMAGE_FOLDER + '/' + \"0a2e5ec5079d9424e239d3dc639f7e1d20c6fba9.jpg\")\n",
    "im = transforms.Compose([transforms.Resize(64),\n",
    "                               transforms.CenterCrop(64),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])(im)\n",
    "print(vgg(im.reshape(1, im.shape[0], im.shape[1], im.shape[2])).shape)\n",
    "\n",
    "\n",
    "test_d = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "for im, l in test_d:\n",
    "    criterion = torch.nn.MultiLabelMarginLoss()\n",
    "    l = one_hot_to_multilabel(l)\n",
    "    print(l)\n",
    "    out = vgg(im)\n",
    "    print(out.shape)\n",
    "    print(criterion(out, l))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Test Accuracy\n",
    "def test():\n",
    "    vgg.eval()\n",
    "    vgg.to(device)\n",
    "    test_d = torch.utils.data.DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
    "    criterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    for im, l in test_d:\n",
    "        l = l.to(device)\n",
    "        imhat = vgg(im.to(device))\n",
    "        print(f\"Test MultiLabel Soft Margin Loss: {criterion(imhat, l)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MultiLabel Margin Loss: 0.6932395100593567\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1 Score\n",
    "def test_score(model, test_set):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    test_d = torch.utils.data.DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
    "    for im, l in test_d:\n",
    "        im = im.to(device)\n",
    "        imhat = model(im)\n",
    "        imhat.to('cpu')\n",
    "        score = f1_score(l, imhat, average='weighted')\n",
    "        print(f\"F1 Score: {score}\")\n",
    "        \n",
    "test_score(vgg, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
