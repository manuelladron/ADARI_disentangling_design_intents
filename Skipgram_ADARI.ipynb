{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Skipgram_ADARI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelladron/mmml_f20/blob/main/Skipgram_ADARI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMKtkR37w8NB",
        "outputId": "3e03b513-971d-4945-d08a-c986ab39195e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn3j5SaMw-Oc"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021')\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/notebooks/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb_Dy2MPw32a",
        "outputId": "d737af3a-6eed-4b4b-d8ea-d9a24d19dcc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import json, pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time, sys, os, random, io\n",
        "from operator import itemgetter\n",
        "from collections import Counter\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is True\n",
            "8 cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KjQliTbw32e",
        "outputId": "ffa761d2-bf79-4a19-9161-a2730283ba7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Check if cuda is available\n",
        "cuda = torch.cuda.is_available()\n",
        "print('CUDA is', cuda)\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "num_workers = 8 if cuda else 0\n",
        "print(num_workers, device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is True\n",
            "8 cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKAp_c92dbO3"
      },
      "source": [
        "def open_json(path):\n",
        "    f = open(path) \n",
        "    data = json.load(f) \n",
        "    f.close()\n",
        "    return data \n",
        "\n",
        "def save_json(file_path, data):\n",
        "    out_file = open(file_path, \"w\")\n",
        "    json.dump(data, out_file)\n",
        "    out_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFoTv4bwRx5d"
      },
      "source": [
        "def update_progress(progress):\n",
        "    bar_length = 20\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "\n",
        "    block = int(round(bar_length * progress))\n",
        "    clear_output(wait = True)\n",
        "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
        "    print(text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JbKPku_w32i"
      },
      "source": [
        "### Glove embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqp9rxlSw32l"
      },
      "source": [
        "### Our embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GoKXyMIc45O"
      },
      "source": [
        "fur_vocab_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/vocabulary/ADARI_furniture_vocab.json'\n",
        "context_w_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/context_words/furniture_context_size_9_all.json'\n",
        "fur_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/clean_files/ADARI_furniture_info.json'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EutMH8bM0x_"
      },
      "source": [
        "def sampling_rate_and_negative_sample(vocab, w2i):\n",
        "    # Returns sampling rate of word (prob of keeping the word ) and negative sampling rate\n",
        "    # 1) variables for sampling_rate\n",
        "    frequencies_ids = dict()\n",
        "    frequencies = dict()\n",
        "    total_number_words = sum(vocab.values())\n",
        "    threshold = 1e-5\n",
        "    for word, count in vocab.items():\n",
        "        # for sampling rate \n",
        "        z_w = count / total_number_words # this all add up to 1\n",
        "        # p_w = z_w**(3/4)\n",
        "        # p_w = (np.sqrt(z_w/0.001)+1) *(0.001/z_w)\n",
        "        # print('prob of keeping: {}= {}'.format(word, p_w))\n",
        "        frequencies[word] = z_w\n",
        "        w_id = w2i[word]\n",
        "        frequencies_ids[w_id] = z_w\n",
        "\n",
        "    # Noise_dist\n",
        "    noise_dist = {key:val**(3/4) for key, val in frequencies.items()}\n",
        "    \n",
        "    # Frequency of dropping\n",
        "    p_drop = {word: 1 - np.sqrt(threshold/frequencies[word]) for word in vocab}\n",
        "    \n",
        "    # Noise dist normalized \n",
        "    Z = sum(noise_dist.values())\n",
        "    neg_sampling = dict()\n",
        "    neg_sampling_ids = dict()\n",
        "    for k, v in noise_dist.items():\n",
        "        k_id = w2i[k]\n",
        "        n_s_value = v/Z\n",
        "        neg_sampling[k] = n_s_value\n",
        "        neg_sampling_ids[k_id] = n_s_value\n",
        "\n",
        "    return frequencies, frequencies_ids, neg_sampling, neg_sampling_ids, p_drop"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqVC_EM275zp"
      },
      "source": [
        "def get_vocab_adjs(path):\n",
        "    data = open_json(path)\n",
        "    vocab = []\n",
        "    targets = dict()\n",
        "    \n",
        "    for a, article in enumerate(data):\n",
        "        if a != len(data)-1:\n",
        "            q_adjs = article['q_adjs']\n",
        "            nq_adjs = article['nq_adjs']\n",
        "            all_adjs = q_adjs + nq_adjs\n",
        "            vocab += all_adjs\n",
        "\n",
        "            # contexts \n",
        "            unique_v = list(set(all_adjs))\n",
        "            for i in range(len(unique_v)):\n",
        "                for j in range(len(unique_v)):\n",
        "                    adj_1 = unique_v[i]\n",
        "                    adj_2 = unique_v[j]\n",
        "                    if i != j:\n",
        "                        if adj_1 not in targets:\n",
        "                            targets[adj_1] = [adj_2]\n",
        "                        else:\n",
        "                            targets[adj_1] += [adj_2]\n",
        "    \n",
        "    vocab_d = Counter(vocab)\n",
        "    \n",
        "    # include <pad>\n",
        "    if '<pad>' not in targets:\n",
        "        targets['<pad>'] = ['<pad>']\n",
        "    if '<pad>' not in vocab_d:\n",
        "        vocab_d['<pad>'] = 1\n",
        "    \n",
        "    # Both dicts have same length \n",
        "    assert(len(vocab_d) == len(targets))\n",
        "\n",
        "    # Get list of words\n",
        "    all_words = list(vocab_d.keys())\n",
        "    all_words.remove('<pad>')\n",
        "    \n",
        "    # Create 2 dicts\n",
        "    w2i = dict()\n",
        "    i2w = dict()\n",
        "    w2i['<pad>'] = 0\n",
        "    i2w[0] = '<pad>'\n",
        "    \n",
        "    for i in range(len(all_words)):\n",
        "        w = all_words[i]\n",
        "        w2i[w] = i+1\n",
        "        i2w[i+1] = w\n",
        "\n",
        "    s_rate, s_rate_ids, n_rate, n_rate_ids, p_drop = sampling_rate_and_negative_sample(vocab_d, w2i)\n",
        "\n",
        "    return vocab_d, targets, w2i, i2w, s_rate, n_rate, s_rate_ids, n_rate_ids, p_drop   "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p54sPoW971j"
      },
      "source": [
        "vocab, targets, w2i, i2w, s_rate, n_rate, s_rate_ids, n_rate_ids, p_drop = get_vocab_adjs(fur_path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAFjZf31bKXF"
      },
      "source": [
        "def get_target_words(vocab_path, context_words_path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - vocab  dict\n",
        "      - targets dict\n",
        "      - w2i and i2w dicts \n",
        "      - sampling rate and sampling rate ids\n",
        "      - negative sampling and negative sampling ids \n",
        "      - p drop of words \n",
        "    \"\"\"\n",
        "    cw = open_json(context_words_path)\n",
        "    vocab = open_json(vocab_path)\n",
        "    # Clean vocab \n",
        "    vocab_list_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "    sixty_most_common = vocab_list_sorted[:60]\n",
        "    stop_words = []\n",
        "    for w,f in sixty_most_common:\n",
        "        stop_words.append(w)\n",
        "    vocab = {word:v for word, v in vocab.items() if vocab[word] > 2 if word not in stop_words}\n",
        "    vocab['<path>'] = 1\n",
        "    # Get targets \n",
        "    targets = dict()\n",
        "    for i, w_context in enumerate(cw): # list with a bunch of dictionaries and a list (containing the center word)\n",
        "\n",
        "        context = []\n",
        "        center_w = None\n",
        "        # Extract center word and context of each entry\n",
        "        for elem in w_context: # elem is a LIST like: [{'word': 'well-priced', 'tag': 'JJ', 'pos': 'ADJ'}, 'root'] or DICT like {'word': 'furniture', 'tag': 'NN', 'pos': 'NOUN'}\n",
        "  \n",
        "            if isinstance(elem, list): # list containing center word \n",
        "                center_w = elem[0]['word']\n",
        "                if center_w not in vocab: \n",
        "                    center_w = None\n",
        "                    break\n",
        "            else:\n",
        "                if elem['word'] in vocab:\n",
        "                    c_word = elem['word']\n",
        "                    if c_word in vocab:\n",
        "                        context.append(elem['word'])\n",
        "\n",
        "        # Ensure center word and context are not empty\n",
        "        if center_w != None:\n",
        "            #assert(center_w in vocab)\n",
        "            if context != []:\n",
        "                # Add to dictionary \n",
        "                if center_w not in targets:\n",
        "                    targets[center_w] = context\n",
        "                else:\n",
        "                    targets[center_w] += context\n",
        "\n",
        "    # Clean reps \n",
        "    targets_c = dict()\n",
        "    for center, context in targets.items():\n",
        "        targets_c[center] = list(set(context))\n",
        "\n",
        "    # Delete words in vocab that do not appear in context words \n",
        "    new_vocab = vocab.copy()\n",
        "    for w in vocab:\n",
        "        if w not in targets_c:\n",
        "            del new_vocab[w]\n",
        "    \n",
        "    if '<pad>' not in targets_c:\n",
        "        targets_c['<pad>'] = ['<pad>']\n",
        "    if '<pad>' not in new_vocab:\n",
        "        new_vocab['<pad>'] = 1\n",
        "    \n",
        "    # Both dicts have same length \n",
        "    assert(len(new_vocab) == len(targets_c))\n",
        "    \n",
        "    # Get list of words\n",
        "    all_words = list(new_vocab.keys())\n",
        "    \n",
        "    # Create 2 dicts\n",
        "    w2i = dict()\n",
        "    i2w = dict()\n",
        "    w2i['<pad>'] = 0\n",
        "    i2w[0] = '<pad>'\n",
        "    \n",
        "    for i in range(len(all_words)):\n",
        "        w = all_words[i]\n",
        "        if w == '<pad>':\n",
        "            continue\n",
        "        w2i[w] = i+1\n",
        "        i2w[i+1] = w\n",
        "\n",
        "    s_rate, s_rate_ids, n_rate, n_rate_ids, p_drop = sampling_rate_and_negative_sample(new_vocab, w2i)\n",
        "\n",
        "    return new_vocab, targets_c, w2i, i2w, s_rate, n_rate, s_rate_ids, n_rate_ids, p_drop"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFtT6QK4rlJq"
      },
      "source": [
        "#vocab, targets_c, w2i, i2w, s_rate, n_rate, s_rate_ids, n_rate_ids, p_drop = get_target_words(fur_vocab_path, context_w_path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijp6XihpB3Sx",
        "outputId": "63bcebf8-aa52-484c-f1d6-b510b9d0c57d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(targets['family-run']), len(targets['italian'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(320, 12250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1srKZ5_MbmB",
        "outputId": "3f4b6c48-2e2f-498a-ca44-5223f0e355bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "targets['well-priced'], i2w[3160], s_rate['italian'], s_rate['studio-designed']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['country-friendly',\n",
              "  'old',\n",
              "  'international',\n",
              "  'three-piece',\n",
              "  'mutual',\n",
              "  'practical',\n",
              "  'low',\n",
              "  'circular',\n",
              "  'cute',\n",
              "  'back',\n",
              "  'simple',\n",
              "  'contemporary',\n",
              "  'modern',\n",
              "  'occasional',\n",
              "  'spacious',\n",
              "  'high-backed',\n",
              "  'physical',\n",
              "  'front',\n",
              "  'well-made',\n",
              "  'useful',\n",
              "  'three-legged',\n",
              "  'small',\n",
              "  'comfortable',\n",
              "  'gorgeous',\n",
              "  'woolly',\n",
              "  'passionate',\n",
              "  'lovely',\n",
              "  'studio-designed',\n",
              "  'gritty',\n",
              "  'coherent',\n",
              "  'british-made',\n",
              "  'about',\n",
              "  'continental',\n",
              "  'least',\n",
              "  'chubby',\n",
              "  'in-house',\n",
              "  'unusual',\n",
              "  'american',\n",
              "  'family-run',\n",
              "  'cultural',\n",
              "  'wonderful',\n",
              "  'general',\n",
              "  'mixed',\n",
              "  'lifestyle-oriented',\n",
              "  'collaborative',\n",
              "  'special',\n",
              "  'single',\n",
              "  'fantastic',\n",
              "  'italian',\n",
              "  'original',\n",
              "  'three-seat',\n",
              "  'strong',\n",
              "  'hand-finished',\n",
              "  'hand-made',\n",
              "  'small',\n",
              "  'different',\n",
              "  'expensive',\n",
              "  'particular',\n",
              "  'major',\n",
              "  'real',\n",
              "  'natural',\n",
              "  'inspiring',\n",
              "  'nice',\n",
              "  'colourful',\n",
              "  'diluted',\n",
              "  'interesting',\n",
              "  'involved',\n",
              "  'great',\n",
              "  'japanese',\n",
              "  'spacious',\n",
              "  'full',\n",
              "  'confused',\n",
              "  'good',\n",
              "  'curved',\n",
              "  'british',\n",
              "  'high-street',\n",
              "  'creative',\n",
              "  'beautiful',\n",
              "  'then-exotic',\n",
              "  'open'],\n",
              " 'warsaw-based',\n",
              " 0.008735163438180468,\n",
              " 1.2586690833113066e-05)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2z3gv8NeBF",
        "outputId": "286f6cb1-0a2d-4222-a912-f1dee896d4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab), len(targets)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7492, 7492)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB5BfkDmyynI",
        "outputId": "93dc4e69-de44-44ef-98aa-47d766538441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(sum(s_rate.values()))\n",
        "print(sum(n_rate.values()))\n",
        "print(sum(p_drop.values()))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000000000000995\n",
            "0.99999999999989\n",
            "2510.8669174032625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOrcMhkP0MpE"
      },
      "source": [
        "**Data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXPrBjGp32AN"
      },
      "source": [
        "class Skipgram_data(Dataset):\n",
        "    def __init__(self, vocab, targets_w, sampling_r, neg_sampling, p_drop, w2i, i2w, number_context):\n",
        "        self.vocab = vocab\n",
        "        self.targets_w = targets_w\n",
        "        self.sampling_r = sampling_r\n",
        "        self.neg_sampling = neg_sampling\n",
        "        self.pdrop = p_drop\n",
        "        self.w2i = w2i\n",
        "        self.i2w = i2w\n",
        "        self.number_context = number_context\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get word from idx\n",
        "        word = self.i2w[index]\n",
        "        # print('word: ', word)\n",
        "        # get list of contexts \n",
        "        context = self.targets_w[word]         # words (strings)\n",
        "        # print('context: ', context)\n",
        "        context_ids = [self.w2i[w] for w in context]# idxs  (ints)\n",
        "        # print('context ids: ', context_ids)\n",
        "        # convert to numpy\n",
        "        context_ids = np.asarray(context_ids)\n",
        "        # print('context ids np: ', context_ids)\n",
        "\n",
        "        # get number of elements in context: hyperparameter\n",
        "        # if len(context_ids) < self.number_context:\n",
        "        #     pad_number = self.number_context - len(context_ids)\n",
        "        #     # print([0]*pad_number)\n",
        "        #     # print(context_ids)\n",
        "        #     context_ids = np.append(context_ids, [0]*pad_number)\n",
        "        #     # print('context ids appended: ', context_ids)\n",
        "        \n",
        "\n",
        "        # elif len(context_ids) > self.number_context:\n",
        "        #     # normalize samples \n",
        "        #     sampling_r = []\n",
        "        #     for c_id in context_ids:\n",
        "        #         sampling_r.append(self.sampling_r[c_id])\n",
        "        #     sampling_r_norm = [s/sum(sampling_r) for s in sampling_r]\n",
        "\n",
        "        #     context_ids = np.random.choice(context_ids, size = self.number_context)#, p=sampling_r_norm)\n",
        "        #     # print('context ids: ', context_ids)\n",
        "        # else: pass\n",
        "        \n",
        "        context_ids = np.random.choice(context_ids, size = self.number_context)#, p=sampling_r_norm)\n",
        "        # Conver to tensor \n",
        "        # print([self.i2w[i] for i in context_ids])\n",
        "        context_ids_t = torch.from_numpy(context_ids).long()\n",
        "        # print('context_ids torch: ', context_ids_t)\n",
        "        \n",
        "        # Negative samples - copy dictionary and remove elements in context \n",
        "        # neg_d = self.neg_sampling.copy()\n",
        "        # for w in context:\n",
        "        #     del neg_d[w]\n",
        "        \n",
        "        # print('neg samping')\n",
        "        neg_samples = np.random.choice(list(self.neg_sampling.keys()), size = self.number_context, p = list(self.neg_sampling.values()))\n",
        "        # neg_samples_ids = np.asarray([self.w2i[w] for w in neg_samples if w not in context])\n",
        "        # print(neg_samples)\n",
        "        neg_samples_ids = []\n",
        "        for w in neg_samples:\n",
        "            # print(w)\n",
        "            w_id = self.w2i[w]\n",
        "            # print(w_id)\n",
        "            if w not in context:\n",
        "                # print('word: {} not in context'.format(w))\n",
        "                neg_samples_ids.append(w_id)\n",
        "        if len(neg_samples_ids) != len(neg_samples):\n",
        "            difference = len(neg_samples) - len(neg_samples_ids)\n",
        "            for i in range(difference):\n",
        "                neg_samples_ids.append(0)\n",
        "        neg_samples_ids = np.asarray(neg_samples_ids)\n",
        "        neg_samples_t = torch.from_numpy(neg_samples_ids).long()\n",
        "        assert(context_ids_t.shape == neg_samples_t.shape)\n",
        "        return index, context_ids_t, neg_samples_t\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IAZZug8nc29"
      },
      "source": [
        "s = Skipgram_data(vocab, targets, s_rate_ids, n_rate, p_drop, w2i, i2w, 5)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr7VgM9knQj8"
      },
      "source": [
        "# to test dataloader\n",
        "it = iter(s)\n",
        "for i in range(10):\n",
        "    next(it)\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjIFFRZ6w32i"
      },
      "source": [
        "glove_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/embeddings/glove.6B.50d.txt'\n",
        "with io.open(glove_path, 'r', encoding='utf8') as f:    \n",
        "    glove_file = f.read()\n",
        "    \n",
        "glove_sentences = glove_file.splitlines()\n",
        "glove_vocab = {}\n",
        "for sentence in glove_sentences:\n",
        "    word = sentence.split()[0]\n",
        "    embedding = np.array(sentence.split()[1:], dtype = float)\n",
        "    glove_vocab[word] = embedding"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSWE6xNrw327"
      },
      "source": [
        "batch_size = 64\n",
        "train_dataset = s\n",
        "train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size, drop_last=False)\n",
        "\n",
        "# test_dataset = EmbeddingDataset(test_data)\n",
        "# test_loader = DataLoader(test_dataset, shuffle = False, batch_size = batch_size)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSt8xm6j3McY"
      },
      "source": [
        "def create_embedding_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.size()\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    if non_trainable:\n",
        "      emb_layer.requires_grad = False\n",
        "    \n",
        "    return emb_layer #, num_embeddings, embedding_dim\n",
        "\n",
        "# Fill vocabulary with glove vectors, if it exists. \n",
        "weights_matrix = torch.zeros((len(w2i), 50))\n",
        "for w, id in w2i.items():\n",
        "    try:\n",
        "        weights_matrix[id] = torch.from_numpy(glove_vocab[w])\n",
        "    except KeyError:\n",
        "        weights_matrix[id] = torch.from_numpy(np.random.normal(scale=0.6, size=(50,)))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fN-3sf_w32_"
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        # self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device) Creates random weights\n",
        "        \n",
        "        self.u_embeddings = create_embedding_layer(weights_matrix, False) # init weights with glove vectors\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=False).to(device)\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        #self.u_embeddings.weight.data.uniform_(-initrange, initrange).to(device)\n",
        "        self.v_embeddings.weight.data.uniform_(-0, 0).to(device)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \n",
        "        emb_u = self.u_embeddings(pos_u) # target -> [batch, 300]\n",
        "        emb_v = self.v_embeddings(pos_v) # context -> [batch, 5, 300]\n",
        "\n",
        "        # bmm between [batch, 10, 300] x [batch, 300, 1]\n",
        "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze() # [batch, 300]\n",
        "        # score = torch.sum(score, dim=1)\n",
        "        log_target = F.logsigmoid(score) # batch size \n",
        "\n",
        "        # for negative \n",
        "        neg_emb_v = self.v_embeddings(neg_v) # neg context -> [batch, 10, 300]\n",
        "        \n",
        "        # bmm between [batch, 10, 300] x [batch, 300, 1]\n",
        "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze() # [batch, 10]\n",
        "        # neg_score = torch.sum(neg_score, dim=1)\n",
        "        sum_log_sampled = F.logsigmoid(-1*neg_score) # batch size\n",
        "\n",
        "        loss = log_target + sum_log_sampled\n",
        "        bs = emb_u.shape[0]\n",
        "        \n",
        "        # Return average batch loss \n",
        "        return -1*loss.sum()/bs\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHWq6LCJHird"
      },
      "source": [
        "def train_epoch_s(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total_predictions = 0.0\n",
        "    correct_predictions = 0.0\n",
        "    \n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for batch_idx, (target, pos, neg) in enumerate(train_loader):   \n",
        "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
        "        \n",
        "        pos = pos.cuda()  # [batch size, context_window] [2, 4]\n",
        "        target = target.cuda() # [batch size] [2]\n",
        "        neg = neg.cuda()\n",
        "        \n",
        "        loss = model(target, pos, neg)    # [batch, vocab_size]\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        # #calculating accuracy\n",
        "        # _, predicted = torch.max(log_probs.data, 1)\n",
        "        # total_predictions += target.size(0)\n",
        "        # correct_predictions += (predicted == target).sum().item()\n",
        "            \n",
        "        # #calculuating confusion matrix\n",
        "        # predictions += list(predicted.cpu().numpy())\n",
        "        # ground_truth += list(target.cpu().numpy())\n",
        "        \n",
        "        if batch_idx%50 == 0:\n",
        "            print('loss: ', loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    print('------ Training -----')\n",
        "    running_loss /= len(train_loader)\n",
        "    # acc = (correct_predictions/total_predictions)*100.0\n",
        "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
        "    # print('Training Accuracy: ', acc, '%')\n",
        "    return running_loss\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc017_2n5KA2"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "vocab_size = len(w2i)\n",
        "embedding_dimension = 50\n",
        "context_size = 10\n",
        "\n",
        "model = SkipGramModel(vocab_size, embedding_dimension)\n",
        "model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "best_loss = 1e30\n",
        "epochs = 200"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2sSVMisIwVW"
      },
      "source": [
        "def create_run_id(name):\n",
        "  run_id = str(int(time.time()))\n",
        "  if not os.path.exists('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments'):\n",
        "      os.mkdir('/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments')\n",
        "  path_name = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments/{}'.format(name)\n",
        "  os.mkdir(path_name)\n",
        "  print(\"Saving models, and predictions to {}\".format(path_name))\n",
        "  return path_name"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKYQAloew33D",
        "outputId": "41235e91-b8a0-4b9a-b0ef-2a6e825bcdc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "save_path = create_run_id('skipgram_adari_v2_adjs_{}d_{}'.format(embedding_dimension, epochs))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving models, and predictions to /content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments/skipgram_adari_v2_adjs_50d_200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G53-fuSvw33F",
        "outputId": "62f3095a-0be7-478c-c14b-a174fb26426e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    print('\\nEpoch: ', epoch)\n",
        "    train_loss = train_epoch_s(model, train_loader, criterion, optimizer)\n",
        "    # test_loss = validate_model_s(model, test_loader, criterion)\n",
        "    print('='*20)\n",
        " \n",
        "    if train_loss < best_loss:\n",
        "        best_loss = train_loss\n",
        "        print(\"Saving model, predictions and generated output for epoch \" + str(epoch) + \" with Loss: \" + str(best_loss))\n",
        "            \n",
        "        torch.save(model, save_path + '/nlp_embed_' + str(epoch+200) + '.pth')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  0\n",
            "loss:  6.931471824645996\n",
            "loss:  6.796605587005615\n",
            "loss:  6.6711835861206055\n",
            "------ Training -----\n",
            "Training Loss:  6.7054078578948975 Time:  14.658074617385864 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 0 with Loss: 6.7054078578948975\n",
            "\n",
            "Epoch:  1\n",
            "loss:  6.687984466552734\n",
            "loss:  6.32261848449707\n",
            "loss:  6.586603164672852\n",
            "------ Training -----\n",
            "Training Loss:  6.489506155757581 Time:  14.573624849319458 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 1 with Loss: 6.489506155757581\n",
            "\n",
            "Epoch:  2\n",
            "loss:  6.217268943786621\n",
            "loss:  6.5249433517456055\n",
            "loss:  6.3776631355285645\n",
            "------ Training -----\n",
            "Training Loss:  6.364770990307048 Time:  14.615103006362915 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 2 with Loss: 6.364770990307048\n",
            "\n",
            "Epoch:  3\n",
            "loss:  6.09870719909668\n",
            "loss:  6.247710227966309\n",
            "loss:  6.488022327423096\n",
            "------ Training -----\n",
            "Training Loss:  6.265937033346144 Time:  14.646153926849365 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 3 with Loss: 6.265937033346144\n",
            "\n",
            "Epoch:  4\n",
            "loss:  6.192976474761963\n",
            "loss:  6.241733551025391\n",
            "loss:  6.381614685058594\n",
            "------ Training -----\n",
            "Training Loss:  6.201773586919752 Time:  14.662542343139648 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 4 with Loss: 6.201773586919752\n",
            "\n",
            "Epoch:  5\n",
            "loss:  6.268826007843018\n",
            "loss:  6.20883321762085\n",
            "loss:  6.146601676940918\n",
            "------ Training -----\n",
            "Training Loss:  6.122232812946126 Time:  14.569133043289185 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 5 with Loss: 6.122232812946126\n",
            "\n",
            "Epoch:  6\n",
            "loss:  5.869132041931152\n",
            "loss:  6.1827712059021\n",
            "loss:  5.966464996337891\n",
            "------ Training -----\n",
            "Training Loss:  6.075505571850275 Time:  14.674061298370361 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 6 with Loss: 6.075505571850275\n",
            "\n",
            "Epoch:  7\n",
            "loss:  5.863359451293945\n",
            "loss:  6.152124404907227\n",
            "loss:  5.978968143463135\n",
            "------ Training -----\n",
            "Training Loss:  5.998897459547399 Time:  14.734417200088501 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 7 with Loss: 5.998897459547399\n",
            "\n",
            "Epoch:  8\n",
            "loss:  6.180560111999512\n",
            "loss:  5.7438249588012695\n",
            "loss:  5.744540214538574\n",
            "------ Training -----\n",
            "Training Loss:  5.969821222757889 Time:  14.629906177520752 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 8 with Loss: 5.969821222757889\n",
            "\n",
            "Epoch:  9\n",
            "loss:  5.794785499572754\n",
            "loss:  5.904926300048828\n",
            "loss:  5.70429801940918\n",
            "------ Training -----\n",
            "Training Loss:  5.878200535046852 Time:  14.667392253875732 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 9 with Loss: 5.878200535046852\n",
            "\n",
            "Epoch:  10\n",
            "loss:  5.82361364364624\n",
            "loss:  5.732377052307129\n",
            "loss:  5.955723285675049\n",
            "------ Training -----\n",
            "Training Loss:  5.817339278883853 Time:  14.675398826599121 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 10 with Loss: 5.817339278883853\n",
            "\n",
            "Epoch:  11\n",
            "loss:  5.957840919494629\n",
            "loss:  5.86776065826416\n",
            "loss:  5.689573287963867\n",
            "------ Training -----\n",
            "Training Loss:  5.774023435883603 Time:  14.45620584487915 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 11 with Loss: 5.774023435883603\n",
            "\n",
            "Epoch:  12\n",
            "loss:  5.63773775100708\n",
            "loss:  5.455101013183594\n",
            "loss:  5.703614234924316\n",
            "------ Training -----\n",
            "Training Loss:  5.744404000751043 Time:  14.500837326049805 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 12 with Loss: 5.744404000751043\n",
            "\n",
            "Epoch:  13\n",
            "loss:  5.295751571655273\n",
            "loss:  5.800901412963867\n",
            "loss:  5.987166881561279\n",
            "------ Training -----\n",
            "Training Loss:  5.665807069358179 Time:  14.325355768203735 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 13 with Loss: 5.665807069358179\n",
            "\n",
            "Epoch:  14\n",
            "loss:  5.541324615478516\n",
            "loss:  5.718867301940918\n",
            "loss:  5.420223236083984\n",
            "------ Training -----\n",
            "Training Loss:  5.654565855608148 Time:  14.228372573852539 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 14 with Loss: 5.654565855608148\n",
            "\n",
            "Epoch:  15\n",
            "loss:  5.507593154907227\n",
            "loss:  5.467683792114258\n",
            "loss:  5.403096675872803\n",
            "------ Training -----\n",
            "Training Loss:  5.586011652219093 Time:  14.350010871887207 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 15 with Loss: 5.586011652219093\n",
            "\n",
            "Epoch:  16\n",
            "loss:  5.1244049072265625\n",
            "loss:  5.397727966308594\n",
            "loss:  5.503556251525879\n",
            "------ Training -----\n",
            "Training Loss:  5.515409712064064 Time:  14.348067998886108 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 16 with Loss: 5.515409712064064\n",
            "\n",
            "Epoch:  17\n",
            "loss:  5.645674705505371\n",
            "loss:  5.644244194030762\n",
            "loss:  5.587916851043701\n",
            "------ Training -----\n",
            "Training Loss:  5.504741523225428 Time:  14.3627188205719 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 17 with Loss: 5.504741523225428\n",
            "\n",
            "Epoch:  18\n",
            "loss:  5.5103349685668945\n",
            "loss:  5.602513313293457\n",
            "loss:  5.448930740356445\n",
            "------ Training -----\n",
            "Training Loss:  5.454905000783629 Time:  14.357893943786621 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 18 with Loss: 5.454905000783629\n",
            "\n",
            "Epoch:  19\n",
            "loss:  5.617932319641113\n",
            "loss:  5.4339189529418945\n",
            "loss:  5.394279479980469\n",
            "------ Training -----\n",
            "Training Loss:  5.408447487879608 Time:  14.271963834762573 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 19 with Loss: 5.408447487879608\n",
            "\n",
            "Epoch:  20\n",
            "loss:  5.632045745849609\n",
            "loss:  5.736234664916992\n",
            "loss:  5.604439735412598\n",
            "------ Training -----\n",
            "Training Loss:  5.34456447827614 Time:  14.29891037940979 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 20 with Loss: 5.34456447827614\n",
            "\n",
            "Epoch:  21\n",
            "loss:  5.244483470916748\n",
            "loss:  5.198418617248535\n",
            "loss:  5.129214763641357\n",
            "------ Training -----\n",
            "Training Loss:  5.28090272515507 Time:  14.359725952148438 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 21 with Loss: 5.28090272515507\n",
            "\n",
            "Epoch:  22\n",
            "loss:  5.1818647384643555\n",
            "loss:  5.405428886413574\n",
            "loss:  5.014097213745117\n",
            "------ Training -----\n",
            "Training Loss:  5.276118759381569 Time:  14.10736346244812 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 22 with Loss: 5.276118759381569\n",
            "\n",
            "Epoch:  23\n",
            "loss:  5.284743309020996\n",
            "loss:  5.400058746337891\n",
            "loss:  5.147356033325195\n",
            "------ Training -----\n",
            "Training Loss:  5.236548751087512 Time:  14.175455093383789 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 23 with Loss: 5.236548751087512\n",
            "\n",
            "Epoch:  24\n",
            "loss:  4.985162258148193\n",
            "loss:  5.172135829925537\n",
            "loss:  5.049686908721924\n",
            "------ Training -----\n",
            "Training Loss:  5.180938466120574 Time:  14.182855606079102 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 24 with Loss: 5.180938466120574\n",
            "\n",
            "Epoch:  25\n",
            "loss:  5.12703275680542\n",
            "loss:  5.324005603790283\n",
            "loss:  5.1046576499938965\n",
            "------ Training -----\n",
            "Training Loss:  5.127433162624553 Time:  14.262479305267334 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 25 with Loss: 5.127433162624553\n",
            "\n",
            "Epoch:  26\n",
            "loss:  5.28732967376709\n",
            "loss:  5.193256378173828\n",
            "loss:  5.237910270690918\n",
            "------ Training -----\n",
            "Training Loss:  5.068315380710667 Time:  14.141184091567993 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 26 with Loss: 5.068315380710667\n",
            "\n",
            "Epoch:  27\n",
            "loss:  4.94410514831543\n",
            "loss:  5.127293586730957\n",
            "loss:  4.816484451293945\n",
            "------ Training -----\n",
            "Training Loss:  5.024050429715949 Time:  14.300203561782837 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 27 with Loss: 5.024050429715949\n",
            "\n",
            "Epoch:  28\n",
            "loss:  5.289270401000977\n",
            "loss:  5.164156436920166\n",
            "loss:  4.840778350830078\n",
            "------ Training -----\n",
            "Training Loss:  4.998000415705018 Time:  14.44959306716919 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 28 with Loss: 4.998000415705018\n",
            "\n",
            "Epoch:  29\n",
            "loss:  4.974837303161621\n",
            "loss:  4.880212783813477\n",
            "loss:  5.419963359832764\n",
            "------ Training -----\n",
            "Training Loss:  4.9557251485727605 Time:  14.258203744888306 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 29 with Loss: 4.9557251485727605\n",
            "\n",
            "Epoch:  30\n",
            "loss:  4.992804527282715\n",
            "loss:  4.945916175842285\n",
            "loss:  4.732708930969238\n",
            "------ Training -----\n",
            "Training Loss:  4.908584202750254 Time:  14.131226301193237 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 30 with Loss: 4.908584202750254\n",
            "\n",
            "Epoch:  31\n",
            "loss:  4.793346881866455\n",
            "loss:  4.813098430633545\n",
            "loss:  5.143041610717773\n",
            "------ Training -----\n",
            "Training Loss:  4.8798326233686025 Time:  14.133687973022461 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 31 with Loss: 4.8798326233686025\n",
            "\n",
            "Epoch:  32\n",
            "loss:  4.840654373168945\n",
            "loss:  4.8309736251831055\n",
            "loss:  4.7638702392578125\n",
            "------ Training -----\n",
            "Training Loss:  4.808363773054996 Time:  14.119215250015259 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 32 with Loss: 4.808363773054996\n",
            "\n",
            "Epoch:  33\n",
            "loss:  4.7818379402160645\n",
            "loss:  5.292686462402344\n",
            "loss:  4.7597975730896\n",
            "------ Training -----\n",
            "Training Loss:  4.79538403123112 Time:  14.092839241027832 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 33 with Loss: 4.79538403123112\n",
            "\n",
            "Epoch:  34\n",
            "loss:  5.143277168273926\n",
            "loss:  4.807826995849609\n",
            "loss:  4.690105438232422\n",
            "------ Training -----\n",
            "Training Loss:  4.742263365600069 Time:  14.132036924362183 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 34 with Loss: 4.742263365600069\n",
            "\n",
            "Epoch:  35\n",
            "loss:  4.773097991943359\n",
            "loss:  4.793092250823975\n",
            "loss:  4.770839691162109\n",
            "------ Training -----\n",
            "Training Loss:  4.685453071432598 Time:  14.06821346282959 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 35 with Loss: 4.685453071432598\n",
            "\n",
            "Epoch:  36\n",
            "loss:  4.384541988372803\n",
            "loss:  4.50469970703125\n",
            "loss:  4.45308256149292\n",
            "------ Training -----\n",
            "Training Loss:  4.610587269572888 Time:  13.991723775863647 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 36 with Loss: 4.610587269572888\n",
            "\n",
            "Epoch:  37\n",
            "loss:  4.40474796295166\n",
            "loss:  4.491789817810059\n",
            "loss:  4.421156883239746\n",
            "------ Training -----\n",
            "Training Loss:  4.587865778955362 Time:  13.970355033874512 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 37 with Loss: 4.587865778955362\n",
            "\n",
            "Epoch:  38\n",
            "loss:  4.295990467071533\n",
            "loss:  4.9779791831970215\n",
            "loss:  4.338937282562256\n",
            "------ Training -----\n",
            "Training Loss:  4.551425131700807 Time:  14.190413236618042 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 38 with Loss: 4.551425131700807\n",
            "\n",
            "Epoch:  39\n",
            "loss:  4.554285049438477\n",
            "loss:  4.407401084899902\n",
            "loss:  4.430309772491455\n",
            "------ Training -----\n",
            "Training Loss:  4.509770033723217 Time:  14.133723735809326 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 39 with Loss: 4.509770033723217\n",
            "\n",
            "Epoch:  40\n",
            "loss:  4.581488609313965\n",
            "loss:  4.507189750671387\n",
            "loss:  4.61134672164917\n",
            "------ Training -----\n",
            "Training Loss:  4.450936737707106 Time:  14.187578439712524 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 40 with Loss: 4.450936737707106\n",
            "\n",
            "Epoch:  41\n",
            "loss:  4.616714000701904\n",
            "loss:  4.419564247131348\n",
            "loss:  4.375543594360352\n",
            "------ Training -----\n",
            "Training Loss:  4.409513477551735 Time:  14.099996328353882 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 41 with Loss: 4.409513477551735\n",
            "\n",
            "Epoch:  42\n",
            "loss:  4.2465925216674805\n",
            "loss:  4.462491512298584\n",
            "loss:  4.492767333984375\n",
            "------ Training -----\n",
            "Training Loss:  4.376211804858709 Time:  14.11814832687378 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 42 with Loss: 4.376211804858709\n",
            "\n",
            "Epoch:  43\n",
            "loss:  4.081082344055176\n",
            "loss:  4.270503044128418\n",
            "loss:  4.173218727111816\n",
            "------ Training -----\n",
            "Training Loss:  4.344145784943791 Time:  14.081825733184814 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 43 with Loss: 4.344145784943791\n",
            "\n",
            "Epoch:  44\n",
            "loss:  3.992436408996582\n",
            "loss:  4.156070709228516\n",
            "loss:  4.176201820373535\n",
            "------ Training -----\n",
            "Training Loss:  4.324419300434953 Time:  14.315980911254883 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 44 with Loss: 4.324419300434953\n",
            "\n",
            "Epoch:  45\n",
            "loss:  3.6732683181762695\n",
            "loss:  4.0754194259643555\n",
            "loss:  4.276248455047607\n",
            "------ Training -----\n",
            "Training Loss:  4.252420512296386 Time:  14.249255418777466 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 45 with Loss: 4.252420512296386\n",
            "\n",
            "Epoch:  46\n",
            "loss:  4.220763683319092\n",
            "loss:  4.160120010375977\n",
            "loss:  4.331888198852539\n",
            "------ Training -----\n",
            "Training Loss:  4.1980997020915405 Time:  14.224332571029663 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 46 with Loss: 4.1980997020915405\n",
            "\n",
            "Epoch:  47\n",
            "loss:  4.02365779876709\n",
            "loss:  4.404416084289551\n",
            "loss:  4.164426803588867\n",
            "------ Training -----\n",
            "Training Loss:  4.141409253669997 Time:  14.3603196144104 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 47 with Loss: 4.141409253669997\n",
            "\n",
            "Epoch:  48\n",
            "loss:  3.896847724914551\n",
            "loss:  4.166007041931152\n",
            "loss:  4.054254531860352\n",
            "------ Training -----\n",
            "Training Loss:  4.131496267803644 Time:  14.27246642112732 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 48 with Loss: 4.131496267803644\n",
            "\n",
            "Epoch:  49\n",
            "loss:  3.792140007019043\n",
            "loss:  3.724618673324585\n",
            "loss:  4.178394317626953\n",
            "------ Training -----\n",
            "Training Loss:  4.075957849874335 Time:  14.434641599655151 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 49 with Loss: 4.075957849874335\n",
            "\n",
            "Epoch:  50\n",
            "loss:  4.2295451164245605\n",
            "loss:  4.314759254455566\n",
            "loss:  4.015985012054443\n",
            "------ Training -----\n",
            "Training Loss:  4.029742135839947 Time:  14.537796020507812 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 50 with Loss: 4.029742135839947\n",
            "\n",
            "Epoch:  51\n",
            "loss:  3.9424424171447754\n",
            "loss:  3.828350067138672\n",
            "loss:  3.989332675933838\n",
            "------ Training -----\n",
            "Training Loss:  3.9835994890180686 Time:  14.300005197525024 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 51 with Loss: 3.9835994890180686\n",
            "\n",
            "Epoch:  52\n",
            "loss:  3.8807921409606934\n",
            "loss:  3.8396129608154297\n",
            "loss:  4.006141662597656\n",
            "------ Training -----\n",
            "Training Loss:  3.948104828090991 Time:  14.317980527877808 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 52 with Loss: 3.948104828090991\n",
            "\n",
            "Epoch:  53\n",
            "loss:  3.8403191566467285\n",
            "loss:  4.049731254577637\n",
            "loss:  3.909254789352417\n",
            "------ Training -----\n",
            "Training Loss:  3.9068537566621426 Time:  14.11781930923462 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 53 with Loss: 3.9068537566621426\n",
            "\n",
            "Epoch:  54\n",
            "loss:  3.826916217803955\n",
            "loss:  3.8873777389526367\n",
            "loss:  3.65274715423584\n",
            "------ Training -----\n",
            "Training Loss:  3.8454355789443193 Time:  14.03350281715393 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 54 with Loss: 3.8454355789443193\n",
            "\n",
            "Epoch:  55\n",
            "loss:  3.4722185134887695\n",
            "loss:  3.801654815673828\n",
            "loss:  4.271238327026367\n",
            "------ Training -----\n",
            "Training Loss:  3.833949321407383 Time:  14.188069105148315 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 55 with Loss: 3.833949321407383\n",
            "\n",
            "Epoch:  56\n",
            "loss:  4.004080295562744\n",
            "loss:  3.4517388343811035\n",
            "loss:  3.6368770599365234\n",
            "------ Training -----\n",
            "Training Loss:  3.7807586092059897 Time:  14.014046669006348 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 56 with Loss: 3.7807586092059897\n",
            "\n",
            "Epoch:  57\n",
            "loss:  3.705890655517578\n",
            "loss:  3.6898529529571533\n",
            "loss:  3.7477211952209473\n",
            "------ Training -----\n",
            "Training Loss:  3.792684270163714 Time:  14.061007499694824 s\n",
            "====================\n",
            "\n",
            "Epoch:  58\n",
            "loss:  3.4433348178863525\n",
            "loss:  3.8613219261169434\n",
            "loss:  3.783529281616211\n",
            "------ Training -----\n",
            "Training Loss:  3.7178230891793462 Time:  14.115658521652222 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 58 with Loss: 3.7178230891793462\n",
            "\n",
            "Epoch:  59\n",
            "loss:  3.771949291229248\n",
            "loss:  3.6902756690979004\n",
            "loss:  3.584027051925659\n",
            "------ Training -----\n",
            "Training Loss:  3.687413159063307 Time:  13.884758472442627 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 59 with Loss: 3.687413159063307\n",
            "\n",
            "Epoch:  60\n",
            "loss:  3.6310043334960938\n",
            "loss:  3.7741336822509766\n",
            "loss:  3.9543659687042236\n",
            "------ Training -----\n",
            "Training Loss:  3.6531860969834407 Time:  14.103752374649048 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 60 with Loss: 3.6531860969834407\n",
            "\n",
            "Epoch:  61\n",
            "loss:  3.6929452419281006\n",
            "loss:  3.6249687671661377\n",
            "loss:  3.5307092666625977\n",
            "------ Training -----\n",
            "Training Loss:  3.6582597675970043 Time:  14.154436588287354 s\n",
            "====================\n",
            "\n",
            "Epoch:  62\n",
            "loss:  3.837474822998047\n",
            "loss:  3.7608706951141357\n",
            "loss:  3.1387314796447754\n",
            "------ Training -----\n",
            "Training Loss:  3.5895117379851262 Time:  14.08395504951477 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 62 with Loss: 3.5895117379851262\n",
            "\n",
            "Epoch:  63\n",
            "loss:  3.6549854278564453\n",
            "loss:  3.6404378414154053\n",
            "loss:  3.3870925903320312\n",
            "------ Training -----\n",
            "Training Loss:  3.5457879022016363 Time:  14.037657022476196 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 63 with Loss: 3.5457879022016363\n",
            "\n",
            "Epoch:  64\n",
            "loss:  3.485755443572998\n",
            "loss:  3.430912494659424\n",
            "loss:  3.3644371032714844\n",
            "------ Training -----\n",
            "Training Loss:  3.5360838679944053 Time:  14.061623334884644 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 64 with Loss: 3.5360838679944053\n",
            "\n",
            "Epoch:  65\n",
            "loss:  2.9747328758239746\n",
            "loss:  3.348723888397217\n",
            "loss:  3.863950252532959\n",
            "------ Training -----\n",
            "Training Loss:  3.5219176785420565 Time:  14.21775770187378 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 65 with Loss: 3.5219176785420565\n",
            "\n",
            "Epoch:  66\n",
            "loss:  3.45408296585083\n",
            "loss:  3.592867612838745\n",
            "loss:  3.3563742637634277\n",
            "------ Training -----\n",
            "Training Loss:  3.494045778856439 Time:  14.214989423751831 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 66 with Loss: 3.494045778856439\n",
            "\n",
            "Epoch:  67\n",
            "loss:  3.465471029281616\n",
            "loss:  3.2363529205322266\n",
            "loss:  3.6303768157958984\n",
            "------ Training -----\n",
            "Training Loss:  3.4414652707213063 Time:  14.15230131149292 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 67 with Loss: 3.4414652707213063\n",
            "\n",
            "Epoch:  68\n",
            "loss:  3.3209547996520996\n",
            "loss:  3.470113754272461\n",
            "loss:  3.633226156234741\n",
            "------ Training -----\n",
            "Training Loss:  3.4049823405378956 Time:  14.010907888412476 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 68 with Loss: 3.4049823405378956\n",
            "\n",
            "Epoch:  69\n",
            "loss:  3.8046650886535645\n",
            "loss:  3.7738351821899414\n",
            "loss:  3.8114259243011475\n",
            "------ Training -----\n",
            "Training Loss:  3.4247263084023687 Time:  14.030636310577393 s\n",
            "====================\n",
            "\n",
            "Epoch:  70\n",
            "loss:  3.528120517730713\n",
            "loss:  3.1926794052124023\n",
            "loss:  3.4045932292938232\n",
            "------ Training -----\n",
            "Training Loss:  3.358696937561035 Time:  14.034227848052979 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 70 with Loss: 3.358696937561035\n",
            "\n",
            "Epoch:  71\n",
            "loss:  3.2497129440307617\n",
            "loss:  3.293148994445801\n",
            "loss:  3.0379467010498047\n",
            "------ Training -----\n",
            "Training Loss:  3.365838753975044 Time:  14.137813568115234 s\n",
            "====================\n",
            "\n",
            "Epoch:  72\n",
            "loss:  2.861967086791992\n",
            "loss:  3.2143235206604004\n",
            "loss:  3.423546314239502\n",
            "------ Training -----\n",
            "Training Loss:  3.315044952651202 Time:  14.49242377281189 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 72 with Loss: 3.315044952651202\n",
            "\n",
            "Epoch:  73\n",
            "loss:  3.1558542251586914\n",
            "loss:  3.0631704330444336\n",
            "loss:  3.3567256927490234\n",
            "------ Training -----\n",
            "Training Loss:  3.2875507686097745 Time:  14.116718530654907 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 73 with Loss: 3.2875507686097745\n",
            "\n",
            "Epoch:  74\n",
            "loss:  3.0301003456115723\n",
            "loss:  3.0392675399780273\n",
            "loss:  3.420653820037842\n",
            "------ Training -----\n",
            "Training Loss:  3.2726722632424305 Time:  13.969494581222534 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 74 with Loss: 3.2726722632424305\n",
            "\n",
            "Epoch:  75\n",
            "loss:  3.3138020038604736\n",
            "loss:  3.4851114749908447\n",
            "loss:  3.5205154418945312\n",
            "------ Training -----\n",
            "Training Loss:  3.2508380008956133 Time:  14.038956880569458 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 75 with Loss: 3.2508380008956133\n",
            "\n",
            "Epoch:  76\n",
            "loss:  3.4046406745910645\n",
            "loss:  3.1812422275543213\n",
            "loss:  3.131948947906494\n",
            "------ Training -----\n",
            "Training Loss:  3.2493011022018172 Time:  14.348878622055054 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 76 with Loss: 3.2493011022018172\n",
            "\n",
            "Epoch:  77\n",
            "loss:  3.2153079509735107\n",
            "loss:  3.4240059852600098\n",
            "loss:  3.502027750015259\n",
            "------ Training -----\n",
            "Training Loss:  3.196559156401683 Time:  14.238516330718994 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 77 with Loss: 3.196559156401683\n",
            "\n",
            "Epoch:  78\n",
            "loss:  3.3223671913146973\n",
            "loss:  2.8813812732696533\n",
            "loss:  3.10683274269104\n",
            "------ Training -----\n",
            "Training Loss:  3.207915154554076 Time:  14.293789863586426 s\n",
            "====================\n",
            "\n",
            "Epoch:  79\n",
            "loss:  3.131753921508789\n",
            "loss:  2.9835755825042725\n",
            "loss:  3.604501247406006\n",
            "------ Training -----\n",
            "Training Loss:  3.1362745741666376 Time:  14.201484680175781 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 79 with Loss: 3.1362745741666376\n",
            "\n",
            "Epoch:  80\n",
            "loss:  2.9968385696411133\n",
            "loss:  3.12005877494812\n",
            "loss:  3.2418484687805176\n",
            "------ Training -----\n",
            "Training Loss:  3.1393877934601346 Time:  14.07538652420044 s\n",
            "====================\n",
            "\n",
            "Epoch:  81\n",
            "loss:  3.2300379276275635\n",
            "loss:  2.9281582832336426\n",
            "loss:  2.69156813621521\n",
            "------ Training -----\n",
            "Training Loss:  3.115650138612521 Time:  14.136388301849365 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 81 with Loss: 3.115650138612521\n",
            "\n",
            "Epoch:  82\n",
            "loss:  3.2855262756347656\n",
            "loss:  3.2585537433624268\n",
            "loss:  3.1871793270111084\n",
            "------ Training -----\n",
            "Training Loss:  3.0886593976263272 Time:  14.309078454971313 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 82 with Loss: 3.0886593976263272\n",
            "\n",
            "Epoch:  83\n",
            "loss:  2.9990508556365967\n",
            "loss:  3.1683127880096436\n",
            "loss:  3.0717036724090576\n",
            "------ Training -----\n",
            "Training Loss:  3.0668359388739375 Time:  14.224594354629517 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 83 with Loss: 3.0668359388739375\n",
            "\n",
            "Epoch:  84\n",
            "loss:  2.7827811241149902\n",
            "loss:  3.0249438285827637\n",
            "loss:  3.168447494506836\n",
            "------ Training -----\n",
            "Training Loss:  3.0552819017636574 Time:  14.160345315933228 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 84 with Loss: 3.0552819017636574\n",
            "\n",
            "Epoch:  85\n",
            "loss:  2.9875905513763428\n",
            "loss:  2.999312162399292\n",
            "loss:  2.7277305126190186\n",
            "------ Training -----\n",
            "Training Loss:  3.0282784700393677 Time:  14.309906482696533 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 85 with Loss: 3.0282784700393677\n",
            "\n",
            "Epoch:  86\n",
            "loss:  2.756448984146118\n",
            "loss:  2.987016201019287\n",
            "loss:  2.683257579803467\n",
            "------ Training -----\n",
            "Training Loss:  3.0432727680367937 Time:  14.208732604980469 s\n",
            "====================\n",
            "\n",
            "Epoch:  87\n",
            "loss:  3.1385889053344727\n",
            "loss:  2.7848258018493652\n",
            "loss:  2.7693538665771484\n",
            "------ Training -----\n",
            "Training Loss:  2.9812486838486234 Time:  14.058004379272461 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 87 with Loss: 2.9812486838486234\n",
            "\n",
            "Epoch:  88\n",
            "loss:  3.119032859802246\n",
            "loss:  3.017951011657715\n",
            "loss:  2.7773518562316895\n",
            "------ Training -----\n",
            "Training Loss:  2.966803960880991 Time:  14.107792615890503 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 88 with Loss: 2.966803960880991\n",
            "\n",
            "Epoch:  89\n",
            "loss:  2.332906723022461\n",
            "loss:  3.061941623687744\n",
            "loss:  2.8497986793518066\n",
            "------ Training -----\n",
            "Training Loss:  2.973241379705526 Time:  14.131425857543945 s\n",
            "====================\n",
            "\n",
            "Epoch:  90\n",
            "loss:  2.9260191917419434\n",
            "loss:  3.005204200744629\n",
            "loss:  3.301225423812866\n",
            "------ Training -----\n",
            "Training Loss:  2.9725546917672885 Time:  14.01307487487793 s\n",
            "====================\n",
            "\n",
            "Epoch:  91\n",
            "loss:  3.194769859313965\n",
            "loss:  2.9182000160217285\n",
            "loss:  2.6186511516571045\n",
            "------ Training -----\n",
            "Training Loss:  2.897765802124799 Time:  13.877245426177979 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 91 with Loss: 2.897765802124799\n",
            "\n",
            "Epoch:  92\n",
            "loss:  2.8337883949279785\n",
            "loss:  3.0104146003723145\n",
            "loss:  3.1511127948760986\n",
            "------ Training -----\n",
            "Training Loss:  2.922127299389597 Time:  13.951640605926514 s\n",
            "====================\n",
            "\n",
            "Epoch:  93\n",
            "loss:  3.593043088912964\n",
            "loss:  2.9149882793426514\n",
            "loss:  3.3228063583374023\n",
            "------ Training -----\n",
            "Training Loss:  2.8915261777780823 Time:  13.944915056228638 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 93 with Loss: 2.8915261777780823\n",
            "\n",
            "Epoch:  94\n",
            "loss:  2.8625850677490234\n",
            "loss:  3.2510976791381836\n",
            "loss:  2.8018288612365723\n",
            "------ Training -----\n",
            "Training Loss:  2.852432513641099 Time:  14.062072515487671 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 94 with Loss: 2.852432513641099\n",
            "\n",
            "Epoch:  95\n",
            "loss:  2.6891348361968994\n",
            "loss:  2.8130998611450195\n",
            "loss:  2.7900898456573486\n",
            "------ Training -----\n",
            "Training Loss:  2.848186563637297 Time:  13.896533489227295 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 95 with Loss: 2.848186563637297\n",
            "\n",
            "Epoch:  96\n",
            "loss:  3.1388044357299805\n",
            "loss:  2.812614917755127\n",
            "loss:  2.6575822830200195\n",
            "------ Training -----\n",
            "Training Loss:  2.8326634330264593 Time:  14.160280704498291 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 96 with Loss: 2.8326634330264593\n",
            "\n",
            "Epoch:  97\n",
            "loss:  2.829061508178711\n",
            "loss:  2.8072304725646973\n",
            "loss:  2.256791114807129\n",
            "------ Training -----\n",
            "Training Loss:  2.828043264857793 Time:  14.057404279708862 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 97 with Loss: 2.828043264857793\n",
            "\n",
            "Epoch:  98\n",
            "loss:  2.784477472305298\n",
            "loss:  2.6066126823425293\n",
            "loss:  2.7405834197998047\n",
            "------ Training -----\n",
            "Training Loss:  2.8224834850278953 Time:  14.064517736434937 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 98 with Loss: 2.8224834850278953\n",
            "\n",
            "Epoch:  99\n",
            "loss:  2.5444936752319336\n",
            "loss:  2.7167396545410156\n",
            "loss:  2.756960391998291\n",
            "------ Training -----\n",
            "Training Loss:  2.788377080933522 Time:  13.996903657913208 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 99 with Loss: 2.788377080933522\n",
            "\n",
            "Epoch:  100\n",
            "loss:  2.7153053283691406\n",
            "loss:  2.6055150032043457\n",
            "loss:  2.7196929454803467\n",
            "------ Training -----\n",
            "Training Loss:  2.802595922502421 Time:  14.03675651550293 s\n",
            "====================\n",
            "\n",
            "Epoch:  101\n",
            "loss:  2.56893253326416\n",
            "loss:  2.5772738456726074\n",
            "loss:  2.557720184326172\n",
            "------ Training -----\n",
            "Training Loss:  2.78648460719545 Time:  13.989786386489868 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 101 with Loss: 2.78648460719545\n",
            "\n",
            "Epoch:  102\n",
            "loss:  2.4488658905029297\n",
            "loss:  2.974682331085205\n",
            "loss:  2.5928189754486084\n",
            "------ Training -----\n",
            "Training Loss:  2.753814945786686 Time:  13.831592559814453 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 102 with Loss: 2.753814945786686\n",
            "\n",
            "Epoch:  103\n",
            "loss:  2.683931589126587\n",
            "loss:  2.89133358001709\n",
            "loss:  2.661529541015625\n",
            "------ Training -----\n",
            "Training Loss:  2.723784499249216 Time:  14.001457452774048 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 103 with Loss: 2.723784499249216\n",
            "\n",
            "Epoch:  104\n",
            "loss:  2.8633575439453125\n",
            "loss:  2.4997847080230713\n",
            "loss:  2.979172706604004\n",
            "------ Training -----\n",
            "Training Loss:  2.733039289207782 Time:  13.934127569198608 s\n",
            "====================\n",
            "\n",
            "Epoch:  105\n",
            "loss:  2.6917359828948975\n",
            "loss:  2.484050750732422\n",
            "loss:  2.5655527114868164\n",
            "------ Training -----\n",
            "Training Loss:  2.7082718166254334 Time:  13.872222423553467 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 105 with Loss: 2.7082718166254334\n",
            "\n",
            "Epoch:  106\n",
            "loss:  2.3583991527557373\n",
            "loss:  2.969883918762207\n",
            "loss:  2.8168745040893555\n",
            "------ Training -----\n",
            "Training Loss:  2.729422068191787 Time:  14.157366275787354 s\n",
            "====================\n",
            "\n",
            "Epoch:  107\n",
            "loss:  2.495774984359741\n",
            "loss:  2.7286252975463867\n",
            "loss:  2.6077494621276855\n",
            "------ Training -----\n",
            "Training Loss:  2.693911150350409 Time:  14.032177448272705 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 107 with Loss: 2.693911150350409\n",
            "\n",
            "Epoch:  108\n",
            "loss:  2.6416244506835938\n",
            "loss:  2.416158676147461\n",
            "loss:  2.7562644481658936\n",
            "------ Training -----\n",
            "Training Loss:  2.692044157092854 Time:  13.998868703842163 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 108 with Loss: 2.692044157092854\n",
            "\n",
            "Epoch:  109\n",
            "loss:  2.3711893558502197\n",
            "loss:  2.67124080657959\n",
            "loss:  2.5896670818328857\n",
            "------ Training -----\n",
            "Training Loss:  2.662264003592022 Time:  13.954791069030762 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 109 with Loss: 2.662264003592022\n",
            "\n",
            "Epoch:  110\n",
            "loss:  2.617659330368042\n",
            "loss:  2.6819283962249756\n",
            "loss:  3.2722063064575195\n",
            "------ Training -----\n",
            "Training Loss:  2.6891535399323803 Time:  13.990974187850952 s\n",
            "====================\n",
            "\n",
            "Epoch:  111\n",
            "loss:  2.9051051139831543\n",
            "loss:  2.977428913116455\n",
            "loss:  2.6010732650756836\n",
            "------ Training -----\n",
            "Training Loss:  2.658282752764427 Time:  13.888441562652588 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 111 with Loss: 2.658282752764427\n",
            "\n",
            "Epoch:  112\n",
            "loss:  2.941465377807617\n",
            "loss:  2.2015204429626465\n",
            "loss:  2.2536520957946777\n",
            "------ Training -----\n",
            "Training Loss:  2.6180087974516013 Time:  13.977668523788452 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 112 with Loss: 2.6180087974516013\n",
            "\n",
            "Epoch:  113\n",
            "loss:  2.5996789932250977\n",
            "loss:  2.7015435695648193\n",
            "loss:  2.786938428878784\n",
            "------ Training -----\n",
            "Training Loss:  2.6173413739366045 Time:  13.972838401794434 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 113 with Loss: 2.6173413739366045\n",
            "\n",
            "Epoch:  114\n",
            "loss:  2.4611001014709473\n",
            "loss:  2.802690029144287\n",
            "loss:  2.6961188316345215\n",
            "------ Training -----\n",
            "Training Loss:  2.601256016957558 Time:  13.981414794921875 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 114 with Loss: 2.601256016957558\n",
            "\n",
            "Epoch:  115\n",
            "loss:  2.5303425788879395\n",
            "loss:  2.6807003021240234\n",
            "loss:  2.4525444507598877\n",
            "------ Training -----\n",
            "Training Loss:  2.611706009355642 Time:  14.143992185592651 s\n",
            "====================\n",
            "\n",
            "Epoch:  116\n",
            "loss:  2.528019428253174\n",
            "loss:  2.6557846069335938\n",
            "loss:  2.980828046798706\n",
            "------ Training -----\n",
            "Training Loss:  2.601088718842652 Time:  14.165859699249268 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 116 with Loss: 2.601088718842652\n",
            "\n",
            "Epoch:  117\n",
            "loss:  2.291682720184326\n",
            "loss:  2.467193603515625\n",
            "loss:  2.1163673400878906\n",
            "------ Training -----\n",
            "Training Loss:  2.571156661389238 Time:  13.908275842666626 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 117 with Loss: 2.571156661389238\n",
            "\n",
            "Epoch:  118\n",
            "loss:  2.6451926231384277\n",
            "loss:  2.630239486694336\n",
            "loss:  2.5011134147644043\n",
            "------ Training -----\n",
            "Training Loss:  2.5178556017956493 Time:  13.866191625595093 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 118 with Loss: 2.5178556017956493\n",
            "\n",
            "Epoch:  119\n",
            "loss:  2.4857177734375\n",
            "loss:  2.6304945945739746\n",
            "loss:  2.2517099380493164\n",
            "------ Training -----\n",
            "Training Loss:  2.545502726304329 Time:  13.839565992355347 s\n",
            "====================\n",
            "\n",
            "Epoch:  120\n",
            "loss:  2.222911834716797\n",
            "loss:  2.5778560638427734\n",
            "loss:  2.326260805130005\n",
            "------ Training -----\n",
            "Training Loss:  2.5237490332732766 Time:  13.780370950698853 s\n",
            "====================\n",
            "\n",
            "Epoch:  121\n",
            "loss:  2.402236223220825\n",
            "loss:  2.5235626697540283\n",
            "loss:  2.581566095352173\n",
            "------ Training -----\n",
            "Training Loss:  2.5400693446902904 Time:  13.807398796081543 s\n",
            "====================\n",
            "\n",
            "Epoch:  122\n",
            "loss:  2.209505796432495\n",
            "loss:  2.5978426933288574\n",
            "loss:  2.6692240238189697\n",
            "------ Training -----\n",
            "Training Loss:  2.534101654917507 Time:  13.78281283378601 s\n",
            "====================\n",
            "\n",
            "Epoch:  123\n",
            "loss:  2.595280170440674\n",
            "loss:  2.1862175464630127\n",
            "loss:  2.8044443130493164\n",
            "------ Training -----\n",
            "Training Loss:  2.533794635433262 Time:  13.910841941833496 s\n",
            "====================\n",
            "\n",
            "Epoch:  124\n",
            "loss:  2.6015968322753906\n",
            "loss:  2.7624757289886475\n",
            "loss:  2.6214847564697266\n",
            "------ Training -----\n",
            "Training Loss:  2.5120955523798023 Time:  13.897024393081665 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 124 with Loss: 2.5120955523798023\n",
            "\n",
            "Epoch:  125\n",
            "loss:  2.1225123405456543\n",
            "loss:  2.7836155891418457\n",
            "loss:  2.752150535583496\n",
            "------ Training -----\n",
            "Training Loss:  2.4921727827039817 Time:  13.877809047698975 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 125 with Loss: 2.4921727827039817\n",
            "\n",
            "Epoch:  126\n",
            "loss:  2.4243531227111816\n",
            "loss:  2.981441020965576\n",
            "loss:  2.2128493785858154\n",
            "------ Training -----\n",
            "Training Loss:  2.4808577268810597 Time:  13.78619909286499 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 126 with Loss: 2.4808577268810597\n",
            "\n",
            "Epoch:  127\n",
            "loss:  2.0076217651367188\n",
            "loss:  2.339613437652588\n",
            "loss:  2.5835680961608887\n",
            "------ Training -----\n",
            "Training Loss:  2.4811351016416388 Time:  13.717718362808228 s\n",
            "====================\n",
            "\n",
            "Epoch:  128\n",
            "loss:  2.2905123233795166\n",
            "loss:  2.0611557960510254\n",
            "loss:  2.871403694152832\n",
            "------ Training -----\n",
            "Training Loss:  2.4610063302314886 Time:  13.70507025718689 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 128 with Loss: 2.4610063302314886\n",
            "\n",
            "Epoch:  129\n",
            "loss:  1.9347776174545288\n",
            "loss:  2.3871660232543945\n",
            "loss:  2.449023723602295\n",
            "------ Training -----\n",
            "Training Loss:  2.454892116077876 Time:  13.926096200942993 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 129 with Loss: 2.454892116077876\n",
            "\n",
            "Epoch:  130\n",
            "loss:  2.5231730937957764\n",
            "loss:  2.6159920692443848\n",
            "loss:  2.643470048904419\n",
            "------ Training -----\n",
            "Training Loss:  2.4393190519284396 Time:  13.893516540527344 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 130 with Loss: 2.4393190519284396\n",
            "\n",
            "Epoch:  131\n",
            "loss:  2.408261775970459\n",
            "loss:  2.711484432220459\n",
            "loss:  2.129415512084961\n",
            "------ Training -----\n",
            "Training Loss:  2.428519181275772 Time:  13.838573217391968 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 131 with Loss: 2.428519181275772\n",
            "\n",
            "Epoch:  132\n",
            "loss:  2.150846004486084\n",
            "loss:  2.6533398628234863\n",
            "loss:  2.7218871116638184\n",
            "------ Training -----\n",
            "Training Loss:  2.392071961346319 Time:  13.966511964797974 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 132 with Loss: 2.392071961346319\n",
            "\n",
            "Epoch:  133\n",
            "loss:  2.4102556705474854\n",
            "loss:  2.3958911895751953\n",
            "loss:  2.1608448028564453\n",
            "------ Training -----\n",
            "Training Loss:  2.411102402008186 Time:  13.821249723434448 s\n",
            "====================\n",
            "\n",
            "Epoch:  134\n",
            "loss:  2.3167972564697266\n",
            "loss:  2.7577192783355713\n",
            "loss:  2.1951942443847656\n",
            "------ Training -----\n",
            "Training Loss:  2.3835438227249406 Time:  13.911143064498901 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 134 with Loss: 2.3835438227249406\n",
            "\n",
            "Epoch:  135\n",
            "loss:  2.7728629112243652\n",
            "loss:  2.7506113052368164\n",
            "loss:  2.354632616043091\n",
            "------ Training -----\n",
            "Training Loss:  2.443862293736409 Time:  13.907907724380493 s\n",
            "====================\n",
            "\n",
            "Epoch:  136\n",
            "loss:  2.299924850463867\n",
            "loss:  2.538479804992676\n",
            "loss:  1.9024007320404053\n",
            "------ Training -----\n",
            "Training Loss:  2.3863652601080427 Time:  13.91759991645813 s\n",
            "====================\n",
            "\n",
            "Epoch:  137\n",
            "loss:  2.440802574157715\n",
            "loss:  2.3993959426879883\n",
            "loss:  2.6762194633483887\n",
            "------ Training -----\n",
            "Training Loss:  2.38151862257618 Time:  13.921255826950073 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 137 with Loss: 2.38151862257618\n",
            "\n",
            "Epoch:  138\n",
            "loss:  2.2919254302978516\n",
            "loss:  1.9179518222808838\n",
            "loss:  2.2459144592285156\n",
            "------ Training -----\n",
            "Training Loss:  2.3834641818272866 Time:  14.012007713317871 s\n",
            "====================\n",
            "\n",
            "Epoch:  139\n",
            "loss:  2.309190511703491\n",
            "loss:  2.2609846591949463\n",
            "loss:  2.4443252086639404\n",
            "------ Training -----\n",
            "Training Loss:  2.359023760941069 Time:  13.780597448348999 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 139 with Loss: 2.359023760941069\n",
            "\n",
            "Epoch:  140\n",
            "loss:  2.095433235168457\n",
            "loss:  2.023314952850342\n",
            "loss:  2.3927366733551025\n",
            "------ Training -----\n",
            "Training Loss:  2.3546141628491677 Time:  13.874601125717163 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 140 with Loss: 2.3546141628491677\n",
            "\n",
            "Epoch:  141\n",
            "loss:  2.4524433612823486\n",
            "loss:  2.2319746017456055\n",
            "loss:  1.8579035997390747\n",
            "------ Training -----\n",
            "Training Loss:  2.342764604899843 Time:  13.9939866065979 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 141 with Loss: 2.342764604899843\n",
            "\n",
            "Epoch:  142\n",
            "loss:  2.2102415561676025\n",
            "loss:  1.9195027351379395\n",
            "loss:  2.3399784564971924\n",
            "------ Training -----\n",
            "Training Loss:  2.345604171187191 Time:  13.886322975158691 s\n",
            "====================\n",
            "\n",
            "Epoch:  143\n",
            "loss:  2.4842276573181152\n",
            "loss:  2.301008939743042\n",
            "loss:  2.1708266735076904\n",
            "------ Training -----\n",
            "Training Loss:  2.358805384676335 Time:  13.952470302581787 s\n",
            "====================\n",
            "\n",
            "Epoch:  144\n",
            "loss:  2.3124196529388428\n",
            "loss:  2.04166316986084\n",
            "loss:  2.0829663276672363\n",
            "------ Training -----\n",
            "Training Loss:  2.323849631568133 Time:  13.987333059310913 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 144 with Loss: 2.323849631568133\n",
            "\n",
            "Epoch:  145\n",
            "loss:  2.176264762878418\n",
            "loss:  2.408310890197754\n",
            "loss:  2.1294054985046387\n",
            "------ Training -----\n",
            "Training Loss:  2.2949938016422724 Time:  13.95252799987793 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 145 with Loss: 2.2949938016422724\n",
            "\n",
            "Epoch:  146\n",
            "loss:  2.4655680656433105\n",
            "loss:  2.3684134483337402\n",
            "loss:  2.202610969543457\n",
            "------ Training -----\n",
            "Training Loss:  2.32517814333156 Time:  13.808079957962036 s\n",
            "====================\n",
            "\n",
            "Epoch:  147\n",
            "loss:  2.516796588897705\n",
            "loss:  1.948751449584961\n",
            "loss:  1.9446799755096436\n",
            "------ Training -----\n",
            "Training Loss:  2.3064986760333435 Time:  13.950631141662598 s\n",
            "====================\n",
            "\n",
            "Epoch:  148\n",
            "loss:  2.417795181274414\n",
            "loss:  1.7848840951919556\n",
            "loss:  2.3626976013183594\n",
            "------ Training -----\n",
            "Training Loss:  2.3194008277634444 Time:  13.984395503997803 s\n",
            "====================\n",
            "\n",
            "Epoch:  149\n",
            "loss:  2.4387500286102295\n",
            "loss:  2.3610129356384277\n",
            "loss:  2.1144227981567383\n",
            "------ Training -----\n",
            "Training Loss:  2.280356166726452 Time:  13.778548002243042 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 149 with Loss: 2.280356166726452\n",
            "\n",
            "Epoch:  150\n",
            "loss:  2.4827327728271484\n",
            "loss:  2.14555025100708\n",
            "loss:  2.060840129852295\n",
            "------ Training -----\n",
            "Training Loss:  2.285134788286888 Time:  13.834396839141846 s\n",
            "====================\n",
            "\n",
            "Epoch:  151\n",
            "loss:  2.178252935409546\n",
            "loss:  2.8582887649536133\n",
            "loss:  2.4211976528167725\n",
            "------ Training -----\n",
            "Training Loss:  2.2850535714020164 Time:  13.844781160354614 s\n",
            "====================\n",
            "\n",
            "Epoch:  152\n",
            "loss:  2.298476219177246\n",
            "loss:  2.231374740600586\n",
            "loss:  2.381295919418335\n",
            "------ Training -----\n",
            "Training Loss:  2.254165625673229 Time:  13.938289403915405 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 152 with Loss: 2.254165625673229\n",
            "\n",
            "Epoch:  153\n",
            "loss:  2.5431180000305176\n",
            "loss:  2.2444870471954346\n",
            "loss:  2.367860794067383\n",
            "------ Training -----\n",
            "Training Loss:  2.2491259615300065 Time:  13.977934837341309 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 153 with Loss: 2.2491259615300065\n",
            "\n",
            "Epoch:  154\n",
            "loss:  1.983025074005127\n",
            "loss:  2.179419994354248\n",
            "loss:  2.585561752319336\n",
            "------ Training -----\n",
            "Training Loss:  2.22636287697291 Time:  13.742204427719116 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 154 with Loss: 2.22636287697291\n",
            "\n",
            "Epoch:  155\n",
            "loss:  1.9477152824401855\n",
            "loss:  2.551723003387451\n",
            "loss:  2.072986364364624\n",
            "------ Training -----\n",
            "Training Loss:  2.2762609144388617 Time:  13.75686526298523 s\n",
            "====================\n",
            "\n",
            "Epoch:  156\n",
            "loss:  2.290367603302002\n",
            "loss:  2.245420455932617\n",
            "loss:  2.2378005981445312\n",
            "------ Training -----\n",
            "Training Loss:  2.1945084121267673 Time:  13.697216033935547 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 156 with Loss: 2.1945084121267673\n",
            "\n",
            "Epoch:  157\n",
            "loss:  2.151998996734619\n",
            "loss:  2.6263489723205566\n",
            "loss:  2.3267388343811035\n",
            "------ Training -----\n",
            "Training Loss:  2.2462756037712097 Time:  13.917614936828613 s\n",
            "====================\n",
            "\n",
            "Epoch:  158\n",
            "loss:  2.1257596015930176\n",
            "loss:  2.218820095062256\n",
            "loss:  2.203404664993286\n",
            "------ Training -----\n",
            "Training Loss:  2.245886195514162 Time:  13.751991271972656 s\n",
            "====================\n",
            "\n",
            "Epoch:  159\n",
            "loss:  2.05879807472229\n",
            "loss:  2.370731830596924\n",
            "loss:  2.8637053966522217\n",
            "------ Training -----\n",
            "Training Loss:  2.221227112462965 Time:  14.020124435424805 s\n",
            "====================\n",
            "\n",
            "Epoch:  160\n",
            "loss:  2.3657379150390625\n",
            "loss:  2.0309457778930664\n",
            "loss:  2.324183464050293\n",
            "------ Training -----\n",
            "Training Loss:  2.205666514776521 Time:  13.977213859558105 s\n",
            "====================\n",
            "\n",
            "Epoch:  161\n",
            "loss:  2.0726876258850098\n",
            "loss:  2.5151474475860596\n",
            "loss:  2.0323290824890137\n",
            "------ Training -----\n",
            "Training Loss:  2.242091188996525 Time:  13.982598066329956 s\n",
            "====================\n",
            "\n",
            "Epoch:  162\n",
            "loss:  2.192373275756836\n",
            "loss:  2.1395716667175293\n",
            "loss:  2.4171500205993652\n",
            "------ Training -----\n",
            "Training Loss:  2.2011376629441473 Time:  13.909415245056152 s\n",
            "====================\n",
            "\n",
            "Epoch:  163\n",
            "loss:  2.029313087463379\n",
            "loss:  2.0744757652282715\n",
            "loss:  2.256635904312134\n",
            "------ Training -----\n",
            "Training Loss:  2.207727938385333 Time:  13.98747181892395 s\n",
            "====================\n",
            "\n",
            "Epoch:  164\n",
            "loss:  2.557338237762451\n",
            "loss:  2.5388686656951904\n",
            "loss:  2.638918399810791\n",
            "------ Training -----\n",
            "Training Loss:  2.2058028831320295 Time:  13.880645751953125 s\n",
            "====================\n",
            "\n",
            "Epoch:  165\n",
            "loss:  1.9703102111816406\n",
            "loss:  2.2308273315429688\n",
            "loss:  2.2628650665283203\n",
            "------ Training -----\n",
            "Training Loss:  2.1852922399165267 Time:  13.95911169052124 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 165 with Loss: 2.1852922399165267\n",
            "\n",
            "Epoch:  166\n",
            "loss:  2.0828843116760254\n",
            "loss:  2.1158294677734375\n",
            "loss:  2.314462900161743\n",
            "------ Training -----\n",
            "Training Loss:  2.184432295419402 Time:  14.070550680160522 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 166 with Loss: 2.184432295419402\n",
            "\n",
            "Epoch:  167\n",
            "loss:  2.0597476959228516\n",
            "loss:  2.195828437805176\n",
            "loss:  1.908290147781372\n",
            "------ Training -----\n",
            "Training Loss:  2.1793259942935683 Time:  13.829681158065796 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 167 with Loss: 2.1793259942935683\n",
            "\n",
            "Epoch:  168\n",
            "loss:  2.3284904956817627\n",
            "loss:  2.254575490951538\n",
            "loss:  1.9345861673355103\n",
            "------ Training -----\n",
            "Training Loss:  2.1280596215846175 Time:  13.81437873840332 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 168 with Loss: 2.1280596215846175\n",
            "\n",
            "Epoch:  169\n",
            "loss:  2.148242473602295\n",
            "loss:  1.9059375524520874\n",
            "loss:  2.2663564682006836\n",
            "------ Training -----\n",
            "Training Loss:  2.1589128455873263 Time:  13.938429832458496 s\n",
            "====================\n",
            "\n",
            "Epoch:  170\n",
            "loss:  2.0384974479675293\n",
            "loss:  2.0500264167785645\n",
            "loss:  2.4580700397491455\n",
            "------ Training -----\n",
            "Training Loss:  2.1647366608603527 Time:  13.987495183944702 s\n",
            "====================\n",
            "\n",
            "Epoch:  171\n",
            "loss:  2.105356216430664\n",
            "loss:  1.992603063583374\n",
            "loss:  2.0366361141204834\n",
            "------ Training -----\n",
            "Training Loss:  2.141150052264585 Time:  13.922136068344116 s\n",
            "====================\n",
            "\n",
            "Epoch:  172\n",
            "loss:  2.056725025177002\n",
            "loss:  2.134488582611084\n",
            "loss:  1.7302591800689697\n",
            "------ Training -----\n",
            "Training Loss:  2.158895221807189 Time:  13.805486917495728 s\n",
            "====================\n",
            "\n",
            "Epoch:  173\n",
            "loss:  1.9461700916290283\n",
            "loss:  2.386474132537842\n",
            "loss:  2.125460147857666\n",
            "------ Training -----\n",
            "Training Loss:  2.1583367737673096 Time:  13.872979640960693 s\n",
            "====================\n",
            "\n",
            "Epoch:  174\n",
            "loss:  2.273211717605591\n",
            "loss:  2.3225326538085938\n",
            "loss:  1.9122660160064697\n",
            "------ Training -----\n",
            "Training Loss:  2.158369290626655 Time:  13.910790920257568 s\n",
            "====================\n",
            "\n",
            "Epoch:  175\n",
            "loss:  1.829645037651062\n",
            "loss:  2.8596372604370117\n",
            "loss:  1.9562113285064697\n",
            "------ Training -----\n",
            "Training Loss:  2.184780927027686 Time:  14.035095691680908 s\n",
            "====================\n",
            "\n",
            "Epoch:  176\n",
            "loss:  2.434324026107788\n",
            "loss:  1.803639531135559\n",
            "loss:  2.534925937652588\n",
            "------ Training -----\n",
            "Training Loss:  2.141856919910948 Time:  13.903308153152466 s\n",
            "====================\n",
            "\n",
            "Epoch:  177\n",
            "loss:  1.8678916692733765\n",
            "loss:  2.429222583770752\n",
            "loss:  1.9982800483703613\n",
            "------ Training -----\n",
            "Training Loss:  2.1011432065802107 Time:  13.806244373321533 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 177 with Loss: 2.1011432065802107\n",
            "\n",
            "Epoch:  178\n",
            "loss:  2.014322280883789\n",
            "loss:  1.837815523147583\n",
            "loss:  2.4388363361358643\n",
            "------ Training -----\n",
            "Training Loss:  2.1546334381830894 Time:  13.863857984542847 s\n",
            "====================\n",
            "\n",
            "Epoch:  179\n",
            "loss:  1.9374147653579712\n",
            "loss:  2.309488296508789\n",
            "loss:  2.271991729736328\n",
            "------ Training -----\n",
            "Training Loss:  2.14405016272755 Time:  13.784664392471313 s\n",
            "====================\n",
            "\n",
            "Epoch:  180\n",
            "loss:  2.421779155731201\n",
            "loss:  1.8144679069519043\n",
            "loss:  2.2086000442504883\n",
            "------ Training -----\n",
            "Training Loss:  2.1170867398633795 Time:  13.746870517730713 s\n",
            "====================\n",
            "\n",
            "Epoch:  181\n",
            "loss:  1.8336889743804932\n",
            "loss:  2.1470065116882324\n",
            "loss:  2.0296010971069336\n",
            "------ Training -----\n",
            "Training Loss:  2.1181164899114835 Time:  13.77726697921753 s\n",
            "====================\n",
            "\n",
            "Epoch:  182\n",
            "loss:  1.9472436904907227\n",
            "loss:  2.4327001571655273\n",
            "loss:  2.2623558044433594\n",
            "------ Training -----\n",
            "Training Loss:  2.112068949109417 Time:  13.885842323303223 s\n",
            "====================\n",
            "\n",
            "Epoch:  183\n",
            "loss:  1.8804097175598145\n",
            "loss:  1.634908676147461\n",
            "loss:  2.221958875656128\n",
            "------ Training -----\n",
            "Training Loss:  2.0862465642266352 Time:  14.068074464797974 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 183 with Loss: 2.0862465642266352\n",
            "\n",
            "Epoch:  184\n",
            "loss:  2.0141427516937256\n",
            "loss:  2.4639172554016113\n",
            "loss:  2.524538516998291\n",
            "------ Training -----\n",
            "Training Loss:  2.11006170915345 Time:  13.79469633102417 s\n",
            "====================\n",
            "\n",
            "Epoch:  185\n",
            "loss:  1.6809971332550049\n",
            "loss:  1.5150058269500732\n",
            "loss:  1.9910836219787598\n",
            "------ Training -----\n",
            "Training Loss:  2.083449708203138 Time:  13.673798322677612 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 185 with Loss: 2.083449708203138\n",
            "\n",
            "Epoch:  186\n",
            "loss:  2.0285158157348633\n",
            "loss:  2.4837303161621094\n",
            "loss:  2.3483808040618896\n",
            "------ Training -----\n",
            "Training Loss:  2.110654027785285 Time:  13.833845853805542 s\n",
            "====================\n",
            "\n",
            "Epoch:  187\n",
            "loss:  1.9759502410888672\n",
            "loss:  2.166522979736328\n",
            "loss:  2.1119813919067383\n",
            "------ Training -----\n",
            "Training Loss:  2.1241679363331554 Time:  13.848616600036621 s\n",
            "====================\n",
            "\n",
            "Epoch:  188\n",
            "loss:  2.031801223754883\n",
            "loss:  2.2807960510253906\n",
            "loss:  1.8214004039764404\n",
            "------ Training -----\n",
            "Training Loss:  2.0811396402827764 Time:  13.833480596542358 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 188 with Loss: 2.0811396402827764\n",
            "\n",
            "Epoch:  189\n",
            "loss:  2.1067299842834473\n",
            "loss:  2.5256905555725098\n",
            "loss:  1.8771369457244873\n",
            "------ Training -----\n",
            "Training Loss:  2.081877262915595 Time:  13.841649293899536 s\n",
            "====================\n",
            "\n",
            "Epoch:  190\n",
            "loss:  2.0011606216430664\n",
            "loss:  2.1842379570007324\n",
            "loss:  2.1282925605773926\n",
            "------ Training -----\n",
            "Training Loss:  2.069020440012722 Time:  13.58597207069397 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 190 with Loss: 2.069020440012722\n",
            "\n",
            "Epoch:  191\n",
            "loss:  2.151212453842163\n",
            "loss:  2.3684568405151367\n",
            "loss:  2.013909339904785\n",
            "------ Training -----\n",
            "Training Loss:  2.0876971260975985 Time:  13.808027982711792 s\n",
            "====================\n",
            "\n",
            "Epoch:  192\n",
            "loss:  1.7423391342163086\n",
            "loss:  2.2058558464050293\n",
            "loss:  2.12446928024292\n",
            "------ Training -----\n",
            "Training Loss:  2.0341584344031447 Time:  13.83545446395874 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 192 with Loss: 2.0341584344031447\n",
            "\n",
            "Epoch:  193\n",
            "loss:  2.0578784942626953\n",
            "loss:  2.3261191844940186\n",
            "loss:  1.9305493831634521\n",
            "------ Training -----\n",
            "Training Loss:  2.059312880039215 Time:  13.857556104660034 s\n",
            "====================\n",
            "\n",
            "Epoch:  194\n",
            "loss:  1.911434531211853\n",
            "loss:  1.9893546104431152\n",
            "loss:  2.0306894779205322\n",
            "------ Training -----\n",
            "Training Loss:  2.0489726293895205 Time:  13.88052487373352 s\n",
            "====================\n",
            "\n",
            "Epoch:  195\n",
            "loss:  1.9499223232269287\n",
            "loss:  2.038378953933716\n",
            "loss:  2.2384214401245117\n",
            "------ Training -----\n",
            "Training Loss:  2.030936942262165 Time:  13.859317302703857 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 195 with Loss: 2.030936942262165\n",
            "\n",
            "Epoch:  196\n",
            "loss:  2.0156478881835938\n",
            "loss:  2.3472118377685547\n",
            "loss:  2.2375736236572266\n",
            "------ Training -----\n",
            "Training Loss:  2.035906279491166 Time:  13.837818622589111 s\n",
            "====================\n",
            "\n",
            "Epoch:  197\n",
            "loss:  2.6100306510925293\n",
            "loss:  1.8154903650283813\n",
            "loss:  1.9498943090438843\n",
            "------ Training -----\n",
            "Training Loss:  2.039220524036278 Time:  13.729154586791992 s\n",
            "====================\n",
            "\n",
            "Epoch:  198\n",
            "loss:  1.9385544061660767\n",
            "loss:  1.987777590751648\n",
            "loss:  1.83295738697052\n",
            "------ Training -----\n",
            "Training Loss:  2.0353008144992892 Time:  13.694837808609009 s\n",
            "====================\n",
            "\n",
            "Epoch:  199\n",
            "loss:  1.7692545652389526\n",
            "loss:  2.0424137115478516\n",
            "loss:  2.4759578704833984\n",
            "------ Training -----\n",
            "Training Loss:  2.009616652787742 Time:  13.870649337768555 s\n",
            "====================\n",
            "Saving model, predictions and generated output for epoch 199 with Loss: 2.009616652787742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK6mToE5Y6Em"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLblDvF7E2hk"
      },
      "source": [
        "# Save embeddings\n",
        "\n",
        "load_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/experiments/skipgram_adari_v2_adjs_50d_200/nlp_embed_199.pth'\n",
        "save_path = '/content/gdrive/My Drive/Colab Notebooks/ieee_slt_2021/data/embeddings/fur_v2_5c_50d_adjs.json'\n",
        "class Embedding(object):\n",
        "    def __init__(self, file_name, load_file):\n",
        "        if load_file != None:\n",
        "            model = torch.load(load_file)\n",
        "        self.file_name = file_name\n",
        "        self.create_embedding()\n",
        "        self.save_json()\n",
        "\n",
        "\n",
        "    def create_embedding(self):\n",
        "        embeddings = model.u_embeddings.weight.cpu().data.numpy()\n",
        "        self.embedding = {}\n",
        "        for id, w in i2w.items():\n",
        "            e = embeddings[id].tolist()\n",
        "            self.embedding[w] = e\n",
        "        \n",
        "    def save_json(self):\n",
        "        out_file = open(self.file_name, \"w\")\n",
        "        json.dump(self.embedding, out_file)\n",
        "        out_file.close()\n",
        "    \n",
        "E = Embedding(save_path, None)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EulHF-Z2KieG"
      },
      "source": [
        "def open_json(path):\n",
        "    f = open(path) \n",
        "    data = json.load(f) \n",
        "    f.close()\n",
        "    return data \n",
        "\n",
        "emb = open_json(save_path)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_G5EywzVLas"
      },
      "source": [
        "a = torch.Tensor(emb['organic'])\n",
        "b = torch.Tensor(emb['curvy'])\n",
        "\n",
        "ga = torch.Tensor(glove_vocab['organic'])\n",
        "gb = torch.Tensor(glove_vocab['curvy'])"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAPW9QFoVO6E",
        "outputId": "199034f8-e5c2-45f1-e300-7d8b63b4030e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "F.cosine_similarity(a, b, dim=0), F.cosine_similarity(ga, gb, dim=0)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3775), tensor(0.0226))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_nkbfspVS1I"
      },
      "source": [
        "a = torch.Tensor(emb['futuristic'])\n",
        "b = torch.Tensor(emb['white'])\n",
        "\n",
        "ga = torch.Tensor(glove_vocab['futuristic'])\n",
        "gb = torch.Tensor(glove_vocab['white'])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgCETrCowKqR",
        "outputId": "3c189ad2-a250-4b2c-e6e1-43497782e2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "F.cosine_similarity(a, b, dim=0), F.cosine_similarity(ga, gb, dim=0)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5099), tensor(0.1415))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S-KarIKwLEy"
      },
      "source": [
        "a = torch.Tensor(emb['dynamic'])\n",
        "b = torch.Tensor(emb['curvy'])\n",
        "\n",
        "ga = torch.Tensor(glove_vocab['dynamic'])\n",
        "gb = torch.Tensor(glove_vocab['curvy'])"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwGSIQixYCzx",
        "outputId": "e2c64683-20fb-4d6d-acb1-59d36d8ffe69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "F.cosine_similarity(a, b, dim=0), F.cosine_similarity(ga, gb, dim=0)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2791), tensor(0.1650))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL1NJduKYERD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}